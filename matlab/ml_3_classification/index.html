
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><p style="font-size:0px">
         \[
         \newcommand{\NN}{\mathbb{N}}
         \newcommand{\CC}{\mathbb{C}}
         \newcommand{\GG}{\mathbb{G}}
         \newcommand{\LL}{\mathbb{L}}
         \newcommand{\PP}{\mathbb{P}}
         \newcommand{\QQ}{\mathbb{Q}}
         \newcommand{\RR}{\mathbb{R}}
         \newcommand{\VV}{\mathbb{V}}
         \newcommand{\ZZ}{\mathbb{Z}}
         \newcommand{\FF}{\mathbb{F}}
         \newcommand{\KK}{\mathbb{K}}
         \newcommand{\UU}{\mathbb{U}}
         \newcommand{\EE}{\mathbb{E}}
         
         \newcommand{\Aa}{\mathcal{A}}
         \newcommand{\Bb}{\mathcal{B}}
         \newcommand{\Cc}{\mathcal{C}}
         \newcommand{\Dd}{\mathcal{D}}
         \newcommand{\Ee}{\mathcal{E}}
         \newcommand{\Ff}{\mathcal{F}}
         \newcommand{\Gg}{\mathcal{G}}
         \newcommand{\Hh}{\mathcal{H}}
         \newcommand{\Ii}{\mathcal{I}}
         \newcommand{\Jj}{\mathcal{J}}
         \newcommand{\Kk}{\mathcal{K}}
         \newcommand{\Ll}{\mathcal{L}}
         \newcommand{\Mm}{\mathcal{M}}
         \newcommand{\Nn}{\mathcal{N}}
         \newcommand{\Oo}{\mathcal{O}}
         \newcommand{\Pp}{\mathcal{P}}
         \newcommand{\Qq}{\mathcal{Q}}
         \newcommand{\Rr}{\mathcal{R}}
         \newcommand{\Ss}{\mathcal{S}}
         \newcommand{\Tt}{\mathcal{T}}
         \newcommand{\Uu}{\mathcal{U}}
         \newcommand{\Vv}{\mathcal{V}}
         \newcommand{\Ww}{\mathcal{W}}
         \newcommand{\Xx}{\mathcal{X}}
         \newcommand{\Yy}{\mathcal{Y}}
         \newcommand{\Zz}{\mathcal{Z}}
         
         \newcommand{\al}{\alpha}
         \newcommand{\la}{\lambda}
         \newcommand{\ga}{\gamma}
         \newcommand{\Ga}{\Gamma}
         \newcommand{\La}{\Lambda}
         \newcommand{\Si}{\Sigma}
         \newcommand{\si}{\sigma}
         \newcommand{\be}{\beta}
         \newcommand{\de}{\delta}
         \newcommand{\De}{\Delta}
         \renewcommand{\phi}{\varphi}
         \renewcommand{\th}{\theta}
         \newcommand{\om}{\omega}
         \newcommand{\Om}{\Omega}
         \renewcommand{\epsilon}{\varepsilon}
         
         \newcommand{\Calpha}{\mathrm{C}^\al}
         \newcommand{\Cbeta}{\mathrm{C}^\be}
         \newcommand{\Cal}{\text{C}^\al}
         \newcommand{\Cdeux}{\text{C}^{2}}
         \newcommand{\Cun}{\text{C}^{1}}
         \newcommand{\Calt}[1]{\text{C}^{#1}}
         
         \newcommand{\lun}{\ell^1}
         \newcommand{\ldeux}{\ell^2}
         \newcommand{\linf}{\ell^\infty}
         \newcommand{\ldeuxj}{{\ldeux_j}}
         \newcommand{\Lun}{\text{\upshape L}^1}
         \newcommand{\Ldeux}{\text{\upshape L}^2}
         \newcommand{\Lp}{\text{\upshape L}^p}
         \newcommand{\Lq}{\text{\upshape L}^q}
         \newcommand{\Linf}{\text{\upshape L}^\infty}
         \newcommand{\lzero}{\ell^0}
         \newcommand{\lp}{\ell^p}
         
         
         \renewcommand{\d}{\ins{d}}
         
         \newcommand{\Grad}{\text{Grad}}
         \newcommand{\grad}{\text{grad}}
         \renewcommand{\div}{\text{div}}
         \newcommand{\diag}{\text{diag}}
         
         \newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
         \newcommand{\pdd}[2]{ \frac{ \partial^2 #1}{\partial #2^2} }
         
         \newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
         \newcommand{\norm}[1]{|\!| #1 |\!|}
         \newcommand{\normi}[1]{\norm{#1}_{\infty}}
         \newcommand{\normu}[1]{\norm{#1}_{1}}
         \newcommand{\normz}[1]{\norm{#1}_{0}}
         \newcommand{\abs}[1]{\vert #1 \vert}
         
         \newcommand{\argmin}{\text{argmin}}
         \newcommand{\argmax}{\text{argmax}}
         \newcommand{\uargmin}[1]{\underset{#1}{\argmin}\;}
         \newcommand{\uargmax}[1]{\underset{#1}{\argmax}\;}
         \newcommand{\umin}[1]{\underset{#1}{\min}\;}
         \newcommand{\umax}[1]{\underset{#1}{\max}\;}
         
         \newcommand{\pa}[1]{\left( #1 \right)}
         \newcommand{\choice}[1]{ \left\{  \begin{array}{l} #1 \end{array} \right. }
         
         \newcommand{\enscond}[2]{ \left\{ #1 \;:\; #2 \right\} }
         
         \newcommand{\qandq}{ \quad \text{and} \quad }
         \newcommand{\qqandqq}{ \qquad \text{and} \qquad }
         \newcommand{\qifq}{ \quad \text{if} \quad }
         \newcommand{\qqifqq}{ \qquad \text{if} \qquad }
         \newcommand{\qwhereq}{ \quad \text{where} \quad }
         \newcommand{\qqwhereqq}{ \qquad \text{where} \qquad }
         \newcommand{\qwithq}{ \quad \text{with} \quad }
         \newcommand{\qqwithqq}{ \qquad \text{with} \qquad }
         \newcommand{\qforq}{ \quad \text{for} \quad }
         \newcommand{\qqforqq}{ \qquad \text{for} \qquad }
         \newcommand{\qqsinceqq}{ \qquad \text{since} \qquad }
         \newcommand{\qsinceq}{ \quad \text{since} \quad }
         \newcommand{\qarrq}{\quad\Longrightarrow\quad}
         \newcommand{\qqarrqq}{\quad\Longrightarrow\quad}
         \newcommand{\qiffq}{\quad\Longleftrightarrow\quad}
         \newcommand{\qqiffqq}{\qquad\Longleftrightarrow\qquad}
         \newcommand{\qsubjq}{ \quad \text{subject to} \quad }
         \newcommand{\qqsubjqq}{ \qquad \text{subject to} \qquad }
         
         \newcommand{\eqdef}{\equiv}
         \]
         
      </p>
      <title>Logistic Classification</title>
      <NOSCRIPT>
         <DIV STYLE="color:#CC0000; text-align:center"><B>Warning: <A HREF="http://www.math.union.edu/locate/jsMath">jsMath</A>
               	requires JavaScript to process the mathematics on this page.<BR>
               	If your browser supports JavaScript, be sure it is enabled.</B></DIV>
         <HR>
      </NOSCRIPT>
      <meta name="generator" content="MATLAB 9.0">
      <meta name="date" content="2017-08-08">
      <meta name="m-file" content="index">
      <LINK REL="stylesheet" HREF="../style.css" TYPE="text/css">
   </head>
   <body>
      <div class="content">
         <h1>Logistic Classification</h1>
         <introduction>
            <p>This tour details the logistic classification method (for 2 classes and multi-classes).</p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Installing toolboxes and setting up the path.</a></li>
               <li><a href="#8">Dataset Loading</a></li>
               <li><a href="#15">Dimenionality Reduction and PCA</a></li>
               <li><a href="#22">Support Vector Machine Classification</a></li>
               <li><a href="#24">Two Classes Logistic Classification</a></li>
               <li><a href="#26">Multi-Classes Logistic Classification</a></li>
            </ul>
         </div>
         <h2>Installing toolboxes and setting up the path.<a name="1"></a></h2>
         <p>You need to download the following files: <a href="../toolbox_general.zip">general toolbox</a>.
         </p>
         <p>You need to unzip these toolboxes in your working directory, so that you have <tt>toolbox_general</tt> in your directory.
         </p>
         <p><b>For Scilab user:</b> you must replace the Matlab comment '%' by its Scilab counterpart '//'.
         </p>
         <p><b>Recommandation:</b> You should create a text file named for instance <tt>numericaltour.sce</tt> (in Scilab) or <tt>numericaltour.m</tt> (in Matlab) to write all the Scilab/Matlab command you want to execute. Then, simply run <tt>exec('numericaltour.sce');</tt> (in Scilab) or <tt>numericaltour;</tt> (in Matlab) to run the commands.
         </p>
         <p>Execute this line only if you are using Matlab.</p><pre class="codeinput">getd = @(p)path(p,path); <span class="comment">% scilab users must *not* execute this</span>
</pre><p>Then you can add the toolboxes to the path.</p><pre class="codeinput">getd(<span class="string">'toolbox_general/'</span>);
</pre><h2>Dataset Loading<a name="8"></a></h2>
         <p>Helpers.</p><pre class="codeinput">SetAR = @(ar)set(gca, <span class="string">'PlotBoxAspectRatio'</span>, [1 ar 1], <span class="string">'FontSize'</span>, 20);
</pre><p>Load the dataset.</p><pre class="codeinput">name = <span class="string">'digits'</span>;
load([<span class="string">'ml-'</span> name]);
</pre><p>Randomly permute it.</p><pre class="codeinput">A = A(randperm(size(A,1)),:);
</pre><p>Separate the features \(X\) from the data \(y\) to predict information.</p><pre class="codeinput">X = A(:,1:end-1);
y = A(:,end);
y = y-min(y)+1;
k = max(y);
</pre><p>\(p\) is the number of samples, \(n\) is the dimensionality of the features,</p><pre class="codeinput">[p,n] = size(X);
</pre><p>Display a few samples digits</p><pre class="codeinput">q = 5;
clf;
<span class="keyword">for</span> i=1:k
    I = find(y==i);
    <span class="keyword">for</span> j=1:q
        f = reshape(X(I(j),:), sqrt(n)*[1 1])';
        subplot(q,k, (j-1)*k+i );
        imagesc(-f); axis <span class="string">image</span>; axis <span class="string">off</span>;
    <span class="keyword">end</span>
<span class="keyword">end</span>
colormap <span class="string">gray(256)</span>;
</pre><img vspace="5" hspace="5" src="index_01.png"> <h2>Dimenionality Reduction and PCA<a name="15"></a></h2>
         <p>In order to display in 2D or 3D the data, dimensionality is needed. The simplest method is the principal component analysis,
            which perform an orthogonal linear projection on the principal axsis (eigenvector) of the covariance matrix.
         </p>
         <p>Compute empirical mean \(m \in \RR^n\) and covariance \(C \in \RR^{n \times n}\).</p><pre class="codeinput">m = mean(X,1);
Xm = X-repmat(m, [p 1]);
C = Xm'*Xm;
</pre><p>Display the covariance matrix.</p><pre class="codeinput">clf;
imagesc(C);
</pre><img vspace="5" hspace="5" src="index_02.png"> <p>Compute PCA ortho-basis.</p><pre class="codeinput">[U,D] = eig(C);
[d,I] = sort(diag(D), <span class="string">'descend'</span>);
U = U(:,I);
</pre><p>Compute the feature in the PCA basis.</p><pre class="codeinput">z = (U'*Xm')';
</pre><p>Plot sqrt of the eigenvalues.</p><pre class="codeinput">clf;
plot(sqrt(max(d,0)), <span class="string">'.-'</span>, <span class="string">'LineWidth'</span>, 2, <span class="string">'MarkerSize'</span>, 30);
axis <span class="string">tight</span>;
SetAR(1/2);
</pre><img vspace="5" hspace="5" src="index_03.png"> <p>Display in 2D.</p><pre class="codeinput">col = {<span class="string">'b'</span> <span class="string">'g'</span> <span class="string">'r'</span> <span class="string">'c'</span> <span class="string">'m'</span> <span class="string">'y'</span> <span class="string">'k'</span>};
ms = 25;
clf; hold <span class="string">on</span>;
lgd = {};
<span class="keyword">for</span> i=1:min(k,length(col))
    I = find(y==i);
    plot(z(I,1), z(I,2), <span class="string">'.'</span>, <span class="string">'Color'</span>, col{i}, <span class="string">'MarkerSize'</span>, ms);
    lgd{end+1} = num2str(i);
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
legend(lgd);
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_04.png"> <h2>Support Vector Machine Classification<a name="22"></a></h2>
         <p>Select only two classes.</p><pre class="codeinput">c = [5, 7];
xx = []; yy = [];
<span class="keyword">for</span> i=1:2
    I = find(y==c(i));
    xx = [xx; X(I,:)];
    yy = [yy; ones(length(I),1)*c(i)];
<span class="keyword">end</span>
</pre><h2>Two Classes Logistic Classification<a name="24"></a></h2>
         <p><i>Warning:</i> Logisitic classification is actually called "logistic regression" in the literature, but it is in fact a classification method.
         </p>
         <h2>Multi-Classes Logistic Classification<a name="26"></a></h2>
         <p class="footer"><br>
            Copyright  (c) 2010 Gabriel Peyre<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Logistic Classification
% This tour details the logistic classification method (for 2 classes and
% multi-classes).

%% Installing toolboxes and setting up the path.

%%
% You need to download the following files: 
% <../toolbox_general.zip general toolbox>.

%%
% You need to unzip these toolboxes in your working directory, so
% that you have 
% |toolbox_general|
% in your directory.

%%
% *For Scilab user:* you must replace the Matlab comment '%' by its Scilab
% counterpart '//'.

%%
% *Recommandation:* You should create a text file named for instance |numericaltour.sce| (in Scilab) or |numericaltour.m| (in Matlab) to write all the
% Scilab/Matlab command you want to execute. Then, simply run |exec('numericaltour.sce');| (in Scilab) or |numericaltour;| (in Matlab) to run the commands. 

%%
% Execute this line only if you are using Matlab.

getd = @(p)path(p,path); % scilab users must *not* execute this

%%
% Then you can add the toolboxes to the path.

getd('toolbox_general/');

%% Dataset Loading

%%
% Helpers.

SetAR = @(ar)set(gca, 'PlotBoxAspectRatio', [1 ar 1], 'FontSize', 20);

%%
% Load the dataset.

name = 'digits';
load(['ml-' name]);

%%
% Randomly permute it.

A = A(randperm(size(A,1)),:);

%%
% Separate the features \(X\) from the data \(y\) to predict information.

X = A(:,1:end-1);
y = A(:,end);
y = y-min(y)+1;
k = max(y);

%%
% \(p\) is the number of samples, \(n\) is the dimensionality of the features,

[p,n] = size(X);

%%
% Display a few samples digits

q = 5;
clf;
for i=1:k
    I = find(y==i);
    for j=1:q
        f = reshape(X(I(j),:), sqrt(n)*[1 1])';
        subplot(q,k, (j-1)*k+i );
        imagesc(-f); axis image; axis off;
    end
end
colormap gray(256);

%% Dimenionality Reduction and PCA
% In order to display in 2D or 3D the data, dimensionality is needed.
% The simplest method is the principal component analysis, which perform an
% orthogonal linear projection on the principal axsis (eigenvector) of the
% covariance matrix. 

%%
% Compute empirical mean \(m \in \RR^n\) and covariance \(C \in \RR^{n \times n}\).

m = mean(X,1);
Xm = X-repmat(m, [p 1]);
C = Xm'*Xm;

%%
% Display the covariance matrix.

clf;
imagesc(C);

%%
% Compute PCA ortho-basis. 

[U,D] = eig(C); 
[d,I] = sort(diag(D), 'descend');
U = U(:,I);

%%
% Compute the feature in the PCA basis.

z = (U'*Xm')';

%%
% Plot sqrt of the eigenvalues.

clf; 
plot(sqrt(max(d,0)), '.-', 'LineWidth', 2, 'MarkerSize', 30);
axis tight;
SetAR(1/2);

%%
% Display in 2D.

col = {'b' 'g' 'r' 'c' 'm' 'y' 'k'};
ms = 25;
clf; hold on;
lgd = {};
for i=1:min(k,length(col))
    I = find(y==i);
    plot(z(I,1), z(I,2), '.', 'Color', col{i}, 'MarkerSize', ms);
    lgd{end+1} = num2str(i);
end
axis tight; axis equal; box on;
legend(lgd);
SetAR(1);


%% Support Vector Machine Classification

%%
% Select only two classes.

c = [5, 7];
xx = []; yy = [];
for i=1:2
    I = find(y==c(i));
    xx = [xx; X(I,:)];
    yy = [yy; ones(length(I),1)*c(i)];
end



%% Two Classes Logistic Classification

%%
% _Warning:_ Logisitic classification is actually called "logistic
% regression" in the literature, but it is in fact a classification method.

%% Multi-Classes Logistic Classification
##### SOURCE END #####
-->
   </body>
</html>