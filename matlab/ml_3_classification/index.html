
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><p style="font-size:0px">
         \[
         \newcommand{\NN}{\mathbb{N}}
         \newcommand{\CC}{\mathbb{C}}
         \newcommand{\GG}{\mathbb{G}}
         \newcommand{\LL}{\mathbb{L}}
         \newcommand{\PP}{\mathbb{P}}
         \newcommand{\QQ}{\mathbb{Q}}
         \newcommand{\RR}{\mathbb{R}}
         \newcommand{\VV}{\mathbb{V}}
         \newcommand{\ZZ}{\mathbb{Z}}
         \newcommand{\FF}{\mathbb{F}}
         \newcommand{\KK}{\mathbb{K}}
         \newcommand{\UU}{\mathbb{U}}
         \newcommand{\EE}{\mathbb{E}}
         
         \newcommand{\Aa}{\mathcal{A}}
         \newcommand{\Bb}{\mathcal{B}}
         \newcommand{\Cc}{\mathcal{C}}
         \newcommand{\Dd}{\mathcal{D}}
         \newcommand{\Ee}{\mathcal{E}}
         \newcommand{\Ff}{\mathcal{F}}
         \newcommand{\Gg}{\mathcal{G}}
         \newcommand{\Hh}{\mathcal{H}}
         \newcommand{\Ii}{\mathcal{I}}
         \newcommand{\Jj}{\mathcal{J}}
         \newcommand{\Kk}{\mathcal{K}}
         \newcommand{\Ll}{\mathcal{L}}
         \newcommand{\Mm}{\mathcal{M}}
         \newcommand{\Nn}{\mathcal{N}}
         \newcommand{\Oo}{\mathcal{O}}
         \newcommand{\Pp}{\mathcal{P}}
         \newcommand{\Qq}{\mathcal{Q}}
         \newcommand{\Rr}{\mathcal{R}}
         \newcommand{\Ss}{\mathcal{S}}
         \newcommand{\Tt}{\mathcal{T}}
         \newcommand{\Uu}{\mathcal{U}}
         \newcommand{\Vv}{\mathcal{V}}
         \newcommand{\Ww}{\mathcal{W}}
         \newcommand{\Xx}{\mathcal{X}}
         \newcommand{\Yy}{\mathcal{Y}}
         \newcommand{\Zz}{\mathcal{Z}}
         
         \newcommand{\al}{\alpha}
         \newcommand{\la}{\lambda}
         \newcommand{\ga}{\gamma}
         \newcommand{\Ga}{\Gamma}
         \newcommand{\La}{\Lambda}
         \newcommand{\Si}{\Sigma}
         \newcommand{\si}{\sigma}
         \newcommand{\be}{\beta}
         \newcommand{\de}{\delta}
         \newcommand{\De}{\Delta}
         \renewcommand{\phi}{\varphi}
         \renewcommand{\th}{\theta}
         \newcommand{\om}{\omega}
         \newcommand{\Om}{\Omega}
         \renewcommand{\epsilon}{\varepsilon}
         
         \newcommand{\Calpha}{\mathrm{C}^\al}
         \newcommand{\Cbeta}{\mathrm{C}^\be}
         \newcommand{\Cal}{\text{C}^\al}
         \newcommand{\Cdeux}{\text{C}^{2}}
         \newcommand{\Cun}{\text{C}^{1}}
         \newcommand{\Calt}[1]{\text{C}^{#1}}
         
         \newcommand{\lun}{\ell^1}
         \newcommand{\ldeux}{\ell^2}
         \newcommand{\linf}{\ell^\infty}
         \newcommand{\ldeuxj}{{\ldeux_j}}
         \newcommand{\Lun}{\text{\upshape L}^1}
         \newcommand{\Ldeux}{\text{\upshape L}^2}
         \newcommand{\Lp}{\text{\upshape L}^p}
         \newcommand{\Lq}{\text{\upshape L}^q}
         \newcommand{\Linf}{\text{\upshape L}^\infty}
         \newcommand{\lzero}{\ell^0}
         \newcommand{\lp}{\ell^p}
         
         
         \renewcommand{\d}{\ins{d}}
         
         \newcommand{\Grad}{\text{Grad}}
         \newcommand{\grad}{\text{grad}}
         \renewcommand{\div}{\text{div}}
         \newcommand{\diag}{\text{diag}}
         
         \newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
         \newcommand{\pdd}[2]{ \frac{ \partial^2 #1}{\partial #2^2} }
         
         \newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
         \newcommand{\norm}[1]{|\!| #1 |\!|}
         \newcommand{\normi}[1]{\norm{#1}_{\infty}}
         \newcommand{\normu}[1]{\norm{#1}_{1}}
         \newcommand{\normz}[1]{\norm{#1}_{0}}
         \newcommand{\abs}[1]{\vert #1 \vert}
         
         \newcommand{\argmin}{\text{argmin}}
         \newcommand{\argmax}{\text{argmax}}
         \newcommand{\uargmin}[1]{\underset{#1}{\argmin}\;}
         \newcommand{\uargmax}[1]{\underset{#1}{\argmax}\;}
         \newcommand{\umin}[1]{\underset{#1}{\min}\;}
         \newcommand{\umax}[1]{\underset{#1}{\max}\;}
         
         \newcommand{\pa}[1]{\left( #1 \right)}
         \newcommand{\choice}[1]{ \left\{  \begin{array}{l} #1 \end{array} \right. }
         
         \newcommand{\enscond}[2]{ \left\{ #1 \;:\; #2 \right\} }
         
         \newcommand{\qandq}{ \quad \text{and} \quad }
         \newcommand{\qqandqq}{ \qquad \text{and} \qquad }
         \newcommand{\qifq}{ \quad \text{if} \quad }
         \newcommand{\qqifqq}{ \qquad \text{if} \qquad }
         \newcommand{\qwhereq}{ \quad \text{where} \quad }
         \newcommand{\qqwhereqq}{ \qquad \text{where} \qquad }
         \newcommand{\qwithq}{ \quad \text{with} \quad }
         \newcommand{\qqwithqq}{ \qquad \text{with} \qquad }
         \newcommand{\qforq}{ \quad \text{for} \quad }
         \newcommand{\qqforqq}{ \qquad \text{for} \qquad }
         \newcommand{\qqsinceqq}{ \qquad \text{since} \qquad }
         \newcommand{\qsinceq}{ \quad \text{since} \quad }
         \newcommand{\qarrq}{\quad\Longrightarrow\quad}
         \newcommand{\qqarrqq}{\quad\Longrightarrow\quad}
         \newcommand{\qiffq}{\quad\Longleftrightarrow\quad}
         \newcommand{\qqiffqq}{\qquad\Longleftrightarrow\qquad}
         \newcommand{\qsubjq}{ \quad \text{subject to} \quad }
         \newcommand{\qqsubjqq}{ \qquad \text{subject to} \qquad }
         
         \newcommand{\eqdef}{\equiv}
         \]
         
      </p>
      <title>Logistic Classification</title>
      <NOSCRIPT>
         <DIV STYLE="color:#CC0000; text-align:center"><B>Warning: <A HREF="http://www.math.union.edu/locate/jsMath">jsMath</A>
               	requires JavaScript to process the mathematics on this page.<BR>
               	If your browser supports JavaScript, be sure it is enabled.</B></DIV>
         <HR>
      </NOSCRIPT>
      <meta name="generator" content="MATLAB 9.0">
      <meta name="date" content="2017-08-09">
      <meta name="m-file" content="index">
      <LINK REL="stylesheet" HREF="../style.css" TYPE="text/css">
   </head>
   <body>
      <div class="content">
         <h1>Logistic Classification</h1>
         <introduction>
            <p>This tour details the logistic classification method (for 2 classes and multi-classes).</p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#4">Installing toolboxes and setting up the path.</a></li>
               <li><a href="#11">Dataset Loading</a></li>
               <li><a href="#18">Dimenionality Reduction and PCA</a></li>
               <li><a href="#25">Two Classes Logistic Classification</a></li>
               <li><a href="#39">Kernelized Logistic Classification</a></li>
               <li><a href="#57">Multi-Classes Logistic Classification</a></li>
            </ul>
         </div>
         <p><i>Warning:</i> Logisitic classification is actually called <a href="https://en.wikipedia.org/wiki/Logistic_regression">"logistic regression"</a> in the literature, but it is in fact a classification method.
         </p>
         <p>We recommend that after doing this Numerical Tours, you apply it to your own data, for instance using a dataset from <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/">LibSVM</a>.
         </p>
         <p><i>Disclaimer:</i> these machine learning tours are intended to be overly-simplistic implementations and applications of baseline machine learning
            methods. For more advanced uses and implementations, we recommend to use a state-of-the-art library, the most well known being
            <a href="http://scikit-learn.org/">Scikit-Learn</a></p>
         <h2>Installing toolboxes and setting up the path.<a name="4"></a></h2>
         <p>You need to download the following files: <a href="../toolbox_general.zip">general toolbox</a>.
         </p>
         <p>You need to unzip these toolboxes in your working directory, so that you have <tt>toolbox_general</tt> in your directory.
         </p>
         <p><b>For Scilab user:</b> you must replace the Matlab comment '%' by its Scilab counterpart '//'.
         </p>
         <p><b>Recommandation:</b> You should create a text file named for instance <tt>numericaltour.sce</tt> (in Scilab) or <tt>numericaltour.m</tt> (in Matlab) to write all the Scilab/Matlab command you want to execute. Then, simply run <tt>exec('numericaltour.sce');</tt> (in Scilab) or <tt>numericaltour;</tt> (in Matlab) to run the commands.
         </p>
         <p>Execute this line only if you are using Matlab.</p><pre class="codeinput">getd = @(p)path(p,path); <span class="comment">% scilab users must *not* execute this</span>
</pre><p>Then you can add the toolboxes to the path.</p><pre class="codeinput">getd(<span class="string">'toolbox_general/'</span>);
</pre><h2>Dataset Loading<a name="11"></a></h2>
         <p>We load a dataset of \(p\) images of size \(n = 8 \times 8\), representing digits from 0 to 9 (so there are \(k=10\) classes).</p>
         <p>First define a few helpers.</p><pre class="codeinput">SetAR = @(ar)set(gca, <span class="string">'PlotBoxAspectRatio'</span>, [1 ar 1], <span class="string">'FontSize'</span>, 20);
Xm = @(X)X-repmat(mean(X,1), [size(X,1) 1]);
Cov = @(X)Xm(X)'*Xm(X);
</pre><p>Load the dataset.</p><pre class="codeinput">name = <span class="string">'digits'</span>;
load([<span class="string">'ml-'</span> name]);
</pre><p>Randomly permute it.</p><pre class="codeinput">A = A(randperm(size(A,1)),:);
</pre><p>Separate the features \(X\) from the data \(y\) to predict information.</p><pre class="codeinput">X = A(:,1:end-1);
y = A(:,end);
y = y-min(y)+1;
k = max(y);
</pre><p>\(p\) is the number of samples, \(n\) is the dimensionality of the features,</p><pre class="codeinput">[p,n] = size(X);
</pre><p>Display a few samples digits</p><pre class="codeinput">q = 5;
clf;
<span class="keyword">for</span> i=1:k
    I = find(y==i);
    <span class="keyword">for</span> j=1:q
        f = reshape(X(I(j),:), sqrt(n)*[1 1])';
        subplot(q,k, (j-1)*k+i );
        imagesc(-f); axis <span class="string">image</span>; axis <span class="string">off</span>;
    <span class="keyword">end</span>
<span class="keyword">end</span>
colormap <span class="string">gray(256)</span>;
</pre><img vspace="5" hspace="5" src="index_01.png"> <h2>Dimenionality Reduction and PCA<a name="18"></a></h2>
         <p>In order to display in 2D or 3D the data, dimensionality is needed. The simplest method is the principal component analysis,
            which perform an orthogonal linear projection on the principal axsis (eigenvector) of the covariance matrix.
         </p>
         <p>Display the covariance matrix \(C \in \RR^{n \times n}\).</p><pre class="codeinput">clf; imagesc( Cov(X) );
</pre><img vspace="5" hspace="5" src="index_02.png"> <p>Compute PCA ortho-basis.</p><pre class="codeinput">[U,D,V] = svd(Xm(X),<span class="string">'econ'</span>);
</pre><p>Compute the feature in the PCA basis.</p><pre class="codeinput">Z = Xm(X) * V;
</pre><p>Plot sqrt of the eigenvalues.</p><pre class="codeinput">clf;
plot(diag(D), <span class="string">'.-'</span>, <span class="string">'LineWidth'</span>, 2, <span class="string">'MarkerSize'</span>, 30);
axis <span class="string">tight</span>;
SetAR(1/2);
</pre><img vspace="5" hspace="5" src="index_03.png"> <p>Display in 2D.</p><pre class="codeinput">col = [ [1 0 0]; [0 1 0]; [0 0 1]; [0 0 0]; [0 1 1]; [1 0 1]; [1 1 0]; <span class="keyword">...</span>
    [1 .5 .5]; [.5 1 .5]; [.5 .5 1]  ]';
ms = 25;
clf; hold <span class="string">on</span>;
lgd = {};
<span class="keyword">for</span> i=1:min(k,size(col,2))
    I = find(y==i);
    plot(Z(I,1), Z(I,2), <span class="string">'.'</span>, <span class="string">'Color'</span>, col(:,i), <span class="string">'MarkerSize'</span>, ms);
    lgd{end+1} = num2str(i);
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
legend(lgd, <span class="string">'Location'</span>, <span class="string">'EastOutside'</span>);
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_04.png"> <p>Display in 3D.</p><pre class="codeinput">col = [ [1 0 0]; [0 1 0]; [0 0 1]; [0 0 0]; [0 1 1]; [1 0 1]; [1 1 0]; <span class="keyword">...</span>
    [1 .5 .5]; [.5 1 .5]; [.5 .5 1]  ]';
ms = 25;
clf; hold <span class="string">on</span>;
lgd = {};
<span class="keyword">for</span> i=1:min(k,size(col,2))
    I = find(y==i);
    plot3(Z(I,1), Z(I,2), Z(I,3), <span class="string">'.'</span>, <span class="string">'Color'</span>, col(:,i), <span class="string">'MarkerSize'</span>, ms);
    lgd{end+1} = num2str(i);
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
view(3)
legend(lgd, <span class="string">'Location'</span>, <span class="string">'EastOutside'</span>);
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_05.png"> <h2>Two Classes Logistic Classification<a name="25"></a></h2>
         <p>Logistic classification is, with <a href="https://en.wikipedia.org/wiki/Support_vector_machine">support vector machine (SVM)</a>, the baseline method to perform classification. Its main advantage over SVM is that is is a smooth minimization problem,
            and that it also output class probabity, offering a probabilistic interpretation of the classification.
         </p>
         <p>Select only two classes. Here classes indexes are set to \(y_i \in \{-1,1\}\) to simplify the equations.</p><pre class="codeinput">c = [1 2]; <span class="comment">% selected classes</span>
c = [1 6];
Xsvg = X; ysvg = y;
X = []; y = [];
<span class="keyword">for</span> i=1:2
    I = find(ysvg==c(i));
    X = [X; Xsvg(I,:)];
    y = [y; ones(length(I),1)*(2*i-3)];
<span class="keyword">end</span>
p = size(X,1);
</pre><p>Plot only the two classes.</p><pre class="codeinput">[U,D,V] = svd(Xm(X),<span class="string">'econ'</span>);
Z = Xm(X) * V;
clf; hold <span class="string">on</span>;
lgd = {};
<span class="keyword">for</span> i=1:2
    I = find(y==2*i-3);
    plot(Z(I,1), Z(I,2), <span class="string">'.'</span>, <span class="string">'Color'</span>, col(:,i), <span class="string">'MarkerSize'</span>, ms);
    lgd{end+1} = num2str(c(i));
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
legend(lgd, <span class="string">'Location'</span>, <span class="string">'EastOutside'</span>);
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_06.png"> <p>Logistic classification minimize a logistic loss in place of the usual \(\ell^2\) loss for regression   \[ \umin{w} E(w) \eqdef
            \frac{1}{p} \sum_{i=1}^p L(\dotp{x_i}{w},y_i)  \] where the logistic loss reads   \[ L( s,y ) \eqdef \log( 1+\exp(-sy) ) \]
            This corresponds to a smooth convex minimization. If \(X\) is injective, this is also strictly convex, hence it has a single
            global minimum.
         </p>
         <p>Compare the binary (ideal) 0-1 loss, the logistic loss and the <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> (the one used for SVM).
         </p><pre class="codeinput">t = linspace(-3,3,255)';
clf;
plot(t, [t&gt;0, log(1+exp(t)), max(t,0)], <span class="string">'LineWidth'</span>, 2 );
axis <span class="string">tight</span>;
legend(<span class="string">'Binary'</span>, <span class="string">'Logistic'</span>, <span class="string">'Hinge'</span>, <span class="string">'Location'</span>, <span class="string">'NorthWest'</span>);
SetAR(1/2);
</pre><img vspace="5" hspace="5" src="index_07.png"> <p>This can be interpreted as a <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimator</a> when one models the probability of  belonging to the two classes for sample \(x_i\) as   \[ h(x_i) \eqdef (\th(x_i),1-\th(x_i))
            \qwhereq           \th(s) \eqdef \frac{e^{s}}{1+e^s} = (1+e^{-s})^{-1}  \]
         </p>
         <p>Re-writting the energy to minimize   \[ E(w) = \Ll(X w,y) \qwhereq \Ll(s,y)= \frac{1}{p}  \sum_i L(s_i,y_i), \] its gradient
            reads   \[ \nabla E(w) = X^\top \nabla \Ll(X w,y)       \qwhereq       \nabla \Ll(s,y) = \frac{y}{p} \odot \th(-y \odot s),
              \] where \(\odot\) is the pointwise multiplication operator, i.e. <tt>.*</tt> in Matlab.
         </p>
         <p>Define the energies.</p><pre class="codeinput">L = @(s,y)1/p * sum( log( 1 + exp(-s.*y) ) );
E = @(w)L(X*w,y);
</pre><p>Define their gradients.</p><pre class="codeinput">theta = @(v)exp(-v) ./ (1+exp(-v));
nablaL = @(s,r)- 1/p * y.* theta(s.*y);
nablaE = @(w)X'*nablaL(X*w,y);
</pre><p><i>Exercice 1:</i> (<a href="../missing-exo/">check the solution</a>) Implement a gradient descent  \[ w_{\ell+1} = w_\ell - \tau_\ell \nabla E(w_\ell). \] Monitor the energy decay. Test different
            step size, and compare with the theory (in particular plot in log domain to illustrate the linear rate).
         </p><pre class="codeinput">exo1;
</pre><img vspace="5" hspace="5" src="index_08.png"> <p>Display the weight vector \(w\).</p><pre class="codeinput">clf;
imagesc(reshape(w,sqrt(n)*[1 1])');
axis <span class="string">image</span>; axis <span class="string">off</span>;
colormap <span class="string">jet(256)</span>;
colorbar;
</pre><img vspace="5" hspace="5" src="index_09.png"> <p>Generate a 2D grid of points over PCA space and map it to feature space.</p><pre class="codeinput">M = max(abs(Z(:)));
q = 101;
t = linspace(-M,M,q);
[B,A] = meshgrid(t,t);
G = zeros(q*q,n);
G(:,1:2) = [A(:), B(:)];
G = G*V' + repmat( mean(X,1), [q*q 1] );
</pre><p>Evaluate class probability associated to weight vectors on this grid.</p><pre class="codeinput">Theta = theta(G*w);
Theta = reshape(Theta, [q q]);
</pre><p>Display the data overlaid on top of the classification probability, this highlight the separating hyperplane?\( \enscond{x}{\dotp{w}{x}=0}
            \).
         </p><pre class="codeinput">clf; hold <span class="string">on</span>;
imagesc(t,t, Theta');
lgd = {};
<span class="keyword">for</span> i=1:2
    I = find(y==2*i-3);
    plot(Z(I,1), Z(I,2), <span class="string">'.'</span>, <span class="string">'Color'</span>, col(:,i), <span class="string">'MarkerSize'</span>, ms);
    lgd{end+1} = num2str(c(i));
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
legend(lgd);
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_10.png"> <h2>Kernelized Logistic Classification<a name="39"></a></h2>
         <p>Logistic classification tries to separate the classes using a linear separating hyperplane \( \enscond{x}{\dotp{w}{x}=0}.
            \)
         </p>
         <p>In order to generate a non-linear descision boundary, one can replace the parametric linear model by a non-linear <a href="https://en.wikipedia.org/wiki/Nonparametric_statistics">non-parametric</a> model, thanks to kernelization. It is non-parametric in the sense that the number of parameter grows with the number \(p\)
            of sample (while for the basic method, the number of parameter is \(n\). This allows in particular to generate decision boundary
            of arbitrary complexity.
         </p>
         <p>The downside is that the numerical complexity of the method grows (at least) quadratically with \(p\).</p>
         <p>The good news however is that thanks to the theory of <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert spaces</a> (RKHS), one can still compute this non-linear decision function using (almost) the same numerical algorithm.
         </p>
         <p>Given a kernel \( \kappa(x,z) \in \RR \) defined for \((x,z) \in \RR^n\), the kernelized method replace the linear decision
            functional \(f(x) = \dotp{x}{w}\) by a sum of kernel centered on the samples \[ f_h(x) = \sum_{i=1}^n h_i k(x_i,x) \] where
            \(h \in \RR^p\) is the unknown vector of weight to find.
         </p>
         <p>When using the linear kernel \(\kappa(x,y)=\dotp{x}{y}\), one retrieves the previously studied linear method.</p>
         <p>Macro to compute pairwise squared Euclidean distance matrix.</p><pre class="codeinput">distmat = @(X,Z)bsxfun(@plus,dot(X',X',1)',dot(Z',Z',1))-2*(X*Z');
</pre><p>The gaussian kernel is the most well known and used kernel \[ \kappa(x,y) \eqdef e^{-\frac{\norm{x-y}^2}{2\sigma^2}} . \]
            The bandwidth parameter \(\si&gt;0\) is crucial and controls the locality of the model. It is typically tuned through cross validation.
         </p><pre class="codeinput">sigma = 10;
kappa = @(X,Z)exp( -distmat(X,Z)/(2*sigma^2) );
</pre><p>Once avaluated on grid points, the kernel define a matrix \[ K = (\kappa(x_i,x_j))_{i,j=1}^p \in \RR^{p \times p}.  \]</p><pre class="codeinput">K = kappa(X,X);
</pre><p>Valid kernels are those that gives rise to positive symmetric matrices \(K\). The linear and Gaussian kernel are valid kernel
            functions. Other popular kernels include the polynomial kernel \( \dotp{x}{y}^a \) for \(a \geq 1\) and the Laplacian kernel
            \( \exp( -\norm{x-y}^2/\si ) \).
         </p>
         <p>The kernelized Logistic minimization reads   \[ \umin{h} F(h) \eqdef \Ll(K h,y). \]</p><pre class="codeinput">F = @(h)L(K*h,y);
nablaF = @(h)K'*nablaL(K*h,y);
</pre><p>This minimization can be related to an infinite dimensional optimization problem where one minimizes directly over the function
            \(f\). This is shown to be equivalent to the above finite-dimenisonal optimization problem thanks to the theory of RKHS.
         </p>
         <p><i>Exercice 2:</i> (<a href="../missing-exo/">check the solution</a>) Implement a gradient descent to minimize \(F(h)\). Monitor the energy decay. Test different step size, and compare with
            the theory.
         </p><pre class="codeinput">exo2;
</pre><img vspace="5" hspace="5" src="index_11.png"> <p>Once this optimal \(h\) has been found, class probability at a point \(x\) are obtained as   \[ (\th(f_h(x)), 1-\th(f_h(x))
            \] where \(f_h\) has been defined above.
         </p>
         <p>We evaluate this classification probability on a grid.</p><pre class="codeinput">Theta = theta(kappa(G,X)*h);
Theta = reshape(Theta, [q,q]);
</pre><p>Display the classification probability.</p><pre class="codeinput">clf; hold <span class="string">on</span>;
imagesc(t,t, Theta');
lgd = {};
<span class="keyword">for</span> i=1:2
    I = find(y==2*i-3);
    plot(Z(I,1), Z(I,2), <span class="string">'.'</span>, <span class="string">'Color'</span>, col(:,i), <span class="string">'MarkerSize'</span>, ms);
    lgd{end+1} = num2str(c(i));
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
legend(lgd);
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_12.png"> <p><i>Exercice 3:</i> (<a href="../missing-exo/">check the solution</a>) Display evolution of the classification probability with \(\sigma\)
         </p><pre class="codeinput">exo3;
</pre><img vspace="5" hspace="5" src="index_13.png"> <p><i>Exercice 4:</i> (<a href="../missing-exo/">check the solution</a>) Separate the dataset into a training set and a testing set. Evaluate the classification performance for varying \(\si\).
            Try to introduce regularization and minmize \[ \umin{h} F(h) \eqdef \Ll(K h,y) + \la R(h) \] where for instance \(R=\norm{\cdot}_2^2\)
            or  \(R=\norm{\cdot}_1\).
         </p><pre class="codeinput">exo4;
</pre><h2>Multi-Classes Logistic Classification<a name="57"></a></h2>
         <p>The logistic classification method is extended to an arbitrary number \(k\) of classes by considering a familly of weight
            vectors \( w_\ell \)_{\ell=1}^k, which are conveniently stored as columns of matrix \(W \in \RR^{n \times k}\).
         </p>
         <p>This allows to model probabilitically the belonging of a point \(x \in \RR^n \) to a the classes using an exponential model
              \[ h(x) = \pa{ \frac{ e^{-\dotp{x}{w_\ell}} }{ \sum_m e^{-\dotp{x}{w_m}} } }_\ell \] This vector \(h(x) \in [0,1]^k \) describes
            the probability of \(x\) belonging to the different classes, and \( \sum_\ell h(x)_\ell = 1 \).
         </p>
         <p>The computation of \(w\) is obtained by solving a maximum likelihood estimator    \[ \umax{w \in \RR^k} \frac{1}{p} \sum_{i=1}^p
            \log( h(x_i)_{y_i} ) \] where we recall that \(y_i \in \{1,\ldots,k\}\) is the class index of point \(x_i\).
         </p>
         <p>This is conveniently rewritten as   \[ \umin{w} \sum_i \text{LSE}( XW )_i - \dotp{XW}{D} \] where \(D \in \{0,1\}^{p \times
            k}\) is the binary class index matrices   \[  D_{i,\ell} = \choice{           1 \qifq y_i=\ell, \\           0 \text{otherwise}.
                  }    \] and LSE is the log-sum-exp operator   \[ \text{LSE}(S) = \log\pa{ \sum_\ell \exp(S_{i,\ell}) } \in \RR^p. \]
         </p><pre class="codeinput">LSE = @(S)log( sum(exp(S), 2) );
</pre><p>The computation of LSE is unstable for large value of \(S\) (numerical overflow, producing NaN), but this can be fixed by
            substrating the largest element in each row, since \( \text{LSE}(S+a)=\text{LSE}(S)+a \) if \(a\) is constant along rows.
            This is the <a href="https://en.wikipedia.org/wiki/LogSumExp">celebrated LSE trick</a>.
         </p><pre class="codeinput">max2 = @(S)repmat(max(S,[],2), [1 size(S,2)]);
LSE = @(S)LSE( S-max2(S) ) + max(S,[],2);
</pre><p>The gradient of the LSE operator is the <a href="https://en.wikipedia.org/wiki/Softmax_function">soft-max operator</a> \[  \nabla \text{LSE}(S) = SM(S) \eqdef       \pa{           \frac{                   e^{S_{i,\ell}}               }{   
                           \sum_m e^{S_{i,m}}               } }   \]
         </p><pre class="codeinput">SM = @(S)exp(S) ./ repmat( sum(exp(S),2), [1 size(S,2)]);
</pre><p>Similarely to the LSE, it needs to be stabilized.</p><pre class="codeinput">SM = @(S)SM(S-max2(S));
</pre><p>Retrieve the full dataset (with all the classes).</p><pre class="codeinput">X = Xsvg;
y = ysvg;
p = length(y);
</pre><p>Compute the \(D\) matrix.</p><pre class="codeinput">D = double( repmat(1:k, [p,1]) == repmat(y, [1,k]) );
</pre><p>Define the energy \(E(W)\).</p><pre class="codeinput">dotp = @(u,v)sum(u(:).*v(:));
E = @(W)1/p*( sum(LSE(X*W)) - dotp(X*W,D)  );
</pre><p>Define its gradients   \[ \nabla E(W) =  \frac{1}{p} X^\top ( \text{SM}(X W) - D ).  \]</p><pre class="codeinput">nablaE = @(W)1/p * X'* ( SM(X*W) -  D );
</pre><p><i>Exercice 5:</i> (<a href="../missing-exo/">check the solution</a>) Implement a gradient descent  \[ W_{\ell+1} = W_\ell - \tau_\ell \nabla E(W_\ell). \] Monitor the energy decay.
         </p><pre class="codeinput">exo5;
</pre><img vspace="5" hspace="5" src="index_14.png"> <p>Generate a 2D grid of points over PCA space and map it to feature space.</p><pre class="codeinput">[U,D,V] = svd(Xm(X),<span class="string">'econ'</span>);
Z = Xm(X) * V;
M = max(abs(Z(:)));
q = 201;
t = linspace(-M,M,q);
[B,A] = meshgrid(t,t);
G = zeros(q*q,n);
G(:,1:2) = [A(:), B(:)];
G = G*V' + repmat( mean(X,1), [q*q 1] );
</pre><p>Evaluate class probability associated to weight vectors on this grid.</p><pre class="codeinput">Theta = SM(G*W);
Theta = reshape(Theta, [q q k]);
</pre><p>Display each probablity map.</p><pre class="codeinput">clf;
<span class="keyword">for</span> i=1:k
    subplot(3,4,i);
    imagesc(Theta(:,:,i)');
    title([<span class="string">'Class '</span> num2str(i)]);
    axis <span class="string">image</span>; axis <span class="string">off</span>;
    colormap <span class="string">jet(256)</span>;
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="index_15.png"> <p>Build a single color image of this map.</p><pre class="codeinput">R = zeros(q,q,3);
<span class="keyword">for</span> i=1:k
    <span class="keyword">for</span> a=1:3
        R(:,:,a) = R(:,:,a) + Theta(:,:,i) .* col(a,i);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p>Display.</p><pre class="codeinput">clf; hold <span class="string">on</span>;
imagesc(t, t, permute(R, [2 1 3]));
lgd = {};
<span class="keyword">for</span> i=1:k
    I = find(y==i);
    plot(Z(I,1), Z(I,2), <span class="string">'o'</span>, <span class="string">'MarkerFaceColor'</span>, col(:,i), <span class="string">'MarkerSize'</span>, 5, <span class="string">'MarkerEdgeColor'</span>, col(:,i)/4);
    lgd{end+1} = num2str(i);
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
legend(lgd, <span class="string">'Location'</span>, <span class="string">'EastOutside'</span>); axis <span class="string">off</span>;
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_16.png"> <p><i>Exercice 6:</i> (<a href="../missing-exo/">check the solution</a>) Separate the dataset into a training set and a testing set. Evaluate the classification performance and display the confusion
            matrix. You can try the impact of kernlization and regularization.
         </p><pre class="codeinput">exo6;
</pre><p class="footer"><br>
            Copyright  (c) 2010 Gabriel Peyre<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Logistic Classification
% This tour details the logistic classification method (for 2 classes and
% multi-classes).

%%
% _Warning:_ Logisitic classification is actually called <https://en.wikipedia.org/wiki/Logistic_regression "logistic
% regression"> in the literature, but it is in fact a classification method.

%%
% We recommend that after doing this Numerical Tours, you apply it to your
% own data, for instance using a dataset from <https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ LibSVM>.

%%
% _Disclaimer:_ these machine learning tours are intended to be
% overly-simplistic implementations and applications of baseline machine learning methods. 
% For more advanced uses and implementations, we recommend
% to use a state-of-the-art library, the most well known being
% <http://scikit-learn.org/ Scikit-Learn>

%% Installing toolboxes and setting up the path.

%%
% You need to download the following files: 
% <../toolbox_general.zip general toolbox>.

%%
% You need to unzip these toolboxes in your working directory, so
% that you have 
% |toolbox_general|
% in your directory.

%%
% *For Scilab user:* you must replace the Matlab comment '%' by its Scilab
% counterpart '//'.

%%
% *Recommandation:* You should create a text file named for instance |numericaltour.sce| (in Scilab) or |numericaltour.m| (in Matlab) to write all the
% Scilab/Matlab command you want to execute. Then, simply run |exec('numericaltour.sce');| (in Scilab) or |numericaltour;| (in Matlab) to run the commands. 

%%
% Execute this line only if you are using Matlab.

getd = @(p)path(p,path); % scilab users must *not* execute this

%%
% Then you can add the toolboxes to the path.

getd('toolbox_general/');

%% Dataset Loading
% We load a dataset of \(p\) images of size \(n = 8 \times 8\), representing digits from 0
% to 9 (so there are \(k=10\) classes). 

%%
% First define a few helpers.

SetAR = @(ar)set(gca, 'PlotBoxAspectRatio', [1 ar 1], 'FontSize', 20);
Xm = @(X)X-repmat(mean(X,1), [size(X,1) 1]);
Cov = @(X)Xm(X)'*Xm(X);

%%
% Load the dataset.

name = 'digits';
load(['ml-' name]);

%%
% Randomly permute it.

A = A(randperm(size(A,1)),:);

%%
% Separate the features \(X\) from the data \(y\) to predict information.

X = A(:,1:end-1);
y = A(:,end);
y = y-min(y)+1;
k = max(y);

%%
% \(p\) is the number of samples, \(n\) is the dimensionality of the features,

[p,n] = size(X);

%%
% Display a few samples digits

q = 5;
clf;
for i=1:k
    I = find(y==i);
    for j=1:q
        f = reshape(X(I(j),:), sqrt(n)*[1 1])';
        subplot(q,k, (j-1)*k+i );
        imagesc(-f); axis image; axis off;
    end
end
colormap gray(256);

%% Dimenionality Reduction and PCA
% In order to display in 2D or 3D the data, dimensionality is needed.
% The simplest method is the principal component analysis, which perform an
% orthogonal linear projection on the principal axsis (eigenvector) of the
% covariance matrix. 

%%
% Display the covariance matrix \(C \in \RR^{n \times n}\).

clf; imagesc( Cov(X) );

%%
% Compute PCA ortho-basis. 

[U,D,V] = svd(Xm(X),'econ');

%%
% Compute the feature in the PCA basis.

Z = Xm(X) * V;

%%
% Plot sqrt of the eigenvalues.

clf; 
plot(diag(D), '.-', 'LineWidth', 2, 'MarkerSize', 30);
axis tight;
SetAR(1/2);

%%
% Display in 2D.

col = [ [1 0 0]; [0 1 0]; [0 0 1]; [0 0 0]; [0 1 1]; [1 0 1]; [1 1 0]; ...
    [1 .5 .5]; [.5 1 .5]; [.5 .5 1]  ]';
ms = 25;
clf; hold on;
lgd = {};
for i=1:min(k,size(col,2))
    I = find(y==i);
    plot(Z(I,1), Z(I,2), '.', 'Color', col(:,i), 'MarkerSize', ms);
    lgd{end+1} = num2str(i);
end
axis tight; axis equal; box on;
legend(lgd, 'Location', 'EastOutside');
SetAR(1);


%%
% Display in 3D.

col = [ [1 0 0]; [0 1 0]; [0 0 1]; [0 0 0]; [0 1 1]; [1 0 1]; [1 1 0]; ...
    [1 .5 .5]; [.5 1 .5]; [.5 .5 1]  ]';
ms = 25;
clf; hold on;
lgd = {};
for i=1:min(k,size(col,2))
    I = find(y==i);
    plot3(Z(I,1), Z(I,2), Z(I,3), '.', 'Color', col(:,i), 'MarkerSize', ms);
    lgd{end+1} = num2str(i);
end
axis tight; axis equal; box on;
view(3)
legend(lgd, 'Location', 'EastOutside');
SetAR(1);


%% Two Classes Logistic Classification
% Logistic classification is, with <https://en.wikipedia.org/wiki/Support_vector_machine support vector machine (SVM)>, the baseline
% method to perform classification. Its main advantage over SVM is that is
% is a smooth minimization problem, and that it also output class
% probabity, offering a probabilistic interpretation of the classification.

%%
% Select only two classes. Here classes indexes are set to \(y_i \in
% \{-1,1\}\) to simplify the equations.

c = [1 2]; % selected classes
c = [1 6];
Xsvg = X; ysvg = y; 
X = []; y = [];
for i=1:2
    I = find(ysvg==c(i));
    X = [X; Xsvg(I,:)];
    y = [y; ones(length(I),1)*(2*i-3)];
end
p = size(X,1);

%%
% Plot only the two classes.

[U,D,V] = svd(Xm(X),'econ');
Z = Xm(X) * V;
clf; hold on;
lgd = {};
for i=1:2
    I = find(y==2*i-3);
    plot(Z(I,1), Z(I,2), '.', 'Color', col(:,i), 'MarkerSize', ms);
    lgd{end+1} = num2str(c(i));
end
axis tight; axis equal; box on;
legend(lgd, 'Location', 'EastOutside');
SetAR(1);

%%
% Logistic classification minimize a logistic loss in place of the usual
% \(\ell^2\) loss for regression
%   \[ \umin{w} E(w) \eqdef \frac{1}{p} \sum_{i=1}^p L(\dotp{x_i}{w},y_i)  \]
% where the logistic loss reads
%   \[ L( s,y ) \eqdef \log( 1+\exp(-sy) ) \]
% This corresponds to a smooth convex minimization. If \(X\) is injective,
% this is also strictly convex, hence it has a single global minimum. 

%%
% Compare the binary (ideal) 0-1 loss, the logistic loss and the
% <https://en.wikipedia.org/wiki/Hinge_loss hinge loss>
% (the one used for SVM).

t = linspace(-3,3,255)';
clf;
plot(t, [t>0, log(1+exp(t)), max(t,0)], 'LineWidth', 2 );
axis tight;
legend('Binary', 'Logistic', 'Hinge', 'Location', 'NorthWest');
SetAR(1/2);

%%
% This can be interpreted as a <https://en.wikipedia.org/wiki/Maximum_likelihood_estimation maximum likelihood estimator> when one
% models the probability of  belonging to the two classes for sample \(x_i\) as
%   \[ h(x_i) \eqdef (\th(x_i),1-\th(x_i)) \qwhereq 
%           \th(s) \eqdef \frac{e^{s}}{1+e^s} = (1+e^{-s})^{-1}  \]

%%
% Re-writting the energy to minimize
%   \[ E(w) = \Ll(X w,y) \qwhereq \Ll(s,y)= \frac{1}{p}  \sum_i L(s_i,y_i), \]
% its gradient reads 
%   \[ \nabla E(w) = X^\top \nabla \Ll(X w,y)  
%       \qwhereq
%       \nabla \Ll(s,y) = \frac{y}{p} \odot \th(-y \odot s),   \]
% where \(\odot\) is the pointwise multiplication operator, i.e. |.*| in
% Matlab.

%%
% Define the energies.

L = @(s,y)1/p * sum( log( 1 + exp(-s.*y) ) );
E = @(w)L(X*w,y);

%% 
% Define their gradients.

theta = @(v)exp(-v) ./ (1+exp(-v));
nablaL = @(s,r)- 1/p * y.* theta(s.*y);
nablaE = @(w)X'*nablaL(X*w,y);

%%
% _Exercice 1:_ (<../missing-exo/ check the solution>)
% Implement a gradient descent
%  \[ w_{\ell+1} = w_\ell - \tau_\ell \nabla E(w_\ell). \]
% Monitor the energy decay.
% Test different step size, and compare with the theory (in particular
% plot in log domain to illustrate the linear rate).

exo1;

%%
% Display the weight vector \(w\).

clf;
imagesc(reshape(w,sqrt(n)*[1 1])');
axis image; axis off;
colormap jet(256);
colorbar;

%%
% Generate a 2D grid of points over PCA space and map it to feature space.

M = max(abs(Z(:)));
q = 101;
t = linspace(-M,M,q);
[B,A] = meshgrid(t,t);
G = zeros(q*q,n);
G(:,1:2) = [A(:), B(:)];
G = G*V' + repmat( mean(X,1), [q*q 1] );

%% 
% Evaluate class probability associated to weight vectors on this grid.

Theta = theta(G*w);
Theta = reshape(Theta, [q q]);


%%
% Display the data overlaid on top of the 
% classification probability, this highlight the
% separating hyperplane?\( \enscond{x}{\dotp{w}{x}=0} \).

clf; hold on;
imagesc(t,t, Theta');
lgd = {};
for i=1:2
    I = find(y==2*i-3);
    plot(Z(I,1), Z(I,2), '.', 'Color', col(:,i), 'MarkerSize', ms);
    lgd{end+1} = num2str(c(i));
end
axis tight; axis equal; box on;
legend(lgd);
SetAR(1);

%% Kernelized Logistic Classification
% Logistic classification tries to separate the classes using
% a linear separating hyperplane \( \enscond{x}{\dotp{w}{x}=0}. \)

%%
% In order to generate a non-linear descision boundary, one can replace the
% parametric linear model by a non-linear <https://en.wikipedia.org/wiki/Nonparametric_statistics non-parametric> model, thanks to
% kernelization. It is non-parametric in the sense that the number of
% parameter grows with the number \(p\) of sample (while for the basic
% method, the number of parameter is \(n\). This allows in particular to
% generate decision boundary of arbitrary complexity. 

%%
% The downside is that the numerical complexity of the method grows
% (at least) quadratically with \(p\).

%%
% The good news however is that thanks to the theory of 
% <https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space reproducing kernel Hilbert spaces>
% (RKHS), one can still compute this non-linear decision
% function using (almost) the same numerical algorithm.

%%
% Given a kernel \( \kappa(x,z) \in \RR \) defined for \((x,z) \in \RR^n\),
% the kernelized method replace the linear decision functional \(f(x) =
% \dotp{x}{w}\) by a sum of kernel centered on the samples 
% \[ f_h(x) = \sum_{i=1}^n h_i k(x_i,x) \]
% where \(h \in \RR^p\) is the unknown vector of weight to find. 

%%
% When using the linear kernel \(\kappa(x,y)=\dotp{x}{y}\), one retrieves
% the previously studied linear method. 

%% 
% Macro to compute pairwise squared Euclidean distance matrix.

distmat = @(X,Z)bsxfun(@plus,dot(X',X',1)',dot(Z',Z',1))-2*(X*Z');


%%
% The gaussian kernel is the most well known and used kernel
% \[ \kappa(x,y) \eqdef e^{-\frac{\norm{x-y}^2}{2\sigma^2}} . \]
% The bandwidth parameter \(\si>0\) is crucial and controls the locality of
% the model. It is typically tuned through cross validation.  

sigma = 10;
kappa = @(X,Z)exp( -distmat(X,Z)/(2*sigma^2) );


%%
% Once avaluated on grid points, the kernel define a matrix
% \[ K = (\kappa(x_i,x_j))_{i,j=1}^p \in \RR^{p \times p}.  \]

K = kappa(X,X);

%%
% Valid kernels are those that gives rise to positive symmetric matrices
% \(K\). The linear and Gaussian kernel are valid kernel functions. Other
% popular kernels include the polynomial kernel \( \dotp{x}{y}^a \) for \(a
% \geq 1\) and the Laplacian kernel \( \exp( -\norm{x-y}^2/\si ) \). 

%%
% The kernelized Logistic minimization reads
%   \[ \umin{h} F(h) \eqdef \Ll(K h,y). \] 

F = @(h)L(K*h,y);
nablaF = @(h)K'*nablaL(K*h,y);

%%
% This minimization can be related to an infinite dimensional optimization
% problem where one minimizes directly over the function \(f\). This
% is shown to be equivalent to the above finite-dimenisonal optimization problem
% thanks to the theory of RKHS. 

%%
% _Exercice 2:_ (<../missing-exo/ check the solution>)
% Implement a gradient descent to minimize \(F(h)\).
% Monitor the energy decay.
% Test different step size, and compare with the theory.

exo2;


%%
% Once this optimal \(h\) has been found, class probability at a point
% \(x\) are obtained as
%   \[ (\th(f_h(x)), 1-\th(f_h(x)) \]
% where \(f_h\) has been defined above.

%%
% We evaluate this classification probability on a grid.

Theta = theta(kappa(G,X)*h);
Theta = reshape(Theta, [q,q]);

%%
% Display the classification probability.

clf; hold on;
imagesc(t,t, Theta');
lgd = {};
for i=1:2
    I = find(y==2*i-3);
    plot(Z(I,1), Z(I,2), '.', 'Color', col(:,i), 'MarkerSize', ms);
    lgd{end+1} = num2str(c(i));
end
axis tight; axis equal; box on;
legend(lgd);
SetAR(1);

%%
% _Exercice 3:_ (<../missing-exo/ check the solution>)
% Display evolution of the classification probability with \(\sigma\)

exo3;

%%
% _Exercice 4:_ (<../missing-exo/ check the solution>)
% Separate the dataset into a training set and a testing set. Evaluate the classification performance 
% for varying \(\si\). Try to introduce regularization and minmize
% \[ \umin{h} F(h) \eqdef \Ll(K h,y) + \la R(h) \]
% where for instance \(R=\norm{\cdot}_2^2\) or  \(R=\norm{\cdot}_1\).

exo4;



%% Multi-Classes Logistic Classification
% The logistic classification method is extended to an arbitrary number
% \(k\) of classes by considering a familly of weight vectors \( w_\ell
% \)_{\ell=1}^k, which are conveniently stored as columns of matrix \(W \in \RR^{n \times k}\).

%%
% This allows to model probabilitically the belonging of a point \(x \in \RR^n \) to a
% the classes using an exponential model
%   \[ h(x) = \pa{ \frac{ e^{-\dotp{x}{w_\ell}} }{ \sum_m e^{-\dotp{x}{w_m}} } }_\ell \]
% This vector \(h(x) \in [0,1]^k \) describes the probability of \(x\)
% belonging to the different classes, and \( \sum_\ell h(x)_\ell = 1 \).

%%
% The computation of \(w\) is obtained by solving a maximum likelihood
% estimator
%    \[ \umax{w \in \RR^k} \frac{1}{p} \sum_{i=1}^p \log( h(x_i)_{y_i} ) \]
% where we recall that \(y_i \in \{1,\ldots,k\}\) is the class index of
% point \(x_i\).

%%
% This is conveniently rewritten as
%   \[ \umin{w} \sum_i \text{LSE}( XW )_i - \dotp{XW}{D} \]
% where \(D \in \{0,1\}^{p \times k}\) is the binary class index matrices
%   \[  D_{i,\ell} = \choice{
%           1 \qifq y_i=\ell, \\
%           0 \text{otherwise}. 
%       }
%    \]
% and LSE is the log-sum-exp operator
%   \[ \text{LSE}(S) = \log\pa{ \sum_\ell \exp(S_{i,\ell}) } \in \RR^p. \]

LSE = @(S)log( sum(exp(S), 2) );

%%
% The computation of LSE is
% unstable for large value of \(S\) (numerical overflow, producing NaN), but this can be
% fixed by substrating the largest element in each row, 
% since \( \text{LSE}(S+a)=\text{LSE}(S)+a \) if \(a\) is constant along rows. This is
% the <https://en.wikipedia.org/wiki/LogSumExp celebrated LSE trick>. 

max2 = @(S)repmat(max(S,[],2), [1 size(S,2)]);
LSE = @(S)LSE( S-max2(S) ) + max(S,[],2);

%%
% The gradient of the LSE operator is the
% <https://en.wikipedia.org/wiki/Softmax_function soft-max operator>
% \[  \nabla \text{LSE}(S) = SM(S) \eqdef
%       \pa{ 
%           \frac{
%                   e^{S_{i,\ell}}
%               }{  
%                   \sum_m e^{S_{i,m}}
%               } }   \]

SM = @(S)exp(S) ./ repmat( sum(exp(S),2), [1 size(S,2)]);

%%
% Similarely to the LSE, it needs to be stabilized.

SM = @(S)SM(S-max2(S));

%%
% Retrieve the full dataset (with all the classes).

X = Xsvg;
y = ysvg;
p = length(y);

%%
% Compute the \(D\) matrix.

D = double( repmat(1:k, [p,1]) == repmat(y, [1,k]) );

%%
% Define the energy \(E(W)\).

dotp = @(u,v)sum(u(:).*v(:));
E = @(W)1/p*( sum(LSE(X*W)) - dotp(X*W,D)  );

%% 
% Define its gradients
%   \[ \nabla E(W) =  \frac{1}{p} X^\top ( \text{SM}(X W) - D ).  \]

nablaE = @(W)1/p * X'* ( SM(X*W) -  D );

%%
% _Exercice 5:_ (<../missing-exo/ check the solution>)
% Implement a gradient descent
%  \[ W_{\ell+1} = W_\ell - \tau_\ell \nabla E(W_\ell). \]
% Monitor the energy decay.

exo5;

%%
% Generate a 2D grid of points over PCA space and map it to feature space.

[U,D,V] = svd(Xm(X),'econ');
Z = Xm(X) * V;
M = max(abs(Z(:)));
q = 201;
t = linspace(-M,M,q);
[B,A] = meshgrid(t,t);
G = zeros(q*q,n);
G(:,1:2) = [A(:), B(:)];
G = G*V' + repmat( mean(X,1), [q*q 1] );

%% 
% Evaluate class probability associated to weight vectors on this grid.

Theta = SM(G*W);
Theta = reshape(Theta, [q q k]);

%%
% Display each probablity map.

clf;
for i=1:k
    subplot(3,4,i);
    imagesc(Theta(:,:,i)');
    title(['Class ' num2str(i)]);
    axis image; axis off;
    colormap jet(256);
end

%%
% Build a single color image of this map.

R = zeros(q,q,3);
for i=1:k
    for a=1:3
        R(:,:,a) = R(:,:,a) + Theta(:,:,i) .* col(a,i);
    end
end

%%
% Display.    

clf; hold on;
imagesc(t, t, permute(R, [2 1 3])); 
lgd = {};
for i=1:k
    I = find(y==i);
    plot(Z(I,1), Z(I,2), 'o', 'MarkerFaceColor', col(:,i), 'MarkerSize', 5, 'MarkerEdgeColor', col(:,i)/4);
    lgd{end+1} = num2str(i);
end
axis tight; axis equal; box on;
legend(lgd, 'Location', 'EastOutside'); axis off;
SetAR(1);


%%
% _Exercice 6:_ (<../missing-exo/ check the solution>)
% Separate the dataset into a training set and a testing set. Evaluate the classification performance 
% and display the confusion matrix. You can try the impact of kernlization and regularization. 

exo6;

##### SOURCE END #####
-->
   </body>
</html>