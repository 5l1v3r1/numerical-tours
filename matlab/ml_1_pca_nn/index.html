
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><p style="font-size:0px">
         \[
         \newcommand{\NN}{\mathbb{N}}
         \newcommand{\CC}{\mathbb{C}}
         \newcommand{\GG}{\mathbb{G}}
         \newcommand{\LL}{\mathbb{L}}
         \newcommand{\PP}{\mathbb{P}}
         \newcommand{\QQ}{\mathbb{Q}}
         \newcommand{\RR}{\mathbb{R}}
         \newcommand{\VV}{\mathbb{V}}
         \newcommand{\ZZ}{\mathbb{Z}}
         \newcommand{\FF}{\mathbb{F}}
         \newcommand{\KK}{\mathbb{K}}
         \newcommand{\UU}{\mathbb{U}}
         \newcommand{\EE}{\mathbb{E}}
         
         \newcommand{\Aa}{\mathcal{A}}
         \newcommand{\Bb}{\mathcal{B}}
         \newcommand{\Cc}{\mathcal{C}}
         \newcommand{\Dd}{\mathcal{D}}
         \newcommand{\Ee}{\mathcal{E}}
         \newcommand{\Ff}{\mathcal{F}}
         \newcommand{\Gg}{\mathcal{G}}
         \newcommand{\Hh}{\mathcal{H}}
         \newcommand{\Ii}{\mathcal{I}}
         \newcommand{\Jj}{\mathcal{J}}
         \newcommand{\Kk}{\mathcal{K}}
         \newcommand{\Ll}{\mathcal{L}}
         \newcommand{\Mm}{\mathcal{M}}
         \newcommand{\Nn}{\mathcal{N}}
         \newcommand{\Oo}{\mathcal{O}}
         \newcommand{\Pp}{\mathcal{P}}
         \newcommand{\Qq}{\mathcal{Q}}
         \newcommand{\Rr}{\mathcal{R}}
         \newcommand{\Ss}{\mathcal{S}}
         \newcommand{\Tt}{\mathcal{T}}
         \newcommand{\Uu}{\mathcal{U}}
         \newcommand{\Vv}{\mathcal{V}}
         \newcommand{\Ww}{\mathcal{W}}
         \newcommand{\Xx}{\mathcal{X}}
         \newcommand{\Yy}{\mathcal{Y}}
         \newcommand{\Zz}{\mathcal{Z}}
         
         \newcommand{\al}{\alpha}
         \newcommand{\la}{\lambda}
         \newcommand{\ga}{\gamma}
         \newcommand{\Ga}{\Gamma}
         \newcommand{\La}{\Lambda}
         \newcommand{\Si}{\Sigma}
         \newcommand{\si}{\sigma}
         \newcommand{\be}{\beta}
         \newcommand{\de}{\delta}
         \newcommand{\De}{\Delta}
         \renewcommand{\phi}{\varphi}
         \renewcommand{\th}{\theta}
         \newcommand{\om}{\omega}
         \newcommand{\Om}{\Omega}
         \renewcommand{\epsilon}{\varepsilon}
         
         \newcommand{\Calpha}{\mathrm{C}^\al}
         \newcommand{\Cbeta}{\mathrm{C}^\be}
         \newcommand{\Cal}{\text{C}^\al}
         \newcommand{\Cdeux}{\text{C}^{2}}
         \newcommand{\Cun}{\text{C}^{1}}
         \newcommand{\Calt}[1]{\text{C}^{#1}}
         
         \newcommand{\lun}{\ell^1}
         \newcommand{\ldeux}{\ell^2}
         \newcommand{\linf}{\ell^\infty}
         \newcommand{\ldeuxj}{{\ldeux_j}}
         \newcommand{\Lun}{\text{\upshape L}^1}
         \newcommand{\Ldeux}{\text{\upshape L}^2}
         \newcommand{\Lp}{\text{\upshape L}^p}
         \newcommand{\Lq}{\text{\upshape L}^q}
         \newcommand{\Linf}{\text{\upshape L}^\infty}
         \newcommand{\lzero}{\ell^0}
         \newcommand{\lp}{\ell^p}
         
         
         \renewcommand{\d}{\ins{d}}
         
         \newcommand{\Grad}{\text{Grad}}
         \newcommand{\grad}{\text{grad}}
         \renewcommand{\div}{\text{div}}
         \newcommand{\diag}{\text{diag}}
         
         \newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
         \newcommand{\pdd}[2]{ \frac{ \partial^2 #1}{\partial #2^2} }
         
         \newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
         \newcommand{\norm}[1]{|\!| #1 |\!|}
         \newcommand{\normi}[1]{\norm{#1}_{\infty}}
         \newcommand{\normu}[1]{\norm{#1}_{1}}
         \newcommand{\normz}[1]{\norm{#1}_{0}}
         \newcommand{\abs}[1]{\vert #1 \vert}
         
         \newcommand{\argmin}{\text{argmin}}
         \newcommand{\argmax}{\text{argmax}}
         \newcommand{\uargmin}[1]{\underset{#1}{\argmin}\;}
         \newcommand{\uargmax}[1]{\underset{#1}{\argmax}\;}
         \newcommand{\umin}[1]{\underset{#1}{\min}\;}
         \newcommand{\umax}[1]{\underset{#1}{\max}\;}
         
         \newcommand{\pa}[1]{\left( #1 \right)}
         \newcommand{\choice}[1]{ \left\{  \begin{array}{l} #1 \end{array} \right. }
         
         \newcommand{\enscond}[2]{ \left\{ #1 \;:\; #2 \right\} }
         
         \newcommand{\qandq}{ \quad \text{and} \quad }
         \newcommand{\qqandqq}{ \qquad \text{and} \qquad }
         \newcommand{\qifq}{ \quad \text{if} \quad }
         \newcommand{\qqifqq}{ \qquad \text{if} \qquad }
         \newcommand{\qwhereq}{ \quad \text{where} \quad }
         \newcommand{\qqwhereqq}{ \qquad \text{where} \qquad }
         \newcommand{\qwithq}{ \quad \text{with} \quad }
         \newcommand{\qqwithqq}{ \qquad \text{with} \qquad }
         \newcommand{\qforq}{ \quad \text{for} \quad }
         \newcommand{\qqforqq}{ \qquad \text{for} \qquad }
         \newcommand{\qqsinceqq}{ \qquad \text{since} \qquad }
         \newcommand{\qsinceq}{ \quad \text{since} \quad }
         \newcommand{\qarrq}{\quad\Longrightarrow\quad}
         \newcommand{\qqarrqq}{\quad\Longrightarrow\quad}
         \newcommand{\qiffq}{\quad\Longleftrightarrow\quad}
         \newcommand{\qqiffqq}{\qquad\Longleftrightarrow\qquad}
         \newcommand{\qsubjq}{ \quad \text{subject to} \quad }
         \newcommand{\qqsubjqq}{ \qquad \text{subject to} \qquad }
         
         \newcommand{\eqdef}{\equiv}
         \]
         
      </p>
      <title>PCA, Nearest-Neighbors and Clustering</title>
      <NOSCRIPT>
         <DIV STYLE="color:#CC0000; text-align:center"><B>Warning: <A HREF="http://www.math.union.edu/locate/jsMath">jsMath</A>
               	requires JavaScript to process the mathematics on this page.<BR>
               	If your browser supports JavaScript, be sure it is enabled.</B></DIV>
         <HR>
      </NOSCRIPT>
      <meta name="generator" content="MATLAB 9.0">
      <meta name="date" content="2017-08-08">
      <meta name="m-file" content="index">
      <LINK REL="stylesheet" HREF="../style.css" TYPE="text/css">
   </head>
   <body>
      <div class="content">
         <h1>PCA, Nearest-Neighbors and Clustering</h1>
         <introduction>
            <p>Test for PCA (visualization), NN classification and K-means on the IRIS dataset.</p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Installing toolboxes and setting up the path.</a></li>
               <li><a href="#8">Dataset Loading</a></li>
               <li><a href="#14">Dimenionality Reduction and PCA</a></li>
               <li><a href="#22">Supervised Learning: Nearest Neighbor Classification</a></li>
               <li><a href="#31">Unsupervised Learning: K-means</a></li>
            </ul>
         </div>
         <h2>Installing toolboxes and setting up the path.<a name="1"></a></h2>
         <p>You need to download the following files: <a href="../toolbox_general.zip">general toolbox</a>.
         </p>
         <p>You need to unzip these toolboxes in your working directory, so that you have <tt>toolbox_general</tt> in your directory.
         </p>
         <p><b>For Scilab user:</b> you must replace the Matlab comment '%' by its Scilab counterpart '//'.
         </p>
         <p><b>Recommandation:</b> You should create a text file named for instance <tt>numericaltour.sce</tt> (in Scilab) or <tt>numericaltour.m</tt> (in Matlab) to write all the Scilab/Matlab command you want to execute. Then, simply run <tt>exec('numericaltour.sce');</tt> (in Scilab) or <tt>numericaltour;</tt> (in Matlab) to run the commands.
         </p>
         <p>Execute this line only if you are using Matlab.</p><pre class="codeinput">getd = @(p)path(p,path); <span class="comment">% scilab users must *not* execute this</span>
</pre><p>Then you can add the toolboxes to the path.</p><pre class="codeinput">getd(<span class="string">'toolbox_general/'</span>);
</pre><h2>Dataset Loading<a name="8"></a></h2>
         <p>Helpers.</p><pre class="codeinput">SetAR = @(ar)set(gca, <span class="string">'PlotBoxAspectRatio'</span>, [1 ar 1], <span class="string">'FontSize'</span>, 20);
</pre><p>Load the dataset.</p><pre class="codeinput">name = <span class="string">'iris'</span>;
load([<span class="string">'ml-'</span> name]);
</pre><p>Randomly permute it.</p><pre class="codeinput">A = A(randperm(size(A,1)),:);
</pre><p>Separate the features from the class information. Be sure to start the class at index 1.</p><pre class="codeinput">X = A(:,1:end-1);
y = A(:,end);
y = y-min(y)+1;
</pre><p>\(p\) is the number of samples, \(n\) is the dimensionality of the features, \(k\) is the number of classes.</p><pre class="codeinput">[p,n] = size(X);
k = max(y);
</pre><h2>Dimenionality Reduction and PCA<a name="14"></a></h2>
         <p>In order to display in 2D or 3D the data, dimensionality is needed. The simplest method is the principal component analysis,
            which perform an orthogonal linear projection on the principal axsis (eigenvector) of the covariance matrix.
         </p>
         <p>Compute empirical mean \(m \in \RR^n\) and covariance \(C \in \RR^{n \times n}\).</p><pre class="codeinput">m = mean(X,1);
Xm = X-repmat(m, [p 1]);
C = Xm'*Xm;
</pre><p>Display the covariance matrix.</p><pre class="codeinput">clf;
imagesc(C);
</pre><img vspace="5" hspace="5" src="index_01.png"> <p>Compute PCA ortho-basis.</p><pre class="codeinput">[U,D] = eig(C);
[d,I] = sort(diag(D), <span class="string">'descend'</span>);
U = U(:,I);
</pre><p>Compute the feature in the PCA basis.</p><pre class="codeinput">z = (U'*Xm')';
</pre><p>Plot sqrt of the eigenvalues.</p><pre class="codeinput">clf;
plot(sqrt(d), <span class="string">'.-'</span>, <span class="string">'LineWidth'</span>, 2, <span class="string">'MarkerSize'</span>, 30);
axis <span class="string">tight</span>;
SetAR(1/2);
</pre><img vspace="5" hspace="5" src="index_02.png"> <p>Display in 2D.</p><pre class="codeinput">col = {<span class="string">'b'</span> <span class="string">'g'</span> <span class="string">'r'</span> <span class="string">'c'</span> <span class="string">'m'</span> <span class="string">'y'</span> <span class="string">'k'</span>};
ms = 25;
clf; hold <span class="string">on</span>;
<span class="keyword">for</span> i=1:k
    I = find(y==i);
    plot(z(I,1), z(I,2), <span class="string">'.'</span>, <span class="string">'Color'</span>, col{i}, <span class="string">'MarkerSize'</span>, ms);
<span class="keyword">end</span>
axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_03.png"> <p>Display in 2D.</p><pre class="codeinput">clf; hold <span class="string">on</span>;
<span class="keyword">for</span> i=1:k
    I = find(y==i);
    plot3(z(I,1), z(I,2), z(I,3), <span class="string">'.'</span>, <span class="string">'Color'</span>, col{i}, <span class="string">'MarkerSize'</span>, ms);
<span class="keyword">end</span>
view(3); axis <span class="string">tight</span>; axis <span class="string">equal</span>; box <span class="string">on</span>;
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_04.png"> <h2>Supervised Learning: Nearest Neighbor Classification<a name="22"></a></h2>
         <p>Split into training and testing.</p><pre class="codeinput">p0 = round(.5*p);
p1 = p-p0;

X0 = X(1:p0,:);     y0 = y(1:p0);
X1 = X(p0+1:end,:); y1 = y(p0+1:end);
</pre><p>Macro to compute pairwise squared Euclidean distance matrix.</p><pre class="codeinput">distmat = @(X,Z)bsxfun(@plus,dot(X',X',1)',dot(Z',Z',1))-2*(X*Z');
</pre><p>Compute Euclidean distance between some \(x_{1,i}\) (for a fixed \(i\)) in the testing set and all other \(x_{1,j}\) in the
            training set.
         </p><pre class="codeinput">i = 1;
D = distmat(X0,X1(i,:));
</pre><p>Sort the distance and generate the list of sorted classes \(Y\).</p><pre class="codeinput">[~,I] = sort(D);
Y = y(I);
</pre><p>Display class evolution as distance grows.</p><pre class="codeinput">clf;
plot(Y, <span class="string">'.'</span>, <span class="string">'MarkerSize'</span>, ms);
axis <span class="string">tight</span>; box <span class="string">on</span>;
SetAR(1);
</pre><img vspace="5" hspace="5" src="index_05.png"> <p>Perform a \(R\)-neareast neighbor classification. The output class \(C\) is the one which is the most represented among the
            \(R\) closest points.
         </p><pre class="codeinput">R = 5;  <span class="comment">% number of NN</span>
h = hist(Y(1:R,:), 1:k);
[~,C] = max(h);
</pre><p><i>Exercice 1:</i> (<a href="../missing-exo/">check the solution</a>) Do the same, but for all the point in the test set, and for varying \(R\). Show how the classification score \(S\) (number
            of correctly classified points) evolves with \(R\)
         </p><pre class="codeinput">exo1;
</pre><img vspace="5" hspace="5" src="index_06.png"> <p><i>Exercice 2:</i> (<a href="../missing-exo/">check the solution</a>) Display, as a function of the position in 2D PCA space, the class output by the R-NN method.
         </p><pre class="codeinput">exo2;
</pre><img vspace="5" hspace="5" src="index_07.png"> <h2>Unsupervised Learning: K-means<a name="31"></a></h2>
         <p class="footer"><br>
            Copyright  (c) 2010 Gabriel Peyre<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% PCA, Nearest-Neighbors and Clustering
% Test for PCA (visualization), NN classification and K-means on the IRIS dataset.

%% Installing toolboxes and setting up the path.

%%
% You need to download the following files: 
% <../toolbox_general.zip general toolbox>.

%%
% You need to unzip these toolboxes in your working directory, so
% that you have 
% |toolbox_general|
% in your directory.

%%
% *For Scilab user:* you must replace the Matlab comment '%' by its Scilab
% counterpart '//'.

%%
% *Recommandation:* You should create a text file named for instance |numericaltour.sce| (in Scilab) or |numericaltour.m| (in Matlab) to write all the
% Scilab/Matlab command you want to execute. Then, simply run |exec('numericaltour.sce');| (in Scilab) or |numericaltour;| (in Matlab) to run the commands. 

%%
% Execute this line only if you are using Matlab.

getd = @(p)path(p,path); % scilab users must *not* execute this

%%
% Then you can add the toolboxes to the path.

getd('toolbox_general/');

%% Dataset Loading

%%
% Helpers.

SetAR = @(ar)set(gca, 'PlotBoxAspectRatio', [1 ar 1], 'FontSize', 20);

%%
% Load the dataset.

name = 'iris';
load(['ml-' name]);

%%
% Randomly permute it.

A = A(randperm(size(A,1)),:);


%%
% Separate the features from the class information.
% Be sure to start the class at index 1. 

X = A(:,1:end-1);
y = A(:,end);
y = y-min(y)+1;

%%
% \(p\) is the number of samples, \(n\) is the dimensionality of the features,
% \(k\) is the number of classes.

[p,n] = size(X);
k = max(y);

%% Dimenionality Reduction and PCA
% In order to display in 2D or 3D the data, dimensionality is needed.
% The simplest method is the principal component analysis, which perform an
% orthogonal linear projection on the principal axsis (eigenvector) of the
% covariance matrix.

%%
% Compute empirical mean \(m \in \RR^n\) and covariance \(C \in \RR^{n \times n}\).

m = mean(X,1);
Xm = X-repmat(m, [p 1]);
C = Xm'*Xm;

%%
% Display the covariance matrix.

clf;
imagesc(C);

%%
% Compute PCA ortho-basis.

[U,D] = eig(C); 
[d,I] = sort(diag(D), 'descend');
U = U(:,I);

%%
% Compute the feature in the PCA basis.

z = (U'*Xm')';

%%
% Plot sqrt of the eigenvalues.

clf; 
plot(sqrt(d), '.-', 'LineWidth', 2, 'MarkerSize', 30);
axis tight;
SetAR(1/2);


%%
% Display in 2D.

col = {'b' 'g' 'r' 'c' 'm' 'y' 'k'};
ms = 25;
clf; hold on;
for i=1:k
    I = find(y==i);
    plot(z(I,1), z(I,2), '.', 'Color', col{i}, 'MarkerSize', ms);
end
axis tight; axis equal; box on;
SetAR(1);


%%
% Display in 2D.

clf; hold on;
for i=1:k
    I = find(y==i);
    plot3(z(I,1), z(I,2), z(I,3), '.', 'Color', col{i}, 'MarkerSize', ms);
end
view(3); axis tight; axis equal; box on;
SetAR(1);

%% Supervised Learning: Nearest Neighbor Classification

%%
% Split into training and testing.

p0 = round(.5*p);
p1 = p-p0;

X0 = X(1:p0,:);     y0 = y(1:p0);
X1 = X(p0+1:end,:); y1 = y(p0+1:end);

%% 
% Macro to compute pairwise squared Euclidean distance matrix.

distmat = @(X,Z)bsxfun(@plus,dot(X',X',1)',dot(Z',Z',1))-2*(X*Z');

%%
% Compute Euclidean distance between some \(x_{1,i}\) (for a fixed \(i\)) in the testing set
% and all other \(x_{1,j}\) in the training set. 

i = 1;
D = distmat(X0,X1(i,:));

%%
% Sort the distance and generate the list of sorted classes \(Y\).

[~,I] = sort(D);
Y = y(I);

%%
% Display class evolution as distance grows.

clf;
plot(Y, '.', 'MarkerSize', ms); 
axis tight; box on;
SetAR(1);

%%
% Perform a \(R\)-neareast neighbor classification.
% The output class \(C\) is the one which is the most represented among
% the \(R\) closest points. 

R = 5;  % number of NN
h = hist(Y(1:R,:), 1:k);
[~,C] = max(h);

%%
% _Exercice 1:_ (<../missing-exo/ check the solution>)
% Do the same, but for all the point in the test set, and for varying \(R\).
% Show how the classification score \(S\) (number of correctly classified points)
% evolves with \(R\)

exo1;

%%
% _Exercice 2:_ (<../missing-exo/ check the solution>)
% Display, as a function of the position in 2D PCA space, the class output by 
% the R-NN method.

exo2;

%% Unsupervised Learning: K-means




##### SOURCE END #####
-->
   </body>
</html>