
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><p style="font-size:0px">
         \[
         \newcommand{\NN}{\mathbb{N}}
         \newcommand{\CC}{\mathbb{C}}
         \newcommand{\GG}{\mathbb{G}}
         \newcommand{\LL}{\mathbb{L}}
         \newcommand{\PP}{\mathbb{P}}
         \newcommand{\QQ}{\mathbb{Q}}
         \newcommand{\RR}{\mathbb{R}}
         \newcommand{\VV}{\mathbb{V}}
         \newcommand{\ZZ}{\mathbb{Z}}
         \newcommand{\FF}{\mathbb{F}}
         \newcommand{\KK}{\mathbb{K}}
         \newcommand{\UU}{\mathbb{U}}
         \newcommand{\EE}{\mathbb{E}}
         
         \newcommand{\Aa}{\mathcal{A}}
         \newcommand{\Bb}{\mathcal{B}}
         \newcommand{\Cc}{\mathcal{C}}
         \newcommand{\Dd}{\mathcal{D}}
         \newcommand{\Ee}{\mathcal{E}}
         \newcommand{\Ff}{\mathcal{F}}
         \newcommand{\Gg}{\mathcal{G}}
         \newcommand{\Hh}{\mathcal{H}}
         \newcommand{\Ii}{\mathcal{I}}
         \newcommand{\Jj}{\mathcal{J}}
         \newcommand{\Kk}{\mathcal{K}}
         \newcommand{\Ll}{\mathcal{L}}
         \newcommand{\Mm}{\mathcal{M}}
         \newcommand{\Nn}{\mathcal{N}}
         \newcommand{\Oo}{\mathcal{O}}
         \newcommand{\Pp}{\mathcal{P}}
         \newcommand{\Qq}{\mathcal{Q}}
         \newcommand{\Rr}{\mathcal{R}}
         \newcommand{\Ss}{\mathcal{S}}
         \newcommand{\Tt}{\mathcal{T}}
         \newcommand{\Uu}{\mathcal{U}}
         \newcommand{\Vv}{\mathcal{V}}
         \newcommand{\Ww}{\mathcal{W}}
         \newcommand{\Xx}{\mathcal{X}}
         \newcommand{\Yy}{\mathcal{Y}}
         \newcommand{\Zz}{\mathcal{Z}}
         
         \newcommand{\al}{\alpha}
         \newcommand{\la}{\lambda}
         \newcommand{\ga}{\gamma}
         \newcommand{\Ga}{\Gamma}
         \newcommand{\La}{\Lambda}
         \newcommand{\Si}{\Sigma}
         \newcommand{\si}{\sigma}
         \newcommand{\be}{\beta}
         \newcommand{\de}{\delta}
         \newcommand{\De}{\Delta}
         \renewcommand{\phi}{\varphi}
         \renewcommand{\th}{\theta}
         \newcommand{\om}{\omega}
         \newcommand{\Om}{\Omega}
         \renewcommand{\epsilon}{\varepsilon}
         
         \newcommand{\Calpha}{\mathrm{C}^\al}
         \newcommand{\Cbeta}{\mathrm{C}^\be}
         \newcommand{\Cal}{\text{C}^\al}
         \newcommand{\Cdeux}{\text{C}^{2}}
         \newcommand{\Cun}{\text{C}^{1}}
         \newcommand{\Calt}[1]{\text{C}^{#1}}
         
         \newcommand{\lun}{\ell^1}
         \newcommand{\ldeux}{\ell^2}
         \newcommand{\linf}{\ell^\infty}
         \newcommand{\ldeuxj}{{\ldeux_j}}
         \newcommand{\Lun}{\text{\upshape L}^1}
         \newcommand{\Ldeux}{\text{\upshape L}^2}
         \newcommand{\Lp}{\text{\upshape L}^p}
         \newcommand{\Lq}{\text{\upshape L}^q}
         \newcommand{\Linf}{\text{\upshape L}^\infty}
         \newcommand{\lzero}{\ell^0}
         \newcommand{\lp}{\ell^p}
         
         
         \renewcommand{\d}{\ins{d}}
         
         \newcommand{\Grad}{\text{Grad}}
         \newcommand{\grad}{\text{grad}}
         \renewcommand{\div}{\text{div}}
         \newcommand{\diag}{\text{diag}}
         
         \newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
         \newcommand{\pdd}[2]{ \frac{ \partial^2 #1}{\partial #2^2} }
         
         \newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
         \newcommand{\norm}[1]{|\!| #1 |\!|}
         \newcommand{\normi}[1]{\norm{#1}_{\infty}}
         \newcommand{\normu}[1]{\norm{#1}_{1}}
         \newcommand{\normz}[1]{\norm{#1}_{0}}
         \newcommand{\abs}[1]{\vert #1 \vert}
         
         
         \newcommand{\argmin}{\text{argmin}}
         \newcommand{\argmax}{\text{argmax}}
         \newcommand{\uargmin}[1]{\underset{#1}{\argmin}\;}
         \newcommand{\uargmax}[1]{\underset{#1}{\argmax}\;}
         \newcommand{\umin}[1]{\underset{#1}{\min}\;}
         \newcommand{\umax}[1]{\underset{#1}{\max}\;}
         
         \newcommand{\pa}[1]{\left( #1 \right)}
         \newcommand{\choice}[1]{ \left\{  \begin{array}{l} #1 \end{array} \right. }
         
         \newcommand{\enscond}[2]{ \left\{ #1 \;:\; #2 \right\} }
         
         \newcommand{\qandq}{ \quad \text{and} \quad }
         \newcommand{\qqandqq}{ \qquad \text{and} \qquad }
         \newcommand{\qifq}{ \quad \text{if} \quad }
         \newcommand{\qqifqq}{ \qquad \text{if} \qquad }
         \newcommand{\qwhereq}{ \quad \text{where} \quad }
         \newcommand{\qqwhereqq}{ \qquad \text{where} \qquad }
         \newcommand{\qwithq}{ \quad \text{with} \quad }
         \newcommand{\qqwithqq}{ \qquad \text{with} \qquad }
         \newcommand{\qforq}{ \quad \text{for} \quad }
         \newcommand{\qqforqq}{ \qquad \text{for} \qquad }
         \newcommand{\qqsinceqq}{ \qquad \text{since} \qquad }
         \newcommand{\qsinceq}{ \quad \text{since} \quad }
         \newcommand{\qarrq}{\quad\Longrightarrow\quad}
         \newcommand{\qqarrqq}{\quad\Longrightarrow\quad}
         \newcommand{\qiffq}{\quad\Longleftrightarrow\quad}
         \newcommand{\qqiffqq}{\qquad\Longleftrightarrow\qquad}
         \newcommand{\qsubjq}{ \quad \text{subject to} \quad }
         \newcommand{\qqsubjqq}{ \qquad \text{subject to} \qquad }
         \]
         
      </p>
      <title>Dictionary Learning</title>
      <NOSCRIPT>
         <DIV STYLE="color:#CC0000; text-align:center"><B>Warning: <A HREF="http://www.math.union.edu/locate/jsMath">jsMath</A> 
               	requires JavaScript to process the mathematics on this page.<BR> 
               	If your browser supports JavaScript, be sure it is enabled.</B></DIV>
         <HR>
      </NOSCRIPT>
      <meta name="generator" content="MATLAB 8.2">
      <meta name="date" content="2014-10-21">
      <meta name="m-file" content="index">
      <LINK REL="stylesheet" HREF="../style.css" TYPE="text/css">
   </head>
   <body>
      <div class="content">
         <h1>Dictionary Learning</h1>
         <introduction>
            <p>Instead of using a fixed data representation such as wavelets or Fourier, one can learn the representation (the dictionary)
               to optimize the sparsity of the representation for a large class of exemplar.
            </p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Installing toolboxes and setting up the path.</a></li>
               <li><a href="#8">Dictionary Learning as a Non-convex Optimization Problem</a></li>
               <li><a href="#25">Patch Extraction</a></li>
               <li><a href="#33">Update of the Coefficients \(X\)</a></li>
               <li><a href="#37">Update the Dictionary \(D\)</a></li>
            </ul>
         </div>
         <h2>Installing toolboxes and setting up the path.<a name="1"></a></h2>
         <p>You need to download the following files: <a href="../toolbox_signal.zip">signal toolbox</a> and <a href="../toolbox_general.zip">general toolbox</a>.
         </p>
         <p>You need to unzip these toolboxes in your working directory, so that you have <tt>toolbox_signal</tt> and <tt>toolbox_general</tt> in your directory.
         </p>
         <p><b>For Scilab user:</b> you must replace the Matlab comment '%' by its Scilab counterpart '//'.
         </p>
         <p><b>Recommandation:</b> You should create a text file named for instance <tt>numericaltour.sce</tt> (in Scilab) or <tt>numericaltour.m</tt> (in Matlab) to write all the Scilab/Matlab command you want to execute. Then, simply run <tt>exec('numericaltour.sce');</tt> (in Scilab) or <tt>numericaltour;</tt> (in Matlab) to run the commands.
         </p>
         <p>Execute this line only if you are using Matlab.</p><pre class="codeinput">getd = @(p)path(p,path); <span class="comment">% scilab users must *not* execute this</span>
</pre><p>Then you can add the toolboxes to the path.</p><pre class="codeinput">getd(<span class="string">'toolbox_signal/'</span>);
getd(<span class="string">'toolbox_general/'</span>);
</pre><h2>Dictionary Learning as a Non-convex Optimization Problem<a name="8"></a></h2>
         <p>Given a set \(Y = (y_j)_{j=1}^m \in \RR^{n \times m} \) of \(m\) signals \(y_j \in \RR^m\), dictionary learning aims at finding
            the best dictionary \(D=(d_i)_{i=1}^p\) of \(p\) atoms \(d_i \in \RR^n\) to sparse code all the data.
         </p>
         <p>In this numerical tour, we consider an application to image denoising, so that each \(y_j \in \RR^n\) is a patch of size \(n=w
            \times w\) extracted from the noisy image.
         </p>
         <p>The idea of learning dictionaries to sparse code image patch was first proposed in:</p>
         <p>Olshausen BA, and Field DJ., <a href="http://www.nature.com/nature/journal/v381/n6583/abs/381607a0.html">Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images.</a> Nature, 381: 607-609, 1996.
         </p>
         <p>The sparse coding of a single data \(y=y_j\) for some \(j=1,\ldots,m\) is obtained by minimizing a \(\ell^0\) constrained
            optimization \[ \umin{ \norm{x}_0 \leq k } \frac{1}{2}\norm{y-Dx}^2 .  \] where the \(\ell^0\) pseudo-norm of \(x \in \RR^p\)
            is \[ \norm{x}_0 = \abs{\enscond{i}{x(i) \neq 0}}. \]
         </p>
         <p>The parameter \(k&gt;0\) controls the amount of sparsity.</p>
         <p>Dictionary learning performs an optimization both on the dictionary \(D\) and the set of coefficients \( X = (x_j)_{j=1}^m
            \in \RR^{p \times m} \) where, for \(j=1,\ldots,m\), \( x_j \) is the set of coefficients of the data \(y_j\). This joint
            optimization reads \[ \umin{ D \in \Dd, X \in \Xx_k } E(X,D) = \frac{1}{2}\norm{Y-DX}^2 = \frac{1}{2} \sum_{j=1}^m \norm{y_j
            - D x_j}^2. \]
         </p>
         <p>The constraint set on \(D\) reads \[ \Dd = \enscond{D \in \RR^{n \times p} }{       \forall i=1,\ldots,p, \quad \norm{D_{\cdot,i}}
            \leq 1  }, \] (the columns of the dictionary are unit normalized). The sparsity constraint set on \(X\) reads \[ \Xx_k = \enscond{X
            \in \RR^{p \times m}}{ \forall j, \: \norm{X_{\cdot,j}}_0 \leq k }. \]
         </p>
         <p>We propose to use a block-coordinate descent method to minimize \(E\): \[ X^{(\ell+1)} \in \uargmin{X \in \Xx_k} E(X,D^{(\ell)}),
            \] \[ D^{(\ell+1)} \in \uargmin{D \in \Dd} E(X^{(\ell+1)},D). \]
         </p>
         <p>One can show the convergence of this minimization scheme, see for instance</p>
         <p>P. Tseng, <a href="http://www.math.washington.edu/~tseng/papers/archive/bcr_jota.pdf">Convergence of Block Coordinate Descent Method for Nondifferentiable Minimization</a>, J. Optim. Theory Appl., 109, 2001, 475-494.
         </p>
         <p>We now define the parameter of the problem.</p>
         <p>Width \(w\) of the patches.</p><pre class="codeinput"><span class="keyword">if</span> not(exist(<span class="string">'w'</span>))
    w = 10;
<span class="keyword">end</span>
</pre><p>Dimension \(n= w \times w\) of the data to be sparse coded.</p><pre class="codeinput">n = w*w;
</pre><p>Number of atoms \(p\) in the dictionary.</p><pre class="codeinput">p = 2*n;
</pre><p>Number \(m\) of patches used for the training.</p><pre class="codeinput">m = 20*p;
</pre><p>Target sparsity \(k\).</p><pre class="codeinput">k = 4;
</pre><h2>Patch Extraction<a name="25"></a></h2>
         <p>Since the learning is computationnaly intensive, one can only apply it to small patches extracted from an image.</p><pre class="codeinput"><span class="keyword">if</span> not(exist(<span class="string">'f'</span>))
    f = rescale( crop(load_image(<span class="string">'barb'</span>),256) );
<span class="keyword">end</span>
n0 = size(f,1);
</pre><p>Display the input image.</p><pre class="codeinput">clf;
imageplot(clamp(f));
</pre><img vspace="5" hspace="5" src="index_01.png"> <p>Random patch location.</p><pre class="codeinput">q = 3*m;
x = floor( rand(1,1,q)*(n0-w) )+1;
y = floor( rand(1,1,q)*(n0-w) )+1;
</pre><p>Extract lots of patches \(y_j \in \RR^n\), and store them in a matrix \(Y=(y_j)_{j=1}^m\).</p><pre class="codeinput">[dY,dX] = meshgrid(0:w-1,0:w-1);
Xp = repmat(dX,[1 1 q]) + repmat(x, [w w 1]);
Yp = repmat(dY,[1 1 q]) + repmat(y, [w w 1]);
Y = f(Xp+(Yp-1)*n0);
Y = reshape(Y, [n q]);
</pre><p>We remove the mean, since we are going to learn a dictionary of zero-mean and unit norm atom.</p><pre class="codeinput">Y = Y - repmat( mean(Y), [n 1] );
</pre><p>Only keep those with largest energy.</p><pre class="codeinput">[tmp,I] = sort(sum(Y.^2), <span class="string">'descend'</span>);
Y = Y(:,I(1:m));
</pre><p>We consider a dictionary \(D \in \RR^{n \times p} \) of \(p \geq n\) atoms in \(\RR^n\). The initial dictionary \(D\) is computed
            by a random selection of patches, and we normalize them to be unit-norm.
         </p><pre class="codeinput">ProjC = @(D)D ./ repmat( sqrt(sum(D.^2)), [w^2, 1] );
sel = randperm(m); sel = sel(1:p);
D0 = ProjC( Y(:,sel) );
D = D0;
</pre><p>Display the initial dictionary.</p><pre class="codeinput">clf;
plot_dictionnary(D, [], [8 12]);
</pre><img vspace="5" hspace="5" src="index_02.png"> <h2>Update of the Coefficients \(X\)<a name="33"></a></h2>
         <p>The optimization on the coefficients \(X\) requires, for each \(y_j = Y_{\cdot,j}\) to compute \(x_j = X_{\cdot,j}\) that
            solves \[ \umin{ \norm{x_j}_0 \leq k } \frac{1}{2} \norm{y-D x_j}^2. \]
         </p>
         <p>This is a non-smooth and non-convex minimization, that can be shown to be NP-hard. A heuristic to solve this method is to
            compute a stationary point of the energy using the Foward-Backward iterative scheme (projected gradient descent): \[ x_j \leftarrow
            \text{Proj}_{\Xx_k}\pa{       x_j - \tau D^* ( D x_j - y )       }       \qwhereq \tau &lt; \frac{2}{\norm{D D^*}}. \]
         </p>
         <p>Denoting \(\abs{\bar x(1)} \leq \ldots \leq \abs{\bar x(n)}\) the ordered magnitudes of a vector \( x \in \RR^n \), the orthogonal
            projector on \(\Xx_k\) reads \(z = \text{Proj}_{\Xx_k}(x)\) with \[ \forall i=1,\ldots,n, \quad       z(i) = \choice{ x(i)
            \qifq \abs{x(i)} \geq \abs{\bar x(k)}, \\       z(i) = 0 \quad \text{otherwise}.   } \]
         </p><pre class="codeinput">select = @(A,k)repmat(A(k,:), [size(A,1) 1]);
ProjX = @(X,k)X .* (abs(X) &gt;= select(sort(abs(X), <span class="string">'descend'</span>),k));
</pre><p><i>Exercice 1:</i> (<a href="../missing-exo/">check the solution</a>) Perform the iterative hard thresholding, and display the decay of the energy \(J(x_j) = \norm{y_j-D x_j}^2\) for several
            \(j\). <i>Remark:</i> note that the iteration can be performed in parallel on all \(x_j\).
         </p><pre class="codeinput">exo1;
</pre><img vspace="5" hspace="5" src="index_03.png"> <h2>Update the Dictionary \(D\)<a name="37"></a></h2>
         <p>Once the sparse coefficients \(X\) have been computed, one can udpate the dictionary. This is achieve by performing the minimization
            \[ \umin{D \in \Dd} \frac{1}{2}\norm{Y-D X}^2. \]
         </p>
         <p>One can perform this minimization with a projected gradient descent \[ D \leftarrow \text{Proj}_{\Cc}\pa{ D - \tau (DX - Y)X^*
            } \] where \( \tau &lt; 2/\norm{XX^*}. \)
         </p>
         <p>Note that the orthogonal projector \(\text{Proj}_{\Cc}\) is implemented in the function <tt>ProjC</tt> already defined.
         </p>
         <p><i>Exercice 2:</i> (<a href="../missing-exo/">check the solution</a>) Perform this gradient descent, and monitor the decay of the energy.
         </p><pre class="codeinput">exo2;
</pre><img vspace="5" hspace="5" src="index_04.png"> <p><i>Exercice 3:</i> (<a href="../missing-exo/">check the solution</a>) Perform the dictionary learning by iterating between sparse coding and dictionary update.
         </p><pre class="codeinput">exo3;
</pre><img vspace="5" hspace="5" src="index_05.png"> <p>Display the dictionary.</p><pre class="codeinput">clf;
plot_dictionnary(D,X, [8 12]);
</pre><pre class="codeoutput">Index exceeds matrix dimensions.

Error in plot_dictionnary (line 69)
vmax = max( max( abs( D(:,I(1:prod(nb))) ) ) );

Error in index (line 256)
plot_dictionnary(D,X, [8 12]);
</pre><p class="footer"><br>
            Copyright  (c) 2010 Gabriel Peyre<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Dictionary Learning
% Instead of using a fixed data representation such as wavelets or Fourier,
% one can learn the representation (the dictionary) to optimize the
% sparsity of the representation for a large class of exemplar.

%% Installing toolboxes and setting up the path.

%%
% You need to download the following files: 
% <../toolbox_signal.zip signal toolbox> and 
% <../toolbox_general.zip general toolbox>.

%%
% You need to unzip these toolboxes in your working directory, so
% that you have 
% |toolbox_signal| and 
% |toolbox_general|
% in your directory.

%%
% *For Scilab user:* you must replace the Matlab comment '%' by its Scilab
% counterpart '//'.

%%
% *Recommandation:* You should create a text file named for instance |numericaltour.sce| (in Scilab) or |numericaltour.m| (in Matlab) to write all the
% Scilab/Matlab command you want to execute. Then, simply run |exec('numericaltour.sce');| (in Scilab) or |numericaltour;| (in Matlab) to run the commands. 

%%
% Execute this line only if you are using Matlab.

getd = @(p)path(p,path); % scilab users must *not* execute this

%%
% Then you can add the toolboxes to the path.

getd('toolbox_signal/');
getd('toolbox_general/');


%% Dictionary Learning as a Non-convex Optimization Problem
% Given a set \(Y = (y_j)_{j=1}^m \in \RR^{n \times m} \) of \(m\) signals
% \(y_j \in \RR^m\), dictionary learning aims at finding the best
% dictionary \(D=(d_i)_{i=1}^p\) of \(p\) atoms \(d_i \in \RR^n\) to sparse
% code all the data.

%%
% In this numerical tour, we consider an application to image denoising, so
% that each \(y_j \in \RR^n\) is a patch of size \(n=w \times w\) extracted
% from the noisy image.

%%
% The idea of learning dictionaries to sparse code image patch was first
% proposed in:

%%
% Olshausen BA, and Field DJ.,  
% <http://www.nature.com/nature/journal/v381/n6583/abs/381607a0.html Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images.>
% Nature, 381: 607-609, 1996.

%%
% The sparse coding of a single data \(y=y_j\) for some \(j=1,\ldots,m\) 
% is obtained by minimizing a \(\ell^0\) constrained optimization
% \[ \umin{ \norm{x}_0 \leq k } \frac{1}{2}\norm{y-Dx}^2 .  \]
% where the \(\ell^0\) pseudo-norm of \(x \in \RR^p\) is
% \[ \norm{x}_0 = \abs{\enscond{i}{x(i) \neq 0}}. \]

%%
% The parameter \(k>0\) controls the amount of sparsity.

%%
% Dictionary learning performs an optimization both on the dictionary \(D\)
% and the set of coefficients \( X = (x_j)_{j=1}^m \in \RR^{p \times m} \)
% where, for \(j=1,\ldots,m\), \( x_j \) 
% is the set of coefficients of the data \(y_j\). This joint optimization reads
% \[ \umin{ D \in \Dd, X \in \Xx_k } E(X,D) = \frac{1}{2}\norm{Y-DX}^2 = 
% \frac{1}{2} \sum_{j=1}^m \norm{y_j - D x_j}^2. \]

%%
% The constraint set on \(D\) reads
% \[ \Dd = \enscond{D \in \RR^{n \times p} }{
%       \forall i=1,\ldots,p, \quad \norm{D_{\cdot,i}} \leq 1  }, \]
% (the columns of the dictionary are unit normalized).
% The sparsity constraint set on \(X\) reads
% \[ \Xx_k = \enscond{X \in \RR^{p \times m}}{ \forall j, \: \norm{X_{\cdot,j}}_0 \leq k }. \]

%% 
% We propose to use a block-coordinate descent method to minimize \(E\):
% \[ X^{(\ell+1)} \in \uargmin{X \in \Xx_k} E(X,D^{(\ell)}), \]
% \[ D^{(\ell+1)} \in \uargmin{D \in \Dd} E(X^{(\ell+1)},D). \]

%%
% One can show the convergence of this minimization scheme, see for
% instance

%%
% P. Tseng, <http://www.math.washington.edu/~tseng/papers/archive/bcr_jota.pdf Convergence of Block Coordinate Descent Method for Nondifferentiable Minimization>,
% J. Optim. Theory Appl., 109, 2001, 475-494.

%%
% We now define the parameter of the problem.

%%
% Width \(w\) of the patches.

if not(exist('w'))
    w = 10;
end

%%
% Dimension \(n= w \times w\) of the data to be sparse coded.

n = w*w;


%%
% Number of atoms \(p\) in the dictionary.

p = 2*n;

%%
% Number \(m\) of patches used for the training.

m = 20*p;

%%
% Target sparsity \(k\).

k = 4;

%% Patch Extraction
% Since the learning is computationnaly intensive, one can only apply it to
% small patches extracted from an image. 

if not(exist('f'))
    f = rescale( crop(load_image('barb'),256) );
end
n0 = size(f,1);

%%
% Display the input image.

clf;
imageplot(clamp(f));

%%
% Random patch location.

q = 3*m;
x = floor( rand(1,1,q)*(n0-w) )+1;
y = floor( rand(1,1,q)*(n0-w) )+1;

%%
% Extract lots of patches \(y_j \in \RR^n\), and store them in a matrix \(Y=(y_j)_{j=1}^m\).

[dY,dX] = meshgrid(0:w-1,0:w-1);
Xp = repmat(dX,[1 1 q]) + repmat(x, [w w 1]);
Yp = repmat(dY,[1 1 q]) + repmat(y, [w w 1]);
Y = f(Xp+(Yp-1)*n0);
Y = reshape(Y, [n q]);

%%
% We remove the mean, since we are going to learn a dictionary of 
% zero-mean and unit norm atom.

Y = Y - repmat( mean(Y), [n 1] );

%%
% Only keep those with largest energy.

[tmp,I] = sort(sum(Y.^2), 'descend');
Y = Y(:,I(1:m));

%%
% We consider a dictionary \(D \in \RR^{n \times p} \) of \(p \geq n\) atoms in \(\RR^n\).
% The initial dictionary \(D\) is computed by a random selection of patches, and we normalize them to be unit-norm.

ProjC = @(D)D ./ repmat( sqrt(sum(D.^2)), [w^2, 1] );
sel = randperm(m); sel = sel(1:p); 
D0 = ProjC( Y(:,sel) );
D = D0;

%%
% Display the initial dictionary.

clf;
plot_dictionnary(D, [], [8 12]);

%% Update of the Coefficients \(X\)
% The optimization on the coefficients \(X\) requires, for each \(y_j =
% Y_{\cdot,j}\) to compute \(x_j = X_{\cdot,j}\) that solves
% \[ \umin{ \norm{x_j}_0 \leq k } \frac{1}{2} \norm{y-D x_j}^2. \]

%%
% This is a non-smooth and non-convex minimization, that can be shown to be
% NP-hard. A heuristic to solve this method is to compute a stationary
% point of the energy using the Foward-Backward iterative scheme (projected gradient descent):
% \[ x_j \leftarrow \text{Proj}_{\Xx_k}\pa{
%       x_j - \tau D^* ( D x_j - y )
%       }
%       \qwhereq \tau < \frac{2}{\norm{D D^*}}. \]

%%
% Denoting \(\abs{\bar x(1)} \leq \ldots \leq \abs{\bar x(n)}\) the ordered
% magnitudes of a vector \( x \in \RR^n \), the orthogonal projector on
% \(\Xx_k\) reads \(z = \text{Proj}_{\Xx_k}(x)\) with 
% \[ \forall i=1,\ldots,n, \quad
%       z(i) = \choice{ x(i) \qifq \abs{x(i)} \geq \abs{\bar x(k)}, \\
%       z(i) = 0 \quad \text{otherwise}. 
%   }
% \]

select = @(A,k)repmat(A(k,:), [size(A,1) 1]);
ProjX = @(X,k)X .* (abs(X) >= select(sort(abs(X), 'descend'),k));

%%
% _Exercice 1:_ (<../missing-exo/ check the solution>)
% Perform the iterative hard thresholding, 
% and display the decay of the energy \(J(x_j) = \norm{y_j-D x_j}^2\) for several \(j\).
% _Remark:_ note that the iteration can be performed in parallel on all
% \(x_j\).

exo1;

%% Update the Dictionary \(D\)
% Once the sparse coefficients \(X\) have been computed, one 
% can udpate the dictionary. This is achieve by performing the minimization
% \[ \umin{D \in \Dd} \frac{1}{2}\norm{Y-D X}^2. \]

%%
% One can perform this minimization with a projected gradient descent
% \[ D \leftarrow \text{Proj}_{\Cc}\pa{ D - \tau (DX - Y)X^* } \]
% where \( \tau < 2/\norm{XX^*}. \)

%%
% Note that the orthogonal projector \(\text{Proj}_{\Cc}\) is implemented in the function
% |ProjC| already defined.

%%
% _Exercice 2:_ (<../missing-exo/ check the solution>)
% Perform this gradient descent, and monitor the decay of the energy.

exo2;


%%
% _Exercice 3:_ (<../missing-exo/ check the solution>)
% Perform the dictionary learning by iterating between sparse coding and 
% dictionary update.

exo3;

%%
% Display the dictionary.

clf;
plot_dictionnary(D,X, [8 12]);

##### SOURCE END #####
-->
   </body>
</html>