
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><p style="font-size:0px">
         \[
         \newcommand{\NN}{\mathbb{N}}
         \newcommand{\CC}{\mathbb{C}}
         \newcommand{\GG}{\mathbb{G}}
         \newcommand{\LL}{\mathbb{L}}
         \newcommand{\PP}{\mathbb{P}}
         \newcommand{\QQ}{\mathbb{Q}}
         \newcommand{\RR}{\mathbb{R}}
         \newcommand{\VV}{\mathbb{V}}
         \newcommand{\ZZ}{\mathbb{Z}}
         \newcommand{\FF}{\mathbb{F}}
         \newcommand{\KK}{\mathbb{K}}
         \newcommand{\UU}{\mathbb{U}}
         \newcommand{\EE}{\mathbb{E}}
         
         \newcommand{\Aa}{\mathcal{A}}
         \newcommand{\Bb}{\mathcal{B}}
         \newcommand{\Cc}{\mathcal{C}}
         \newcommand{\Dd}{\mathcal{D}}
         \newcommand{\Ee}{\mathcal{E}}
         \newcommand{\Ff}{\mathcal{F}}
         \newcommand{\Gg}{\mathcal{G}}
         \newcommand{\Hh}{\mathcal{H}}
         \newcommand{\Ii}{\mathcal{I}}
         \newcommand{\Jj}{\mathcal{J}}
         \newcommand{\Kk}{\mathcal{K}}
         \newcommand{\Ll}{\mathcal{L}}
         \newcommand{\Mm}{\mathcal{M}}
         \newcommand{\Nn}{\mathcal{N}}
         \newcommand{\Oo}{\mathcal{O}}
         \newcommand{\Pp}{\mathcal{P}}
         \newcommand{\Qq}{\mathcal{Q}}
         \newcommand{\Rr}{\mathcal{R}}
         \newcommand{\Ss}{\mathcal{S}}
         \newcommand{\Tt}{\mathcal{T}}
         \newcommand{\Uu}{\mathcal{U}}
         \newcommand{\Vv}{\mathcal{V}}
         \newcommand{\Ww}{\mathcal{W}}
         \newcommand{\Xx}{\mathcal{X}}
         \newcommand{\Yy}{\mathcal{Y}}
         \newcommand{\Zz}{\mathcal{Z}}
         
         \newcommand{\al}{\alpha}
         \newcommand{\la}{\lambda}
         \newcommand{\ga}{\gamma}
         \newcommand{\Ga}{\Gamma}
         \newcommand{\La}{\Lambda}
         \newcommand{\Si}{\Sigma}
         \newcommand{\si}{\sigma}
         \newcommand{\be}{\beta}
         \newcommand{\de}{\delta}
         \newcommand{\De}{\Delta}
         \renewcommand{\phi}{\varphi}
         \renewcommand{\th}{\theta}
         \newcommand{\om}{\omega}
         \newcommand{\Om}{\Omega}
         \renewcommand{\epsilon}{\varepsilon}
         
         \newcommand{\Calpha}{\mathrm{C}^\al}
         \newcommand{\Cbeta}{\mathrm{C}^\be}
         \newcommand{\Cal}{\text{C}^\al}
         \newcommand{\Cdeux}{\text{C}^{2}}
         \newcommand{\Cun}{\text{C}^{1}}
         \newcommand{\Calt}[1]{\text{C}^{#1}}
         
         \newcommand{\lun}{\ell^1}
         \newcommand{\ldeux}{\ell^2}
         \newcommand{\linf}{\ell^\infty}
         \newcommand{\ldeuxj}{{\ldeux_j}}
         \newcommand{\Lun}{\text{\upshape L}^1}
         \newcommand{\Ldeux}{\text{\upshape L}^2}
         \newcommand{\Lp}{\text{\upshape L}^p}
         \newcommand{\Lq}{\text{\upshape L}^q}
         \newcommand{\Linf}{\text{\upshape L}^\infty}
         \newcommand{\lzero}{\ell^0}
         \newcommand{\lp}{\ell^p}
         
         
         \renewcommand{\d}{\ins{d}}
         
         \newcommand{\Grad}{\text{Grad}}
         \newcommand{\grad}{\text{grad}}
         \renewcommand{\div}{\text{div}}
         \newcommand{\diag}{\text{diag}}
         
         \newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
         \newcommand{\pdd}[2]{ \frac{ \partial^2 #1}{\partial #2^2} }
         
         \newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
         \newcommand{\norm}[1]{|\!| #1 |\!|}
         \newcommand{\normi}[1]{\norm{#1}_{\infty}}
         \newcommand{\normu}[1]{\norm{#1}_{1}}
         \newcommand{\normz}[1]{\norm{#1}_{0}}
         \newcommand{\abs}[1]{\vert #1 \vert}
         
         
         \newcommand{\argmin}{\text{argmin}}
         \newcommand{\argmax}{\text{argmax}}
         \newcommand{\uargmin}[1]{\underset{#1}{\argmin}\;}
         \newcommand{\uargmax}[1]{\underset{#1}{\argmax}\;}
         \newcommand{\umin}[1]{\underset{#1}{\min}\;}
         \newcommand{\umax}[1]{\underset{#1}{\max}\;}
         
         \newcommand{\pa}[1]{\left( #1 \right)}
         \newcommand{\choice}[1]{ \left\{  \begin{array}{l} #1 \end{array} \right. }
         
         \newcommand{\enscond}[2]{ \left\{ #1 \;:\; #2 \right\} }
         
         \newcommand{\qandq}{ \quad \text{and} \quad }
         \newcommand{\qqandqq}{ \qquad \text{and} \qquad }
         \newcommand{\qifq}{ \quad \text{if} \quad }
         \newcommand{\qqifqq}{ \qquad \text{if} \qquad }
         \newcommand{\qwhereq}{ \quad \text{where} \quad }
         \newcommand{\qqwhereqq}{ \qquad \text{where} \qquad }
         \newcommand{\qwithq}{ \quad \text{with} \quad }
         \newcommand{\qqwithqq}{ \qquad \text{with} \qquad }
         \newcommand{\qforq}{ \quad \text{for} \quad }
         \newcommand{\qqforqq}{ \qquad \text{for} \qquad }
         \newcommand{\qqsinceqq}{ \qquad \text{since} \qquad }
         \newcommand{\qsinceq}{ \quad \text{since} \quad }
         \newcommand{\qarrq}{\quad\Longrightarrow\quad}
         \newcommand{\qqarrqq}{\quad\Longrightarrow\quad}
         \newcommand{\qiffq}{\quad\Longleftrightarrow\quad}
         \newcommand{\qqiffqq}{\qquad\Longleftrightarrow\qquad}
         \newcommand{\qsubjq}{ \quad \text{subject to} \quad }
         \newcommand{\qqsubjqq}{ \qquad \text{subject to} \qquad }
         \]
         
      </p>
      <title>Denoising by Sobolev and Total Variation Regularization</title>
      <NOSCRIPT>
         <DIV STYLE="color:#CC0000; text-align:center"><B>Warning: <A HREF="http://www.math.union.edu/locate/jsMath">jsMath</A> 
               	requires JavaScript to process the mathematics on this page.<BR> 
               	If your browser supports JavaScript, be sure it is enabled.</B></DIV>
         <HR>
      </NOSCRIPT>
      <meta name="generator" content="MATLAB 8.2">
      <meta name="date" content="2014-10-20">
      <meta name="m-file" content="index">
      <LINK REL="stylesheet" HREF="../style.css" TYPE="text/css">
   </head>
   <body>
      <div class="content">
         <h1>Denoising by Sobolev and Total Variation Regularization</h1>
         <introduction>
            <p>This numerical tour explores the use of variational minimization to perform denoising. It consider the Sobolev and the Total
               Variation regularization functional (priors).
            </p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Installing toolboxes and setting up the path.</a></li>
               <li><a href="#8">Prior and Regularization</a></li>
               <li><a href="#14">Sobolev Prior</a></li>
               <li><a href="#22">Heat Regularization for Denoising</a></li>
               <li><a href="#33">Total Variation Prior</a></li>
               <li><a href="#46">Total Variation Regulariation for Denoising</a></li>
            </ul>
         </div>
         <h2>Installing toolboxes and setting up the path.<a name="1"></a></h2>
         <p>You need to download the following files: <a href="../toolbox_signal.zip">signal toolbox</a> and <a href="../toolbox_general.zip">general toolbox</a>.
         </p>
         <p>You need to unzip these toolboxes in your working directory, so that you have <tt>toolbox_signal</tt> and <tt>toolbox_general</tt> in your directory.
         </p>
         <p><b>For Scilab user:</b> you must replace the Matlab comment '%' by its Scilab counterpart '//'.
         </p>
         <p><b>Recommandation:</b> You should create a text file named for instance <tt>numericaltour.sce</tt> (in Scilab) or <tt>numericaltour.m</tt> (in Matlab) to write all the Scilab/Matlab command you want to execute. Then, simply run <tt>exec('numericaltour.sce');</tt> (in Scilab) or <tt>numericaltour;</tt> (in Matlab) to run the commands.
         </p>
         <p>Execute this line only if you are using Matlab.</p><pre class="codeinput">getd = @(p)path(p,path); <span class="comment">% scilab users must *not* execute this</span>
</pre><p>Then you can add the toolboxes to the path.</p><pre class="codeinput">getd(<span class="string">'toolbox_signal/'</span>);
getd(<span class="string">'toolbox_general/'</span>);
</pre><h2>Prior and Regularization<a name="8"></a></h2>
         <p>For a given image \(f\), a prior \(J(f) \in \mathbb{R}\) assign a score supposed to be small for the image of interest.</p>
         <p>A denoising of some noisy image \(y\) is obtained by a variational minimization that mixes a fit to the data (usually using
            an \(L^2\) norm) and the prior. \[ \min_f \frac{1}{2}\|y-f\|^2 + \lambda J(f) \]
         </p>
         <p>If \(J(f)\) is a convex function of \(f\), then the minimum exists and is unique.</p>
         <p>The parameter \(\tau&gt;0\) should be adapted to the noise level. Increasing its value means a more agressive denoising.</p>
         <p>If \(J(f)\) is a smooth functional of the image \(f\), a minimizer of this problem can be computed by gradient descent. It
            defines a series of images \(f^{(\ell)}\) indexed by \(\ell \in \mathbb{N}\) as \[ f^{(\ell+1)} = f^{(\ell)} + \tau \left(
            f^{(\ell)}-y + \lambda \text{Grad}J(f^{(\ell)}) \right). \]
         </p>
         <p>Note that for \(f^{(\ell)}\) to converge with \(\ell&nbsp;\rightarrow +\infty\) toward a solution of the problem, \(\tau\) needs
            to be small enough, more precisely, if the functional \(J\) is twice differentiable, \[ \tau &lt; \frac{2}{1 + \lambda \max_f
            \|D^2 J(f)\|}. \]
         </p>
         <h2>Sobolev Prior<a name="14"></a></h2>
         <p>The Sobolev image prior is a quadratic prior, i.e. an Hilbert (pseudo)-norm.</p>
         <p>First we load a clean image.</p><pre class="codeinput">n = 256;
name = <span class="string">'hibiscus'</span>;
f0 = load_image(name,n);
f0 = rescale( sum(f0,3) );
</pre><p>For a smooth continuous function \(f\) it is defined as \[J(f) = \int \|\nabla f(x)\|^2 d x \]</p>
         <p>Where the gradient vector at point \(x\) is defined as \[ \nabla f(x) = \left(  \frac{\partial f(x)}{\partial x_1},\frac{\partial
            f(x)}{\partial x_2} \right) \]
         </p>
         <p>For a discrete pixelized image \(f \in \mathbb{R}^N\) where \(N=n \times n\) is the number of pixel, \(\nabla f(x) \in \mathbb{R}^2\)
            is computed using finite difference.
         </p><pre class="codeinput">Gr = grad(f0);
</pre><p>One can compute the norm of gradient, \(d(x) = \|\nabla f(x)\| \).</p><pre class="codeinput">d = sqrt(sum3(Gr.^2,3));
</pre><p>Display.</p><pre class="codeinput">clf;
imageplot(Gr, strcat([<span class="string">'grad'</span>]), 1,2,1);
imageplot(d, strcat([<span class="string">'|grad|'</span>]), 1,2,2);
</pre><img vspace="5" hspace="5" src="index_01.png"> <p>The Sobolev norm is the (squared) \(L^2\) norm of \(\nabla f \in \mathbb{R}^{N \times 2}\).</p><pre class="codeinput">sob = sum(d(:).^2);
</pre><h2>Heat Regularization for Denoising<a name="22"></a></h2>
         <p>Heat regularization smoothes the image using a low pass filter. Increasing the value of <tt>\lambda</tt> increases the amount of smoothing.
         </p>
         <p>Add some noise to the original image.</p><pre class="codeinput">sigma = .1;
y = f0 + randn(n,n)*sigma;
</pre><p>The solution \(f^{\star}\) of the Sobolev regularization can be computed exactly using the Fourier transform. \[\hat f^{\star}(\omega)
            = \frac{\hat y(\omega)}{ 1+\lambda S(\omega) } \quad\text{where}\quad S(\omega)=\|\omega\|^2. \]
         </p>
         <p>This shows that \(f^{\star}\) is a filtering of \(y\).</p>
         <p>Useful for later: Fourier transform of the observations.</p><pre class="codeinput">yF = fft2(y);
</pre><p>Compute the Sobolev prior penalty <tt>S</tt>&nbsp;(rescale to [0,1]).
         </p><pre class="codeinput">x = [0:n/2-1, -n/2:-1];
[Y,X] = meshgrid(x,x);
S = (X.^2 + Y.^2)*(2/n)^2;
</pre><p>Regularization parameter:</p><pre class="codeinput">lambda = 20;
</pre><p>Perform the denoising by filtering.</p><pre class="codeinput">fSob = real( ifft2( yF ./ ( 1 + lambda*S) ) );
</pre><p>Display.</p><pre class="codeinput">clf;
imageplot(clamp(fSob));
</pre><img vspace="5" hspace="5" src="index_02.png"> <p><i>Exercice 1:</i> (<a href="../missing-exo/">check the solution</a>) Compute the solution for several value of \(\lambda\) and choose the optimal <tt>lambda</tt> and the corresponding optimal denoising <tt>fSob0</tt>. You can increase progressively lambda and reduce considerably the number of iterations.
         </p><pre class="codeinput">exo1;
</pre><img vspace="5" hspace="5" src="index_03.png"> <p>Display best "oracle" denoising result.</p><pre class="codeinput">esob = snr(f0,fSob0);  enoisy = snr(f0,y);
clf;
imageplot(clamp(y), strcat([<span class="string">'Noisy '</span> num2str(enoisy,3) <span class="string">'dB'</span>]), 1,2,1);
imageplot(clamp(fSob0), strcat([<span class="string">'Sobolev regularization '</span> num2str(esob,3) <span class="string">'dB'</span>]), 1,2,2);
</pre><img vspace="5" hspace="5" src="index_04.png"> <h2>Total Variation Prior<a name="33"></a></h2>
         <p>The total variation is a Banach norm. On the contrary to the Sobolev norm, it is able to take into account step edges.</p>
         <p>The total variation of a smooth image \(f\) is defined as \[J(f)=\int \|\nabla f(x)\| d x\]</p>
         <p>It is extended to non-smooth images having step discontinuities.</p>
         <p>The total variation of an image is also equal to the total length of its level sets. \[J(f)=\int_{-\infty}^{+\infty} L( S_t(f)
            ) dt\]
         </p>
         <p>Where \(S_t(f)\) is the level set at \(t\) of the image \(f\) \[S_t(f)=\{ x \backslash f(x)=t \}\]</p>
         <p><i>Exercice 2:</i> (<a href="../missing-exo/">check the solution</a>) Compute the total variation of <tt>f0</tt>.
         </p><pre class="codeinput">exo2;
</pre><img vspace="5" hspace="5" src="index_05.png"> <p>The Gradient of the TV norm is \[ \text{Grad}J(f) = \text{div}\left( \frac{\nabla f}{\|\nabla f\|} \right) . \]</p>
         <p>The gradient of the TV norm is not defined if at a pixel \(x\) one has \(\nabla f(x)=0\). This means that the TV norm is difficult
            to minimize, and its gradient flow is not well defined.
         </p>
         <p>To define a gradient flow, we consider instead a smooth TV norm \[J_\epsilon(f) = \int \sqrt{ \varepsilon^2 + \| \nabla f(x)
            \|^2 } d x\]
         </p>
         <p>This corresponds to replacing \(\|u\|\) by \( \sqrt{\varepsilon^2 + \|u\|^2} \) which is a smooth function.</p>
         <p>We display (in 1D) the smoothing of the absolute value.</p><pre class="codeinput">u = linspace(-5,5)';
clf;
subplot(2,1,1); hold(<span class="string">'on'</span>);
plot(u, abs(u), <span class="string">'b'</span>);
plot(u, sqrt(.5^2+u.^2), <span class="string">'r'</span>);
title(<span class="string">'\epsilon=1/2'</span>); axis(<span class="string">'square'</span>);
subplot(2,1,2); hold(<span class="string">'on'</span>);
plot(u, abs(u), <span class="string">'b'</span>);
plot(u, sqrt(1^2+u.^2), <span class="string">'r'</span>);
title(<span class="string">'\epsilon=1'</span>); axis(<span class="string">'square'</span>);
</pre><img vspace="5" hspace="5" src="index_06.png"> <p>The Gradient of the smoothed TV norm is \[ \text{Grad}J(f) = \text{div}\left( \frac{\nabla f}{\sqrt{\varepsilon^2 + \|\nabla
            f\|^2}} \right) . \]
         </p>
         <p>When \(\varepsilon\) increases, the smoothed TV gradient approaches the Laplacian (normalized by \(1/\varepsilon\)).</p><pre class="codeinput">epsilon_list = [1e-9 1e-2 1e-1 .5];
clf;
<span class="keyword">for</span> i=1:length(epsilon_list)
    G = div( Gr ./ repmat( sqrt( epsilon_list(i)^2 + d.^2 ) , [1 1 2]) );
    imageplot(G, strcat([<span class="string">'epsilon='</span> num2str(epsilon_list(i))]), 2,2,i);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="index_07.png"> <h2>Total Variation Regulariation for Denoising<a name="46"></a></h2>
         <p>Total variation regularization was introduced by Rudin, Osher and Fatemi, to better respect the edge of image than linear
            filtering.
         </p>
         <p>We set a small enough regularization parameter.</p><pre class="codeinput">epsilon = 1e-2;
</pre><p>Define the regularization parameter \(\lambda\).</p><pre class="codeinput">lambda = .1;
</pre><p>The step size for diffusion should satisfy: \[ \tau &lt; \frac{2}{1 + \lambda 8 / \varepsilon} . \]</p><pre class="codeinput">tau = 2 / ( 1 + lambda * 8 / epsilon);
</pre><p>Initialization of the minimization.</p><pre class="codeinput">fTV = y;
</pre><p>Compute the gradient of the smoothed TV norm.</p><pre class="codeinput">Gr = grad(fTV);
d = sqrt(sum3(Gr.^2,3));
G = -div( Gr ./ repmat( sqrt( epsilon^2 + d.^2 ) , [1 1 2]) );
</pre><p>One step of descent.</p><pre class="codeinput">fTV = fTV - tau*( y-fTV + lambda* G);
</pre><p><i>Exercice 3:</i> (<a href="../missing-exo/">check the solution</a>) Compute the gradient descent and monitor the minimized energy.
         </p><pre class="codeinput">exo3;
</pre><img vspace="5" hspace="5" src="index_08.png"> <p>Display the image.</p><pre class="codeinput">clf;
imageplot(fTV);
</pre><img vspace="5" hspace="5" src="index_09.png"> <p><i>Exercice 4:</i> (<a href="../missing-exo/">check the solution</a>) Compute the solution for several value of \(\lambda\) and choose the optimal \(\lambda\) and the corresponding optimal denoising
            <tt>fSob0</tt>. You can increase progressively \(\lambda\) and reduce considerably the number of iterations.
         </p><pre class="codeinput">exo4;
</pre><img vspace="5" hspace="5" src="index_10.png"> <p>Display best "oracle" denoising result.</p><pre class="codeinput">etvr = snr(f0,fTV0);
clf;
imageplot(clamp(y), strcat([<span class="string">'Noisy '</span> num2str(enoisy,3) <span class="string">'dB'</span>]), 1,2,1);
imageplot(clamp(fTV0), strcat([<span class="string">'TV regularization '</span> num2str(etvr,3) <span class="string">'dB'</span>]), 1,2,2);
</pre><img vspace="5" hspace="5" src="index_11.png"> <p><i>Exercice 5:</i> (<a href="../missing-exo/">check the solution</a>) Compare the TV denoising with a hard thresholding in a translation invariant tight frame of wavelets.
         </p><pre class="codeinput">exo5;
</pre><img vspace="5" hspace="5" src="index_12.png"> <p class="footer"><br>
            Copyright  (c) 2010 Gabriel Peyre<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Denoising by Sobolev and Total Variation Regularization
% This numerical tour explores the use of variational minimization to
% perform denoising. It consider the Sobolev and the Total Variation
% regularization functional (priors).

%% Installing toolboxes and setting up the path.

%%
% You need to download the following files: 
% <../toolbox_signal.zip signal toolbox> and 
% <../toolbox_general.zip general toolbox>.

%%
% You need to unzip these toolboxes in your working directory, so
% that you have 
% |toolbox_signal| and 
% |toolbox_general|
% in your directory.

%%
% *For Scilab user:* you must replace the Matlab comment '%' by its Scilab
% counterpart '//'.

%%
% *Recommandation:* You should create a text file named for instance |numericaltour.sce| (in Scilab) or |numericaltour.m| (in Matlab) to write all the
% Scilab/Matlab command you want to execute. Then, simply run |exec('numericaltour.sce');| (in Scilab) or |numericaltour;| (in Matlab) to run the commands. 

%%
% Execute this line only if you are using Matlab.

getd = @(p)path(p,path); % scilab users must *not* execute this

%%
% Then you can add the toolboxes to the path.

getd('toolbox_signal/');
getd('toolbox_general/');


%% Prior and Regularization
% For a given image \(f\), a prior \(J(f) \in \mathbb{R}\) assign a score
% supposed to be small for the image of interest.

%%
% A denoising of some noisy image \(y\) is obtained by a variational minimization
% that mixes a fit to the data (usually using an \(L^2\) norm) and the
% prior.
% \[ \min_f \frac{1}{2}\|y-f\|^2 + \lambda J(f) \]

%%
% If \(J(f)\) is a convex function of \(f\), then the minimum exists and is
% unique.

%%
% The parameter \(\tau>0\) should be adapted to the noise level. Increasing
% its value means a more agressive denoising.

%%
% If \(J(f)\) is a smooth functional of the image \(f\), a minimizer of
% this problem can be computed by gradient descent.
% It defines a series of images \(f^{(\ell)}\) indexed by \(\ell \in \mathbb{N}\)
% as
% \[ f^{(\ell+1)} = f^{(\ell)} + \tau \left( f^{(\ell)}-y + \lambda \text{Grad}J(f^{(\ell)}) \right). \]

%%
% Note that for \(f^{(\ell)}\) to converge with 
% \(\ell \rightarrow +\infty\) toward a solution of the problem, \(\tau\) needs to be small enough,
% more precisely, if the functional \(J\) is twice differentiable, 
% \[ \tau < \frac{2}{1 + \lambda \max_f \|D^2 J(f)\|}. \]

%% Sobolev Prior
% The Sobolev image prior is a quadratic prior, i.e. an Hilbert
% (pseudo)-norm.

%%
% First we load a clean image.

n = 256;
name = 'hibiscus';
f0 = load_image(name,n);
f0 = rescale( sum(f0,3) );

%%
% For a smooth continuous function \(f\) it is defined as
% \[J(f) = \int \|\nabla f(x)\|^2 d x \]

%%
% Where the gradient vector at point \(x\) is defined as
% \[ \nabla f(x) = \left(  \frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2} \right) \]

%%
% For a discrete pixelized image \(f \in \mathbb{R}^N\) where \(N=n \times n\) is the number of pixel, 
% \(\nabla f(x) \in \mathbb{R}^2\) is computed using finite difference.

Gr = grad(f0);

%%
% One can compute the norm of gradient, \(d(x) = \|\nabla f(x)\| \).

d = sqrt(sum3(Gr.^2,3));

%% 
% Display.

clf;
imageplot(Gr, strcat(['grad']), 1,2,1);
imageplot(d, strcat(['|grad|']), 1,2,2);
    
%%
% The Sobolev norm is the (squared) \(L^2\) norm of \(\nabla f \in
% \mathbb{R}^{N \times 2}\).

sob = sum(d(:).^2);

%% Heat Regularization for Denoising
% Heat regularization smoothes the image using a low pass filter.
% Increasing the value of |\lambda| increases the amount of smoothing.

%% 
% Add some noise to the original image.

sigma = .1;
y = f0 + randn(n,n)*sigma;


%%
% The solution \(f^{\star}\) of the Sobolev regularization can be computed exactly 
% using the Fourier transform.
% \[\hat f^{\star}(\omega) = \frac{\hat y(\omega)}{ 1+\lambda S(\omega) } \quad\text{where}\quad S(\omega)=\|\omega\|^2. \]

%%
% This shows that \(f^{\star}\) is a filtering of \(y\).

%%
% Useful for later: Fourier transform of the observations.

yF = fft2(y);


%%
% Compute the Sobolev prior penalty |S| (rescale to [0,1]).

x = [0:n/2-1, -n/2:-1];
[Y,X] = meshgrid(x,x);
S = (X.^2 + Y.^2)*(2/n)^2;

%%
% Regularization parameter:

lambda = 20;

%%
% Perform the denoising by filtering.

fSob = real( ifft2( yF ./ ( 1 + lambda*S) ) );

%%
% Display.

clf;
imageplot(clamp(fSob));

%%
% _Exercice 1:_ (<../missing-exo/ check the solution>)
% Compute the solution for several value of \(\lambda\) and choose the
% optimal |lambda| and the corresponding optimal denoising |fSob0|. 
% You can increase progressively lambda and reduce
% considerably the number of iterations.

exo1;

%% 
% Display best "oracle" denoising result.

esob = snr(f0,fSob0);  enoisy = snr(f0,y);
clf;
imageplot(clamp(y), strcat(['Noisy ' num2str(enoisy,3) 'dB']), 1,2,1);
imageplot(clamp(fSob0), strcat(['Sobolev regularization ' num2str(esob,3) 'dB']), 1,2,2);



%% Total Variation Prior
% The total variation is a Banach norm.
% On the contrary to the Sobolev norm, it is able to take into account step edges.

%%
% The total variation of a smooth image \(f\) is defined as
% \[J(f)=\int \|\nabla f(x)\| d x\]

%%
% It is extended to non-smooth images having step discontinuities.

%%
% The total variation of an image is also equal to the total length of its level sets.
% \[J(f)=\int_{-\infty}^{+\infty} L( S_t(f) ) dt\]

%%
% Where \(S_t(f)\) is the level set at \(t\) of the image \(f\)
% \[S_t(f)=\{ x \backslash f(x)=t \}\]

%%
% _Exercice 2:_ (<../missing-exo/ check the solution>)
% Compute the total variation of |f0|.

exo2;

%%
% The Gradient of the TV norm is
% \[ \text{Grad}J(f) = \text{div}\left( \frac{\nabla f}{\|\nabla f\|} \right) . \]

%%
% The gradient of the TV norm is not defined if at a pixel \(x\)
% one has \(\nabla f(x)=0\). This means that the TV norm is difficult to
% minimize, and its gradient flow is not well defined.

%%
% To define a gradient flow, we consider instead a smooth TV norm
% \[J_\epsilon(f) = \int \sqrt{ \varepsilon^2 + \| \nabla f(x) \|^2 } d x\]

%% 
% This corresponds to replacing \(\|u\|\) by \( \sqrt{\varepsilon^2 + \|u\|^2} \) 
% which is a smooth function.

%%
% We display (in 1D) the smoothing of the absolute value.

u = linspace(-5,5)';
clf;
subplot(2,1,1); hold('on');
plot(u, abs(u), 'b');
plot(u, sqrt(.5^2+u.^2), 'r');
title('\epsilon=1/2'); axis('square');
subplot(2,1,2); hold('on');
plot(u, abs(u), 'b');
plot(u, sqrt(1^2+u.^2), 'r');
title('\epsilon=1'); axis('square');

%%
% The Gradient of the smoothed TV norm is
% \[ \text{Grad}J(f) = \text{div}\left( \frac{\nabla f}{\sqrt{\varepsilon^2 + \|\nabla f\|^2}} \right) . \]

%% 
% When \(\varepsilon\) increases, the smoothed TV gradient approaches the
% Laplacian (normalized by \(1/\varepsilon\)).

epsilon_list = [1e-9 1e-2 1e-1 .5];
clf;
for i=1:length(epsilon_list)
    G = div( Gr ./ repmat( sqrt( epsilon_list(i)^2 + d.^2 ) , [1 1 2]) );
    imageplot(G, strcat(['epsilon=' num2str(epsilon_list(i))]), 2,2,i);
end


%% Total Variation Regulariation for Denoising
% Total variation regularization was introduced by Rudin, Osher and Fatemi,
% to better respect the edge of image than linear filtering.

%%
% We set a small enough regularization parameter.

epsilon = 1e-2;

%%
% Define the regularization parameter \(\lambda\).

lambda = .1;


%% 
% The step size for diffusion should satisfy:
% \[ \tau < \frac{2}{1 + \lambda 8 / \varepsilon} . \]


tau = 2 / ( 1 + lambda * 8 / epsilon);

%% 
% Initialization of the minimization.

fTV = y;

%%
% Compute the gradient of the smoothed TV norm.

Gr = grad(fTV);
d = sqrt(sum3(Gr.^2,3));
G = -div( Gr ./ repmat( sqrt( epsilon^2 + d.^2 ) , [1 1 2]) );

%% 
% One step of descent.

fTV = fTV - tau*( y-fTV + lambda* G);

%%
% _Exercice 3:_ (<../missing-exo/ check the solution>)
% Compute the gradient descent and monitor
% the minimized energy.

exo3;

%%
% Display the image.

clf;
imageplot(fTV);

%%
% _Exercice 4:_ (<../missing-exo/ check the solution>)
% Compute the solution for several value of \(\lambda\) and choose the
% optimal \(\lambda\) and the corresponding optimal denoising |fSob0|. 
% You can increase progressively \(\lambda\) and reduce
% considerably the number of iterations.

exo4;


%% 
% Display best "oracle" denoising result.

etvr = snr(f0,fTV0); 
clf;
imageplot(clamp(y), strcat(['Noisy ' num2str(enoisy,3) 'dB']), 1,2,1);
imageplot(clamp(fTV0), strcat(['TV regularization ' num2str(etvr,3) 'dB']), 1,2,2);


%%
% _Exercice 5:_ (<../missing-exo/ check the solution>)
% Compare the TV denoising with a hard thresholding in a translation
% invariant tight frame of wavelets.

exo5;


##### SOURCE END #####
-->
   </body>
</html>