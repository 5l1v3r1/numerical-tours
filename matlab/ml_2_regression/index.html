
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script><p style="font-size:0px">
         \[
         \newcommand{\NN}{\mathbb{N}}
         \newcommand{\CC}{\mathbb{C}}
         \newcommand{\GG}{\mathbb{G}}
         \newcommand{\LL}{\mathbb{L}}
         \newcommand{\PP}{\mathbb{P}}
         \newcommand{\QQ}{\mathbb{Q}}
         \newcommand{\RR}{\mathbb{R}}
         \newcommand{\VV}{\mathbb{V}}
         \newcommand{\ZZ}{\mathbb{Z}}
         \newcommand{\FF}{\mathbb{F}}
         \newcommand{\KK}{\mathbb{K}}
         \newcommand{\UU}{\mathbb{U}}
         \newcommand{\EE}{\mathbb{E}}
         
         \newcommand{\Aa}{\mathcal{A}}
         \newcommand{\Bb}{\mathcal{B}}
         \newcommand{\Cc}{\mathcal{C}}
         \newcommand{\Dd}{\mathcal{D}}
         \newcommand{\Ee}{\mathcal{E}}
         \newcommand{\Ff}{\mathcal{F}}
         \newcommand{\Gg}{\mathcal{G}}
         \newcommand{\Hh}{\mathcal{H}}
         \newcommand{\Ii}{\mathcal{I}}
         \newcommand{\Jj}{\mathcal{J}}
         \newcommand{\Kk}{\mathcal{K}}
         \newcommand{\Ll}{\mathcal{L}}
         \newcommand{\Mm}{\mathcal{M}}
         \newcommand{\Nn}{\mathcal{N}}
         \newcommand{\Oo}{\mathcal{O}}
         \newcommand{\Pp}{\mathcal{P}}
         \newcommand{\Qq}{\mathcal{Q}}
         \newcommand{\Rr}{\mathcal{R}}
         \newcommand{\Ss}{\mathcal{S}}
         \newcommand{\Tt}{\mathcal{T}}
         \newcommand{\Uu}{\mathcal{U}}
         \newcommand{\Vv}{\mathcal{V}}
         \newcommand{\Ww}{\mathcal{W}}
         \newcommand{\Xx}{\mathcal{X}}
         \newcommand{\Yy}{\mathcal{Y}}
         \newcommand{\Zz}{\mathcal{Z}}
         
         \newcommand{\al}{\alpha}
         \newcommand{\la}{\lambda}
         \newcommand{\ga}{\gamma}
         \newcommand{\Ga}{\Gamma}
         \newcommand{\La}{\Lambda}
         \newcommand{\Si}{\Sigma}
         \newcommand{\si}{\sigma}
         \newcommand{\be}{\beta}
         \newcommand{\de}{\delta}
         \newcommand{\De}{\Delta}
         \renewcommand{\phi}{\varphi}
         \renewcommand{\th}{\theta}
         \newcommand{\om}{\omega}
         \newcommand{\Om}{\Omega}
         \renewcommand{\epsilon}{\varepsilon}
         
         \newcommand{\Calpha}{\mathrm{C}^\al}
         \newcommand{\Cbeta}{\mathrm{C}^\be}
         \newcommand{\Cal}{\text{C}^\al}
         \newcommand{\Cdeux}{\text{C}^{2}}
         \newcommand{\Cun}{\text{C}^{1}}
         \newcommand{\Calt}[1]{\text{C}^{#1}}
         
         \newcommand{\lun}{\ell^1}
         \newcommand{\ldeux}{\ell^2}
         \newcommand{\linf}{\ell^\infty}
         \newcommand{\ldeuxj}{{\ldeux_j}}
         \newcommand{\Lun}{\text{\upshape L}^1}
         \newcommand{\Ldeux}{\text{\upshape L}^2}
         \newcommand{\Lp}{\text{\upshape L}^p}
         \newcommand{\Lq}{\text{\upshape L}^q}
         \newcommand{\Linf}{\text{\upshape L}^\infty}
         \newcommand{\lzero}{\ell^0}
         \newcommand{\lp}{\ell^p}
         
         
         \renewcommand{\d}{\ins{d}}
         
         \newcommand{\Grad}{\text{Grad}}
         \newcommand{\grad}{\text{grad}}
         \renewcommand{\div}{\text{div}}
         \newcommand{\diag}{\text{diag}}
         
         \newcommand{\pd}[2]{ \frac{ \partial #1}{\partial #2} }
         \newcommand{\pdd}[2]{ \frac{ \partial^2 #1}{\partial #2^2} }
         
         \newcommand{\dotp}[2]{\langle #1,\,#2\rangle}
         \newcommand{\norm}[1]{|\!| #1 |\!|}
         \newcommand{\normi}[1]{\norm{#1}_{\infty}}
         \newcommand{\normu}[1]{\norm{#1}_{1}}
         \newcommand{\normz}[1]{\norm{#1}_{0}}
         \newcommand{\abs}[1]{\vert #1 \vert}
         
         \newcommand{\argmin}{\text{argmin}}
         \newcommand{\argmax}{\text{argmax}}
         \newcommand{\uargmin}[1]{\underset{#1}{\argmin}\;}
         \newcommand{\uargmax}[1]{\underset{#1}{\argmax}\;}
         \newcommand{\umin}[1]{\underset{#1}{\min}\;}
         \newcommand{\umax}[1]{\underset{#1}{\max}\;}
         
         \newcommand{\pa}[1]{\left( #1 \right)}
         \newcommand{\choice}[1]{ \left\{  \begin{array}{l} #1 \end{array} \right. }
         
         \newcommand{\enscond}[2]{ \left\{ #1 \;:\; #2 \right\} }
         
         \newcommand{\qandq}{ \quad \text{and} \quad }
         \newcommand{\qqandqq}{ \qquad \text{and} \qquad }
         \newcommand{\qifq}{ \quad \text{if} \quad }
         \newcommand{\qqifqq}{ \qquad \text{if} \qquad }
         \newcommand{\qwhereq}{ \quad \text{where} \quad }
         \newcommand{\qqwhereqq}{ \qquad \text{where} \qquad }
         \newcommand{\qwithq}{ \quad \text{with} \quad }
         \newcommand{\qqwithqq}{ \qquad \text{with} \qquad }
         \newcommand{\qforq}{ \quad \text{for} \quad }
         \newcommand{\qqforqq}{ \qquad \text{for} \qquad }
         \newcommand{\qqsinceqq}{ \qquad \text{since} \qquad }
         \newcommand{\qsinceq}{ \quad \text{since} \quad }
         \newcommand{\qarrq}{\quad\Longrightarrow\quad}
         \newcommand{\qqarrqq}{\quad\Longrightarrow\quad}
         \newcommand{\qiffq}{\quad\Longleftrightarrow\quad}
         \newcommand{\qqiffqq}{\qquad\Longleftrightarrow\qquad}
         \newcommand{\qsubjq}{ \quad \text{subject to} \quad }
         \newcommand{\qqsubjqq}{ \qquad \text{subject to} \qquad }
         
         \newcommand{\eqdef}{\equiv}
         \]
         
      </p>
      <title>Linear Regression and Kernel Methods</title>
      <NOSCRIPT>
         <DIV STYLE="color:#CC0000; text-align:center"><B>Warning: <A HREF="http://www.math.union.edu/locate/jsMath">jsMath</A>
               	requires JavaScript to process the mathematics on this page.<BR>
               	If your browser supports JavaScript, be sure it is enabled.</B></DIV>
         <HR>
      </NOSCRIPT>
      <meta name="generator" content="MATLAB 9.0">
      <meta name="date" content="2017-08-08">
      <meta name="m-file" content="index">
      <LINK REL="stylesheet" HREF="../style.css" TYPE="text/css">
   </head>
   <body>
      <div class="content">
         <h1>Linear Regression and Kernel Methods</h1>
         <introduction>
            <p>This tour studies linear regression method, and its non-linear variant using kernlization.</p>
         </introduction>
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#1">Installing toolboxes and setting up the path.</a></li>
               <li><a href="#8">Dataset Loading</a></li>
               <li><a href="#14">Dimenionality Reduction and PCA</a></li>
               <li><a href="#21">Linear Regression</a></li>
               <li><a href="#32">Kernelized Ridge Regression</a></li>
            </ul>
         </div>
         <h2>Installing toolboxes and setting up the path.<a name="1"></a></h2>
         <p>You need to download the following files: <a href="../toolbox_general.zip">general toolbox</a>.
         </p>
         <p>You need to unzip these toolboxes in your working directory, so that you have <tt>toolbox_general</tt> in your directory.
         </p>
         <p><b>For Scilab user:</b> you must replace the Matlab comment '%' by its Scilab counterpart '//'.
         </p>
         <p><b>Recommandation:</b> You should create a text file named for instance <tt>numericaltour.sce</tt> (in Scilab) or <tt>numericaltour.m</tt> (in Matlab) to write all the Scilab/Matlab command you want to execute. Then, simply run <tt>exec('numericaltour.sce');</tt> (in Scilab) or <tt>numericaltour;</tt> (in Matlab) to run the commands.
         </p>
         <p>Execute this line only if you are using Matlab.</p><pre class="codeinput">getd = @(p)path(p,path); <span class="comment">% scilab users must *not* execute this</span>
</pre><p>Then you can add the toolboxes to the path.</p><pre class="codeinput">getd(<span class="string">'toolbox_general/'</span>);
</pre><h2>Dataset Loading<a name="8"></a></h2>
         <p>Helpers.</p><pre class="codeinput">SetAR = @(ar)set(gca, <span class="string">'PlotBoxAspectRatio'</span>, [1 ar 1], <span class="string">'FontSize'</span>, 20);
</pre><p>Load the dataset.</p><pre class="codeinput">name = <span class="string">'boston_house_prices'</span>;
load([<span class="string">'ml-'</span> name]);
</pre><p>Randomly permute it.</p><pre class="codeinput">A = A(randperm(size(A,1)),:);
</pre><p>Separate the features \(X\) from the data \(y\) to predict information.</p><pre class="codeinput">X = A(:,1:end-1);
y = A(:,end);
</pre><p>\(p\) is the number of samples, \(n\) is the dimensionality of the features,</p><pre class="codeinput">[p,n] = size(X);
</pre><h2>Dimenionality Reduction and PCA<a name="14"></a></h2>
         <p>In order to display in 2D or 3D the data, dimensionality is needed. The simplest method is the principal component analysis,
            which perform an orthogonal linear projection on the principal axsis (eigenvector) of the covariance matrix.
         </p>
         <p>Compute empirical mean \(m \in \RR^n\) and covariance \(C \in \RR^{n \times n}\).</p><pre class="codeinput">m = mean(X,1);
Xm = X-repmat(m, [p 1]);
C = Xm'*Xm;
</pre><p>Display the covariance matrix.</p><pre class="codeinput">clf;
imagesc(C);
</pre><img vspace="5" hspace="5" src="index_01.png"> <p>Compute PCA ortho-basis.</p><pre class="codeinput">[U,D] = eig(C);
[d,I] = sort(diag(D), <span class="string">'descend'</span>);
U = U(:,I);
</pre><p>Compute the feature in the PCA basis.</p><pre class="codeinput">z = (U'*Xm')';
</pre><p>Plot sqrt of the eigenvalues.</p><pre class="codeinput">clf;
plot(sqrt(d), <span class="string">'.-'</span>, <span class="string">'LineWidth'</span>, 2, <span class="string">'MarkerSize'</span>, 30);
axis <span class="string">tight</span>;
SetAR(1/2);
</pre><img vspace="5" hspace="5" src="index_02.png"> <p>1D plot of the function to regress along the main eigenvector axes.</p><pre class="codeinput">col = {<span class="string">'b'</span> <span class="string">'g'</span> <span class="string">'r'</span> <span class="string">'c'</span> <span class="string">'m'</span> <span class="string">'y'</span> <span class="string">'k'</span>};
clf;
<span class="keyword">for</span> i=1:min(n,3)
    subplot(3,1,i);
    plot(z(:,i), y, <span class="string">'.'</span>, <span class="string">'Color'</span>, col{i}, <span class="string">'MarkerSize'</span>, 20);
    axis <span class="string">tight</span>;
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="index_03.png"> <h2>Linear Regression<a name="21"></a></h2>
         <p>We look for a linear relationship   \( y_i = \dotp{w}{x_i} \) written in matrix format   \( y= x w \) where the rows of \(x\)
            stores the features \(x_i\).
         </p>
         <p>Since here \( p &gt; n \), this is an over-determined system, which can solved in the least square sense   \[ \umin{ w }  \norm{xw-y}^2
            \] whose solution is given using the Moore-Penrose pseudo-inverse   \[ w = (x^\top x)^{-1} x^\top y \]
         </p>
         <p>Split into training and testing.</p><pre class="codeinput">p0 = round(.5*p);
p1 = p-p0;

X0 = X(1:p0,:);     y0 = y(1:p0);
X1 = X(p0+1:end,:); y1 = y(p0+1:end);
</pre><p>Least square solution.</p><pre class="codeinput">w = (X0'*X0) \ (X0'*y0);
</pre><p>Mean-square error on testing set.</p><pre class="codeinput">E = sqrt( sum( (X1*w-y1).^2 ) / p1 );
</pre><p>Regularization is obtained by introducing a penalty. It is often called ridge regression, and is defined as   \[ \umin{ w
            }  \norm{xw-y}^2 + \lambda*\norm{w}^2 \] where \(\lambda&gt;0\) is the regularization parameter.
         </p>
         <p>The solution is given using the following equivalent formula   \[ w = (x^\top x + \lambda \Id_n )^{-1} x^\top y \]   \[ w
            = x^\top ( x x^\top + \lambda \Id_p )^{-1} y \] When \(n&lt;p\) (which is the case here), the first formula should be prefered.
         </p>
         <p>In contrast, when the dimensionality \(n\) of the feature is very large and there is little data, the second is faster. Furthermore,
            this second expression is generalizable to Kernel Hilbert space setting, corresponding possibly to \(n=+\infty\) for some
            kernels.
         </p><pre class="codeinput">lambda = .1;
w = (X0'*X0+lambda*eye(n)) \ (X0'*y0);
w1 = X0'*( (X0*X0'+lambda*eye(p0)) \ y0 );
fprintf(<span class="string">'Error (should be 0): %.4f\n'</span>, norm(w-w1)/norm(w));
</pre><pre class="codeoutput">Error (should be 0): 0.0000
</pre><p><i>Exercice 1:</i> (<a href="../missing-exo/">check the solution</a>) Display the evolution of the error \(E\) as a function of \(\lambda\).
         </p><pre class="codeinput">exo1;
</pre><img vspace="5" hspace="5" src="index_04.png"> <p><i>Exercice 2:</i> (<a href="../missing-exo/">check the solution</a>) Display the regularization path, i.e. the evolution of \(w\) as a function of \(\lambda\).
         </p><pre class="codeinput">exo2;
</pre><img vspace="5" hspace="5" src="index_05.png"> <p><i>Exercice 3:</i> (<a href="../missing-exo/">check the solution</a>) Perform feature selection using \(\ell^1\) regularization (aka the Lasso)   \[ \umin{ w }  \norm{xw-y}^2 + \lambda*\norm{w}_1
            \]
         </p><pre class="codeinput">exo3;
</pre><h2>Kernelized Ridge Regression<a name="32"></a></h2>
         <p>Generate synthetic data in 2D. Add noise to a deterministic map.</p><pre class="codeinput">B = 3;
p = 500; n = 2;
X = 2*B*rand(p,2)-B;
rho = .5; <span class="comment">% noise level</span>
y = peaks(X(:,1), X(:,2)) + randn(p,1)*rho;
</pre><p>Display as scattered plot.</p><pre class="codeinput">clf;
scatter(X(:,1), X(:,2), ones(p,1)*20, y, <span class="string">'filled'</span>);
colormap <span class="string">jet(256)</span>;
axis <span class="string">equal</span>; axis([-B B -B B]); box <span class="string">on</span>;
</pre><img vspace="5" hspace="5" src="index_06.png"> <p>Macro to compute pairwise squared Euclidean distance matrix.</p><pre class="codeinput">distmat = @(X,Z)bsxfun(@plus,dot(X',X',1)',dot(Z',Z',1))-2*(X*Z');
</pre><p>Gaussian kernel.</p><pre class="codeinput">sigma = .3;
K = @(X,Z)exp( -distmat(X,Z)/(2*sigma^2) );
</pre><p>Weights.</p><pre class="codeinput">lambda = 0.01;
r = (K(X,X)+lambda*eye(p))\y;
</pre><p>Regressor.</p><pre class="codeinput">Y = @(x)K(x,X)*r;
</pre><p>Evaluation on a 2D grid.</p><pre class="codeinput">q = 101;
t = linspace(-B,B,q);
[v,u] = meshgrid(t,t);
Xn = [u(:), v(:)];
</pre><p>Display as an image.</p><pre class="codeinput">yn = reshape(Y(Xn),[q,q]);
clf;
imagesc(t,t,yn); axis <span class="string">image</span>; axis <span class="string">off</span>;
colormap <span class="string">jet(256)</span>;
</pre><img vspace="5" hspace="5" src="index_07.png"> <p><i>Exercice 4:</i> (<a href="../missing-exo/">check the solution</a>) Display the evolution of the regression as a function of \(\sigma\).
         </p><pre class="codeinput">exo4;
</pre><img vspace="5" hspace="5" src="index_08.png"> <p><i>Exercice 5:</i> (<a href="../missing-exo/">check the solution</a>) Apply the kernelize regression to a real life dataset. Study the influence of \(\la\) and \(\si\).
         </p><pre class="codeinput">exo5;
</pre><p class="footer"><br>
            Copyright  (c) 2010 Gabriel Peyre<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Linear Regression and Kernel Methods
% This tour studies linear regression method, and its non-linear variant
% using kernlization.

%% Installing toolboxes and setting up the path.

%%
% You need to download the following files: 
% <../toolbox_general.zip general toolbox>.

%%
% You need to unzip these toolboxes in your working directory, so
% that you have 
% |toolbox_general|
% in your directory.

%%
% *For Scilab user:* you must replace the Matlab comment '%' by its Scilab
% counterpart '//'.

%%
% *Recommandation:* You should create a text file named for instance |numericaltour.sce| (in Scilab) or |numericaltour.m| (in Matlab) to write all the
% Scilab/Matlab command you want to execute. Then, simply run |exec('numericaltour.sce');| (in Scilab) or |numericaltour;| (in Matlab) to run the commands. 

%%
% Execute this line only if you are using Matlab.

getd = @(p)path(p,path); % scilab users must *not* execute this

%%
% Then you can add the toolboxes to the path.

getd('toolbox_general/');

%% Dataset Loading

%%
% Helpers.

SetAR = @(ar)set(gca, 'PlotBoxAspectRatio', [1 ar 1], 'FontSize', 20);

%%
% Load the dataset.

name = 'boston_house_prices';
load(['ml-' name]);

%%
% Randomly permute it.

A = A(randperm(size(A,1)),:);

%%
% Separate the features \(X\) from the data \(y\) to predict information.

X = A(:,1:end-1);
y = A(:,end);

%%
% \(p\) is the number of samples, \(n\) is the dimensionality of the features,

[p,n] = size(X);

%% Dimenionality Reduction and PCA
% In order to display in 2D or 3D the data, dimensionality is needed.
% The simplest method is the principal component analysis, which perform an
% orthogonal linear projection on the principal axsis (eigenvector) of the
% covariance matrix.

%%
% Compute empirical mean \(m \in \RR^n\) and covariance \(C \in \RR^{n \times n}\).

m = mean(X,1);
Xm = X-repmat(m, [p 1]);
C = Xm'*Xm;

%%
% Display the covariance matrix.

clf;
imagesc(C);

%%
% Compute PCA ortho-basis.

[U,D] = eig(C); 
[d,I] = sort(diag(D), 'descend');
U = U(:,I);

%%
% Compute the feature in the PCA basis.

z = (U'*Xm')';

%%
% Plot sqrt of the eigenvalues.

clf; 
plot(sqrt(d), '.-', 'LineWidth', 2, 'MarkerSize', 30);
axis tight;
SetAR(1/2);

%%
% 1D plot of the function to regress along the main eigenvector axes.

col = {'b' 'g' 'r' 'c' 'm' 'y' 'k'};
clf;
for i=1:min(n,3)
    subplot(3,1,i);
    plot(z(:,i), y, '.', 'Color', col{i}, 'MarkerSize', 20);
    axis tight;
end

%% Linear Regression
% We look for a linear relationship 
%   \( y_i = \dotp{w}{x_i} \)
% written in matrix format 
%   \( y= x w \)
% where the rows of \(x\) stores the features \(x_i\).

%%
% Since here \( p > n \), this is an over-determined system, which can
% solved in the least square sense
%   \[ \umin{ w }  \norm{xw-y}^2 \]
% whose solution is given using the Moore-Penrose pseudo-inverse
%   \[ w = (x^\top x)^{-1} x^\top y \]


%%
% Split into training and testing.

p0 = round(.5*p);
p1 = p-p0;

X0 = X(1:p0,:);     y0 = y(1:p0);
X1 = X(p0+1:end,:); y1 = y(p0+1:end);


%%
% Least square solution.

w = (X0'*X0) \ (X0'*y0);

%%
% Mean-square error on testing set.

E = sqrt( sum( (X1*w-y1).^2 ) / p1 );


%%
% Regularization is obtained by introducing a penalty. It is often called
% ridge regression, and is defined as 
%   \[ \umin{ w }  \norm{xw-y}^2 + \lambda*\norm{w}^2 \]
% where \(\lambda>0\) is the regularization parameter.

%%
% The solution is given using the following equivalent formula
%   \[ w = (x^\top x + \lambda \Id_n )^{-1} x^\top y \]
%   \[ w = x^\top ( x x^\top + \lambda \Id_p )^{-1} y \]
% When \(n<p\) (which is the case here), the first formula should be
% prefered. 

%%
% In contrast, when the dimensionality \(n\) of the feature is very
% large and there is little data, the second is faster. Furthermore, this
% second expression is generalizable to Kernel Hilbert space setting,
% corresponding possibly to \(n=+\infty\) for some kernels. 

lambda = .1;
w = (X0'*X0+lambda*eye(n)) \ (X0'*y0);
w1 = X0'*( (X0*X0'+lambda*eye(p0)) \ y0 );
fprintf('Error (should be 0): %.4f\n', norm(w-w1)/norm(w));

%%
% _Exercice 1:_ (<../missing-exo/ check the solution>)
% Display the evolution of the error \(E\) as a function of \(\lambda\).

exo1;

%%
% _Exercice 2:_ (<../missing-exo/ check the solution>)
% Display the regularization path, i.e. the evolution of \(w\) as a function 
% of \(\lambda\).

exo2;

%%
% _Exercice 3:_ (<../missing-exo/ check the solution>)
% Perform feature selection using \(\ell^1\) regularization (aka the Lasso)
%   \[ \umin{ w }  \norm{xw-y}^2 + \lambda*\norm{w}_1 \]

exo3;


%% Kernelized Ridge Regression


%%
% Generate synthetic data in 2D.
% Add noise to a deterministic map.

B = 3;
p = 500; n = 2;
X = 2*B*rand(p,2)-B;
rho = .5; % noise level
y = peaks(X(:,1), X(:,2)) + randn(p,1)*rho;

%%
% Display as scattered plot.

clf;
scatter(X(:,1), X(:,2), ones(p,1)*20, y, 'filled');
colormap jet(256); 
axis equal; axis([-B B -B B]); box on;

%% 
% Macro to compute pairwise squared Euclidean distance matrix.

distmat = @(X,Z)bsxfun(@plus,dot(X',X',1)',dot(Z',Z',1))-2*(X*Z');

%%
% Gaussian kernel.

sigma = .3;
K = @(X,Z)exp( -distmat(X,Z)/(2*sigma^2) );

%%
% Weights.

lambda = 0.01;
r = (K(X,X)+lambda*eye(p))\y;

%%
% Regressor.

Y = @(x)K(x,X)*r;

%%
% Evaluation on a 2D grid.

q = 101;
t = linspace(-B,B,q);
[v,u] = meshgrid(t,t);
Xn = [u(:), v(:)];

%%
% Display as an image.

yn = reshape(Y(Xn),[q,q]);
clf;
imagesc(t,t,yn); axis image; axis off; 
colormap jet(256);
    
%%
% _Exercice 4:_ (<../missing-exo/ check the solution>)
% Display the evolution of the regression as a function of \(\sigma\).

exo4;

%%
% _Exercice 5:_ (<../missing-exo/ check the solution>)
% Apply the kernelize regression to a real life dataset. Study the influence of \(\la\) and \(\si\).

exo5;

##### SOURCE END #####
-->
   </body>
</html>