{
  "metadata": {
    "name": ""
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Newton Method"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "This tour explores the use of the Newton method for the unconstrained\n", 
            "optimization of a smooth function\n", 
            ""
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "*Important:* You need to download the file `nt_toolbox.py` from the \n", 
            "root of the github repository.\n", 
            "$\\newcommand{\\dotp}[2]{\\langle #1, #2 \\rangle}\n", 
            "\\newcommand{\\enscond}[2]{\\lbrace #1, #2 \\rbrace}\n", 
            "\\newcommand{\\pd}[2]{ \\frac{ \\partial #1}{\\partial #2} }\n", 
            "\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}\n", 
            "\\newcommand{\\norm}[1]{\\|#1\\|}\n", 
            "\\newcommand{\\abs}[1]{\\left|#1\\right|}\n", 
            "\\newcommand{\\choice}[1]{ \\left\\{  \\begin{array}{l} #1 \\end{array} \\right. }\n", 
            "\\newcommand{\\pa}[1]{\\left(#1\\right)}\n", 
            "\\newcommand{\\qandq}{\\quad\\text{and}\\quad}\n", 
            "\\newcommand{\\qwhereq}{\\quad\\text{where}\\quad}\n", 
            "\\newcommand{\\qifq}{ \\quad \\text{if} \\quad }\n", 
            "\\newcommand{\\qarrq}{ \\quad \\Longrightarrow \\quad }\n", 
            "\\newcommand{\\ZZ}{\\mathbb{Z}}\n", 
            "\\newcommand{\\RR}{\\mathbb{R}}\n", 
            "\\newcommand{\\Nn}{\\mathcal{N}}\n", 
            "\\newcommand{\\Hh}{\\mathcal{H}}\n", 
            "\\newcommand{\\Bb}{\\mathcal{B}}\n", 
            "\\newcommand{\\EE}{\\mathbb{E}}\n", 
            "\\newcommand{\\CC}{\\mathbb{C}}\n", 
            "\\newcommand{\\si}{\\sigma}\n", 
            "\\newcommand{\\al}{\\alpha}\n", 
            "\\newcommand{\\la}{\\lambda}\n", 
            "\\newcommand{\\ga}{\\gamma}\n", 
            "\\newcommand{\\Ga}{\\Gamma}\n", 
            "\\newcommand{\\La}{\\Lambda}\n", 
            "\\newcommand{\\si}{\\sigma}\n", 
            "\\newcommand{\\Si}{\\Sigma}\n", 
            "\\newcommand{\\be}{\\beta}\n", 
            "\\newcommand{\\de}{\\delta}\n", 
            "\\newcommand{\\De}{\\Delta}\n", 
            "\\renewcommand{\\phi}{\\varphi}\n", 
            "\\renewcommand{\\th}{\\theta}\n", 
            "\\newcommand{\\om}{\\omega}\n", 
            "\\newcommand{\\Om}{\\Omega}\n", 
            "$"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "from nt_toolbox import *", 
            "%matplotlib inline", 
            "%load_ext autoreload", 
            "%autoreload 2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Newton Method for Unconstrained Problems\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We test here Newton method for the minimization of a 2-D function.\n", 
            "\n", 
            "\n", 
            "We define a highly anisotropic function,\n", 
            "the Rosenbrock function\n", 
            "\n", 
            "$$ g(x) = (1-x_1)^2 + 100 (x_2-x_1^2)^2 $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "f = lambda x1,x2: (1-x1).^2 + 100*(x2-x1.^2).^2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The minimum of the function is reached at $x^\\star=(1,1)$ and\n", 
            "$f(x^\\star)=0$.\n", 
            "\n", 
            "\n", 
            "Evaluate the function on a regular grid.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "x1 = linspace(-2,2,150)\n", 
            "x2 = linspace(-.5,3,150)\n", 
            "[X2,X1] = meshgrid(x2,x1)\n", 
            "F = f(X1,X2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "3-D display.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "clf; surf(x2,x1, F, perform_hist_eq(F, 'linear') )\n", 
            "shading interp\n", 
            "camlight\n", 
            "axis tight\n", 
            "colormap jet"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "2-D display (histogram equalization helps to better visualize\n", 
            "the iso-contours).\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot( perform_hist_eq(F, 'linear') )\n", 
            "colormap jet(256)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Gradient descent methods, that only use first order (gradient)\n", 
            "information about $f$ are not able to efficiently minimize this\n", 
            "function because of its high anisotropy.\n", 
            "\n", 
            "\n", 
            "Define the gradient of $f$\n", 
            "$$ \\nabla g(x) = \\pa{ \\pd{g(x)}{x_1}, \\pd{g(x)}{x_2} } =\n", 
            "      \\pa{ 2 (x_1-1) + 400 x_1 (x_1^2-x_2),\n", 
            "      200 (x_2-x_1^2) } \\in \\RR^2. $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "gradf = lambda x1,x2: [2*(x1-1) + 400*x1.*(x1.^2-x2); 200*(x2-x1.^2)]\n", 
            "Gradf = lambda x: gradf(x(1),x(2))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Compute its Hessian\n", 
            "$$\n", 
            "      Hf(x) =\n", 
            "      \\begin{pmatrix}\n", 
            "          \\frac{\\partial^2 g(x)}{\\partial x_1^2} &\n", 
            "          \\frac{\\partial^2 g(x)}{\\partial x_1 \\partial x_2} \\\\\n", 
            "          \\frac{\\partial^2 g(x)}{\\partial x_1 \\partial x_2} &\n", 
            "          \\frac{\\partial^2 g(x)}{\\partial x_2^2}\n", 
            "      \\end{pmatrix}\n", 
            "      =\n", 
            "      \\begin{pmatrix}\n", 
            "          2 + 400 (x_1^2-x_2) + 800 x_1^2 & -400 x_1 \\\\\n", 
            "          -400 x_1 & 200\n", 
            "      \\end{pmatrix} \\in \\RR^{2 \\times 2}\n", 
            "$$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "hessf = lambda x1,x2: [2 + 400*(x1.^2-x2) + 800*x1.^2, -400*x1; ...\n", 
            "                -400*x1,  200]\n", 
            "Hessf = lambda x: hessf(x(1),x(2))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The Newton descent method starting from some $x^{(0)} \\in \\RR^2$,\n", 
            "$$ x^{(\\ell+1)} = x^{(\\ell)} - Hf( x^{(\\ell)} )^{-1} \\nabla f(x^{(\\ell)}). $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 1"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Implement the Newton algorithm.\n", 
            "Display the evolution of $f(x^{(\\ell)})$ and $\\norm{x^{(\\ell)}-x^{(+\\infty)}}$\n", 
            "during the iterations.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "display\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 2"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Display the evolution of $x^{(\\ell)}$, from several starting points.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Gradient and Divergence of Images\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "Local differential operators like gradient, divergence and laplacian are\n", 
            "the building blocks for variational image processing.\n", 
            "\n", 
            "\n", 
            "Load an image $g \\in \\RR^N$ of $N=n \\times n$ pixels.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "n = 256\n", 
            "g = rescale( load_image('lena',n) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display it.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(g)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "For continuous functions, the gradient reads\n", 
            "$$ \\nabla g(x) = \\pa{ \\pd{g(x)}{x_1}, \\pd{g(x)}{x_2} } \\in \\RR^2. $$\n", 
            "(note that here, the variable $x$ denotes the 2-D spacial position).\n", 
            "\n", 
            "\n", 
            "We discretize this differential operator using first order finite\n", 
            "differences.\n", 
            "$$ (\\nabla g)_i = ( g_{i_1,i_2}-g_{i_1-1,i_2}, g_{i_1,i_2}-g_{i_1,i_2-1} ) \\in \\RR^2. $$\n", 
            "Note that for simplity we use periodic boundary conditions.\n", 
            "\n", 
            "\n", 
            "Compute its gradient, using finite differences.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "s = [n 1:n-1]\n", 
            "grad = lambda f: cat(3, f-f(s,:), f-f(:,s))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One thus has $ \\nabla : \\RR^N \\mapsto \\RR^{N \\times 2}. $\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "v = grad(g)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can display each of its components.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(v(:,:,1), 'd/dx', 1,2,1)\n", 
            "imageplot(v(:,:,2), 'd/dy', 1,2,2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can also display it using a color image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(v)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can display its magnitude $\\norm{\\nabla g(x)}$, which is large near edges.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot( sqrt( sum3(v.^2,3) ) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The divergence operator maps vector field to images.\n", 
            "For continuous vector fields $v(x) \\in \\RR^2$, it is defined as\n", 
            "$$ \\text{div}(v)(x) = \\pd{v_1(x)}{x_1} +  \\pd{v_2(x)}{x_2} \\in \\RR. $$\n", 
            "(note that here, the variable $x$ denotes the 2-D spacial position).\n", 
            "It is minus the adjoint of the gadient, i.e. $\\text{div} = - \\nabla^*$.\n", 
            "\n", 
            "\n", 
            "It is discretized, for $v=(v^1,v^2)$ as\n", 
            "$$ \\text{div}(v)_i = v^1_{i_1+1,i_2} - v^1_{i_1,i_2} + v^2_{i_1,i_2+1} - v^2_{i_1,i_2} . $$\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "t = [2:n 1]\n", 
            "div = lambda v: v(t,:,1)-v(:,:,1) + v(:,t,2)-v(:,:,2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "The Laplacian operatore is defined as $\\Delta=\\text{div} \\circ  \\nabla =\n", 
            "-\\nabla^* \\circ \\nabla$. It is thus a negative symmetric operator.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "delta = lambda f: div(grad(f))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display $\\Delta f_0$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(delta(g))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Check that the relation $ \\norm{\\nabla f} = - \\dotp{\\Delta f}{f}.  $\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "dotp = lambda a,b: sum(a(:).*b(:))\n", 
            "fprintf('Should be 0: %.3i\\n', dotp(grad(g), grad(g)) + dotp(delta(g),g) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Newton Method in Image Processing\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider now the problem of denoising an image $y \\in \\RR^N$ where\n", 
            "$N = n \\times n$ is the number of pixels ($n$ being the number of rows/columns in the image).\n", 
            "\n", 
            "\n", 
            "Add noise to the clean image, to simulate a noisy image $y$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "sigma = .1\n", 
            "y = g + randn(n)*sigma"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display the noisy image $y$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(clamp(y))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Denoising is obtained by minimizing the following functional\n", 
            "$$ \\umin{x \\in \\RR^d} g(x) = \\frac{1}{2} \\norm{ y-x}^2 + \\la J(x) $$\n", 
            "where $J(x)$ is a smoothed total variation of the image.\n", 
            "$$ J(x) = \\sum_i \\norm{ (G x)_i }_{\\epsilon} $$\n", 
            "where $ (Gx)_i \\in \\RR^2 $ is an approximation of the gradient of $x$ at pixel\n", 
            "$i$ and for $u \\in \\RR^2$, we use the following smoothing of the\n", 
            "$L^2$ norm in $\\RR^2$\n", 
            "$$ \\norm{u}_\\epsilon = \\sqrt{ \\epsilon^2 + \\norm{u}^2 }, $$\n", 
            "for a small value of $\\epsilon>0$.\n", 
            "\n", 
            "\n", 
            "Value for $\\lambda$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "lambda = .3/5"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Value for $\\epsilon$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "epsilon = 1e-3"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "TV norm.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Amplitude = lambda u: sqrt(epsilon^2 + sum(u.^2,3))\n", 
            "J = lambda x: sum(sum(Amplitude(grad(x))))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Function to minimize.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "f = lambda x: 1/2*norm(x-y,'fro')^2 + lambda*J(x)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The gradient of the functional read\n", 
            "$$ \\nabla g(x) = x-y + \\lambda \\nabla J(x) $$\n", 
            "where the gradient of the smoothed TV norm is\n", 
            "$$ \\nabla J(x)_i = G^*( u ) \\qwhereq\n", 
            "   u_i = \\frac{ (G x)_i }{\\norm{ (G x)_i }_\\epsilon} $$\n", 
            "where $G^*$ is the adjoint operator of $G$ which corresponds to minus\n", 
            "a discretized divergence.\n", 
            "\n", 
            "\n", 
            "Define the gradient of TV norm. Note that |div| implement $-G^*$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Normalize = lambda u: u./repmat(Amplitude(u), [1 1 2])\n", 
            "GradJ = lambda x: -div( Normalize(grad(x)) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Gradient of the functional.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Gradf = lambda x: x-y+lambda*GradJ(x)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The gradient of the functional can be written as\n", 
            "$$ \\nabla g(x) = x - y + G^* \\circ \\delta_{A(x)} \\circ G (x) $$\n", 
            "where $A(x) \\in \\RR^N$ is the norm of the gradient\n", 
            "$$ A(x)_i = \\norm{(G u)_i} $$\n", 
            "and where, for $x \\in \\RR^N$ and $ a \\in \\RR^{N} $,\n", 
            "$$ \\de_{a}(u)_i = \\frac{u_i}{ \\sqrt{\\epsilon^2+a_i^2} }. $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "A = lambda x: sqrt(sum(grad(x).^2, 3))\n", 
            "delta = lambda a,u: u./sqrt( epsilon^2 + repmat(a.^2,[1 1 2]) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "This shows that the Hessian of $f$ is\n", 
            "$$ Hf(x) = \\text{Id} + G^* \\circ \\delta_{d(x)} \\circ G + G^* \\circ \\Delta_{x} (G(x)) $$\n", 
            "where $\\Delta_{x}(u) $ is the derivative of the mapping $ x \\mapsto\n", 
            "\\delta_{d(x)}(u) $.\n", 
            "\n", 
            "\n", 
            "Note that since $f$ is strictly convex, $Hf(x)$ is a positive\n", 
            "definite matrix.\n", 
            "\n", 
            "\n", 
            "We use a quasi-Newton scheme\n", 
            "$$ x^{(\\ell+1)} = x^{(\\ell)} - H_\\ell^{-1} \\nabla f(x^{(\\ell)}), $$\n", 
            "where $ H_\\ell $ is an approximation of the Hessian $H f(x^{(\\ell)})$.\n", 
            "\n", 
            "\n", 
            "We define this approximation as\n", 
            "$$ H_\\ell = \\tilde H f(x^{(\\ell)})\n", 
            "      \\qwhereq\n", 
            "      \\tilde Hf(x) = \\text{Id} + G^* \\circ \\delta_{x} \\circ G\n", 
            "$$\n", 
            "\n", 
            "\n", 
            "$H_\\ell$ is a symmetric positive\n", 
            "definite matrix, that is bounded by bellow by the identity,\n", 
            "which ensures the convergence of the method.\n", 
            "\n", 
            "\n", 
            "\n", 
            "Implement $\\tilde H f(x)$. Note that is parameterized by $a=A(x)$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "H = lambda z,a: z - lambda*div( delta(a, grad(z)) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Implement $\\tilde H f(x)^{-1}$ using a conjugate gradient.\n", 
            "Note that it takes as argument a previous solution |u_prev|.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "flat = lambda x: x(:); flatI = lambda x: reshape(x,[n,n])\n", 
            "tol = eps; maxit = 40\n", 
            "Hinv = lambda u,a,u_prev: cgs(lambda z: flat(H(flatI(z),a)), flat(u),tol,maxit,[],[],flat(u_prev))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Initialize the algorithm $x^{(0)} = y$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "x = y"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Compute the vector $d = H_\\ell^{-1} \\nabla f(x^{(\\ell)})$.\n", 
            "\n", 
            "\n", 
            "_Important:_ to get read of the warning message, you should use the code\n", 
            "as this:\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "d = zeros(n); % replace thie line by the previous iterate d value\n", 
            "[d,~] = Hinv(Gradf(x), A(x), d)\n", 
            "d = flatI(d)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Compute $x^{(\\ell+1)} = x^{(\\ell)} - d$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "x = x - d"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 3"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Implement the Newton descent algorithm.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "  d = Hinv(Gradf(x), A(x), d);\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Display the result.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(clamp(x))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 4"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Compare the Newton descent with the gradient descent with a fixed step\n", 
            "size, in term of decay of the energy.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 5"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "The direct comparison between gradient method and Newton is not fair in\n", 
            "term of iteration count. Indeed, and iteration of Newton requires\n", 
            "several steps of conjugate gradient, which takes some time.\n", 
            "Try to set-up a fair comparison benchmark that takes into account the\n", 
            "runing time of the methods. Pay a particular attention to the number of\n", 
            "steps (or the tolerance criterion) that parameterize |cgs|."
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }
      ]
    }
  ]
}