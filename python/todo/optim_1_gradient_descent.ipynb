{
  "metadata": {
    "name": ""
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Gradient Descent Methods"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "This tour explores the use of gradient descent method for unconstrained\n", 
            "and constrained optimization of a smooth function\n", 
            ""
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "*Important:* You need to download the file `nt_toolbox.py` from the \n", 
            "root of the github repository.\n", 
            "$\\newcommand{\\dotp}[2]{\\langle #1, #2 \\rangle}\n", 
            "\\newcommand{\\enscond}[2]{\\lbrace #1, #2 \\rbrace}\n", 
            "\\newcommand{\\pd}[2]{ \\frac{ \\partial #1}{\\partial #2} }\n", 
            "\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}\n", 
            "\\newcommand{\\norm}[1]{\\|#1\\|}\n", 
            "\\newcommand{\\abs}[1]{\\left|#1\\right|}\n", 
            "\\newcommand{\\choice}[1]{ \\left\\{  \\begin{array}{l} #1 \\end{array} \\right. }\n", 
            "\\newcommand{\\pa}[1]{\\left(#1\\right)}\n", 
            "\\newcommand{\\qandq}{\\quad\\text{and}\\quad}\n", 
            "\\newcommand{\\qwhereq}{\\quad\\text{where}\\quad}\n", 
            "\\newcommand{\\qifq}{ \\quad \\text{if} \\quad }\n", 
            "\\newcommand{\\qarrq}{ \\quad \\Longrightarrow \\quad }\n", 
            "\\newcommand{\\ZZ}{\\mathbb{Z}}\n", 
            "\\newcommand{\\RR}{\\mathbb{R}}\n", 
            "\\newcommand{\\Nn}{\\mathcal{N}}\n", 
            "\\newcommand{\\Hh}{\\mathcal{H}}\n", 
            "\\newcommand{\\Bb}{\\mathcal{B}}\n", 
            "\\newcommand{\\EE}{\\mathbb{E}}\n", 
            "\\newcommand{\\CC}{\\mathbb{C}}\n", 
            "\\newcommand{\\si}{\\sigma}\n", 
            "\\newcommand{\\al}{\\alpha}\n", 
            "\\newcommand{\\la}{\\lambda}\n", 
            "\\newcommand{\\ga}{\\gamma}\n", 
            "\\newcommand{\\Ga}{\\Gamma}\n", 
            "\\newcommand{\\La}{\\Lambda}\n", 
            "\\newcommand{\\si}{\\sigma}\n", 
            "\\newcommand{\\Si}{\\Sigma}\n", 
            "\\newcommand{\\be}{\\beta}\n", 
            "\\newcommand{\\de}{\\delta}\n", 
            "\\newcommand{\\De}{\\Delta}\n", 
            "\\renewcommand{\\phi}{\\varphi}\n", 
            "\\renewcommand{\\th}{\\theta}\n", 
            "\\newcommand{\\om}{\\omega}\n", 
            "\\newcommand{\\Om}{\\Omega}\n", 
            "$"
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "from nt_toolbox import *", 
            "%matplotlib inline", 
            "%load_ext autoreload", 
            "%autoreload 2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Gradient Descent for Unconstrained Problems\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider the problem of finding a minimum of a function $f$, hence\n", 
            "solving\n", 
            "$$ \\umin{x \\in \\RR^d} f(x) $$\n", 
            "where $f : \\RR^d \\rightarrow \\RR$ is a smooth function.\n", 
            "\n", 
            "\n", 
            "Note that the minimum is not necessarily unique. In the general case,\n", 
            "$f$ might exhibit local minima, in which case the proposed algorithms\n", 
            "is not expected to find a global minimizer of the problem.\n", 
            "In this tour, we restrict our attention to convex function, so that\n", 
            "the methods will converge to a global minimizer.\n", 
            "\n", 
            "\n", 
            "The simplest method is the gradient descent, that computes\n", 
            "$$ x^{(k+1)} = x^{(k)} - \\tau_k \\nabla f(x^{(k)}), $$\n", 
            "where $\\tau_k>0$ is a step size, and $\\nabla f(x) \\in \\RR^d$ is the\n", 
            "gradient of $f$ at the point $x$, and $x^{(0)} \\in \\RR^d$ is any initial point.\n", 
            "\n", 
            "\n", 
            "In the convex case, if $f$ is of class $C^2$,\n", 
            "in order to ensure convergence, the step size should satisfy\n", 
            "$$ 0 < \\tau_k < \\frac{2}{ \\sup_x \\norm{Hf(x)} } $$\n", 
            "where $Hf(x) \\in \\RR^{d \\times d}$ is the Hessian of $f$ at $x$\n", 
            "and $ \\norm{\\cdot} $ is the spectral operator norm (largest eigenvalue).\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Gradient Descent in 2-D\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider a simple problem, corresponding to the minimization of a 2-D\n", 
            "quadratic form\n", 
            "$$ f(x) = \\frac{1}{2} \\pa{ x_1^2 + \\eta x_2^2, } $$\n", 
            "where $ \\eta>0 $ controls the anisotropy, and hence the difficulty, of\n", 
            "the problem.\n", 
            "\n", 
            "\n", 
            "Anisotropy parameter $\\eta$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "eta = 10"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Function $f$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "f = lambda x: ( x(1)^2 + eta*x(2)^2 ) /2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Background image of the function.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "t = linspace(-.7,.7,101)\n", 
            "[u,v] = meshgrid(t,t)\n", 
            "F = ( u.^2 + eta*v.^2 )/2 "
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display the function as a 2-D image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "clf; hold on\n", 
            "imagesc(t,t,F); colormap jet(256)\n", 
            "contour(t,t,F, 20, 'k')\n", 
            "axis off; axis equal"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Gradient.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Gradf = lambda x: [x(1); eta*x(2)]"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The step size should satisfy $\\tau_k < 2/\\eta$. We use here a constrant\n", 
            "step size.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "tau = 1.8/eta"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 1"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Perform the gradient descent using a fixed step size $\\tau_k=\\tau$.\n", 
            "Display the decay of the energy $f(x^{(k)})$ through the iteration.\n", 
            "Save the iterates so that |X(:,k)| corresponds to $x^{(k)}$.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Display the iterations.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "clf; hold on\n", 
            "imagesc(t,t,F); colormap jet(256)\n", 
            "contour(t,t,F, 20, 'k')\n", 
            "h = plot(X(1,:), X(2,:), 'k.-')\n", 
            "set(h, 'LineWidth', 2)\n", 
            "set(h, 'MarkerSize', 15)\n", 
            "axis off; axis equal"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 2"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Display the iteration for several different step sizes.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Gradient and Divergence of Images\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "Local differential operators like gradient, divergence and laplacian are\n", 
            "the building blocks for variational image processing.\n", 
            "\n", 
            "\n", 
            "Load an image $x_0 \\in \\RR^N$ of $N=n \\times n$ pixels.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "n = 256\n", 
            "x0 = rescale( load_image('lena',n) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display it.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(x0)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "For a continuous function $g$, the gradient reads\n", 
            "$$ \\nabla g(s) = \\pa{ \\pd{g(s)}{s_1}, \\pd{g(s)}{s_2} } \\in \\RR^2. $$\n", 
            "(note that here, the variable $s$ denotes the 2-D spacial position).\n", 
            "\n", 
            "\n", 
            "We discretize this differential operator on a discrete image $x \\in \\RR^N$ using first order finite\n", 
            "differences.\n", 
            "$$ (\\nabla x)_i = ( x_{i_1,i_2}-x_{i_1-1,i_2}, x_{i_1,i_2}-x_{i_1,i_2-1} ) \\in \\RR^2. $$\n", 
            "Note that for simplity we use periodic boundary conditions.\n", 
            "\n", 
            "\n", 
            "Compute its gradient, using finite differences.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "grad = lambda x: cat(3, x-x([end 1:end-1],:), x-x(:,[end 1:end-1]))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One thus has $ \\nabla : \\RR^N \\mapsto \\RR^{N \\times 2}. $\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "v = grad(x0)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can display each of its components.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(v(:,:,1), 'd/dx', 1,2,1)\n", 
            "imageplot(v(:,:,2), 'd/dy', 1,2,2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can also display it using a color image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(v)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can display its magnitude $\\norm{(\\nabla x)_i}$, which is large near edges.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot( sqrt( sum3(v.^2,3) ) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The divergence operator maps vector field to images.\n", 
            "For continuous vector fields $v(s) \\in \\RR^2$, it is defined as\n", 
            "$$ \\text{div}(v)(s) = \\pd{v_1(s)}{s_1} +  \\pd{v_2(s)}{s_2} \\in \\RR. $$\n", 
            "(note that here, the variable $s$ denotes the 2-D spacial position).\n", 
            "It is minus the adjoint of the gadient, i.e. $\\text{div} = - \\nabla^*$.\n", 
            "\n", 
            "\n", 
            "It is discretized, for $v=(v^1,v^2)$ as\n", 
            "$$ \\text{div}(v)_i = v^1_{i_1+1,i_2} - v^1_{i_1,i_2} + v^2_{i_1,i_2+1} - v^2_{i_1,i_2} . $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "div = lambda v: v([2:end 1],:,1)-v(:,:,1) + v(:,[2:end 1],2)-v(:,:,2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "The Laplacian operatore is defined as $\\Delta=\\text{div} \\circ  \\nabla =\n", 
            "-\\nabla^* \\circ \\nabla$. It is thus a negative symmetric operator.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "delta = lambda x: div(grad(x))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display $\\Delta x_0$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(delta(x0))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Check that the relation $ \\norm{\\nabla x} = - \\dotp{\\Delta x}{x}.  $\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "dotp = lambda a,b: sum(a(:).*b(:))\n", 
            "fprintf('Should be 0: %.3i\\n', dotp(grad(x0), grad(x0)) + dotp(delta(x0),x0) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Gradient Descent in Image Processing\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider now the problem of denoising an image $y \\in \\RR^d$ where\n", 
            "$d = n \\times n$ is the number of pixels ($n$ being the number of rows/columns in the image).\n", 
            "\n", 
            "\n", 
            "Add noise to the original image, to simulate a noisy image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "sigma = .1\n", 
            "y = x0 + randn(n)*sigma"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display the noisy image $y$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(clamp(y))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Denoising is obtained by minimizing the following functional\n", 
            "$$ \\umin{x \\in \\RR^d} f(x) = \\frac{1}{2} \\norm{y-x}^2 + \\la J_\\epsilon(x) $$\n", 
            "where $J_\\epsilon(x)$ is a smoothed total variation of the image.\n", 
            "$$ J_\\epsilon(x) = \\sum_i \\norm{ (G x)_i }_{\\epsilon} $$\n", 
            "where $ (Gx)_i \\in \\RR^2 $ is an approximation of the gradient of $x$ at pixel\n", 
            "$i$ and for $u \\in \\RR^2$, we use the following smoothing of the\n", 
            "$L^2$ norm in $\\RR^2$\n", 
            "$$ \\norm{u}_\\epsilon = \\sqrt{ \\epsilon^2 + \\norm{u}^2 }, $$\n", 
            "for a small value of $\\epsilon>0$.\n", 
            "\n", 
            "\n", 
            "The gradient of the functional read\n", 
            "$$ \\nabla f(x) = x-y + \\lambda \\nabla J_\\epsilon(x) $$\n", 
            "where the gradient of the smoothed TV norm is\n", 
            "$$ \\nabla J_\\epsilon(x)_i = G^*( u ) \\qwhereq\n", 
            "   u_i = \\frac{ (G x)_i }{\\norm{ (G x)_i }_\\epsilon} $$\n", 
            "where $G^*$ is the adjoint operator of $G$ which corresponds to minus\n", 
            "a discretized divergence.\n", 
            "\n", 
            "\n", 
            "Value for $\\lambda$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "lambda = .3/5"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Value for $\\epsilon$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "epsilon = 1e-3"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "TV norm.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "NormEps = lambda u,epsilon: sqrt(epsilon^2 + sum(u.^2,3))\n", 
            "J = lambda x,epsilon: sum(sum(NormEps(grad(x),epsilon)))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Function $f$ to minimize.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "f = lambda y,x,epsilon: 1/2*norm(x-y,'fro')^2 + lambda*J(x,epsilon)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Gradient of $J_\\epsilon$. Note that |div| implement $-G^*$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Normalize = lambda u,epsilon: u./repmat(NormEps(u,epsilon), [1 1 2])\n", 
            "GradJ = lambda x,epsilon: -div( Normalize(grad(x),epsilon) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Gradient of the functional.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Gradf = lambda y,x,epsilon: x-y+lambda*GradJ(x,epsilon)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The step size should satisfy\n", 
            "$$ 0 < \\tau_k < \\frac{2}{ 1 + 4 \\lambda / \\epsilon }. $$\n", 
            "Here we use a slightly larger step size, which still work in practice.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "tau = 1.8/( 1 + lambda*8/epsilon )\n", 
            "tau = tau*4"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 3"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Implement the gradient descent. Monitor the decay of $f$ through the\n", 
            "iterations.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Display the resulting denoised image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(clamp(x))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Constrained Optimization Using Projected Gradient Descent\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider a linear imaging operator $\\Phi : x \\mapsto \\Phi(x)$\n", 
            "that maps high resolution images to low dimensional observations.\n", 
            "Here we consider a pixel masking operator, that is diagonal over the\n", 
            "spacial domain.\n", 
            "\n", 
            "\n", 
            "To emphasis the effect of the TV functional, we use a simple geometric\n", 
            "image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "n = 64\n", 
            "name = 'square'\n", 
            "x0 = load_image(name,n)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "We consider here the inpainting problem. This simply corresponds to a\n", 
            "masking operator.\n", 
            "Here we remove the central part of the image.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "a = 4\n", 
            "Lambda = ones(n)\n", 
            "Lambda(end/2-a:end/2+a,:) = 0"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Masking operator $ \\Phi $. Note that it is symmetric, i.e. $\\Phi^*=\\Phi$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "Phi  = lambda x: x.*Lambda\n", 
            "PhiS = lambda x: Phi(x)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Noiseless observations $y=\\Phi x_0$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "y = Phi(x0)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(x0, 'Original', 1,2,1)\n", 
            "imageplot(y, 'Damaged', 1,2,2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "We want to solve the noiseless inverse problem $y=\\Phi f$ using a total\n", 
            "variation regularization:\n", 
            "$$ \\umin{ y=\\Phi x } J_\\epsilon(x). $$\n", 
            "We use the following projected gradient descent\n", 
            "$$ x^{(k+1)} = \\text{Proj}_{\\Hh}( x^{(k)} - \\tau_k \\nabla J_{\\epsilon}(x^{(k)}) ) $$\n", 
            "where $\\text{Proj}_{\\Hh}$ is the orthogonal projection on the set of\n", 
            "linear constraint $\\Phi x = y$, and is easy to compute for inpainting\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "ProjH = lambda x,y:  x + PhiS( y - Phi(x) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 4"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Display the evolution of the inpainting process.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 5"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Try with several values of $\\epsilon$.\n", 
            "\n", 
            "\n", 
            "tau = tau * 100;\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }
      ]
    }
  ]
}