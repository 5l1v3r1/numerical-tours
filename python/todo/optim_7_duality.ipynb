{
  "metadata": {
    "name": ""
  }, 
  "nbformat": 3, 
  "nbformat_minor": 0, 
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading", 
          "level": 1, 
          "metadata": {}, 
          "source": [
            "Fenchel-Rockafellar Duality"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "This numerical tour is an introduction to convex duality with an\n", 
            "application to total variation denoising.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "*Important:* You need to download the file `nt_toolbox.py` from the \n", 
            "root of the github repository.\n", 
            "$\\newcommand{\\dotp}[2]{\\langle #1, #2 \\rangle}\n", 
            "\\newcommand{\\enscond}[2]{\\lbrace #1, #2 \\rbrace}\n", 
            "\\newcommand{\\pd}[2]{ \\frac{ \\partial #1}{\\partial #2} }\n", 
            "\\newcommand{\\umin}[1]{\\underset{#1}{\\min}\\;}\n", 
            "\\newcommand{\\norm}[1]{\\|#1\\|}\n", 
            "\\newcommand{\\abs}[1]{\\left|#1\\right|}\n", 
            "\\newcommand{\\choice}[1]{ \\left\\{  \\begin{array}{l} #1 \\end{array} \\right. }\n", 
            "\\newcommand{\\pa}[1]{\\left(#1\\right)}\n", 
            "\\newcommand{\\qandq}{\\quad\\text{and}\\quad}\n", 
            "\\newcommand{\\qwhereq}{\\quad\\text{where}\\quad}\n", 
            "\\newcommand{\\qifq}{ \\quad \\text{if} \\quad }\n", 
            "\\newcommand{\\qarrq}{ \\quad \\Longrightarrow \\quad }\n", 
            "\\newcommand{\\ZZ}{\\mathbb{Z}}\n", 
            "\\newcommand{\\RR}{\\mathbb{R}}\n", 
            "\\newcommand{\\Nn}{\\mathcal{N}}\n", 
            "\\newcommand{\\Hh}{\\mathcal{H}}\n", 
            "\\newcommand{\\Bb}{\\mathcal{B}}\n", 
            "\\newcommand{\\EE}{\\mathbb{E}}\n", 
            "\\newcommand{\\CC}{\\mathbb{C}}\n", 
            "\\newcommand{\\si}{\\sigma}\n", 
            "\\newcommand{\\al}{\\alpha}\n", 
            "\\newcommand{\\la}{\\lambda}\n", 
            "\\newcommand{\\ga}{\\gamma}\n", 
            "\\newcommand{\\Ga}{\\Gamma}\n", 
            "\\newcommand{\\La}{\\Lambda}\n", 
            "\\newcommand{\\si}{\\sigma}\n", 
            "\\newcommand{\\Si}{\\Sigma}\n", 
            "\\newcommand{\\be}{\\beta}\n", 
            "\\newcommand{\\de}{\\delta}\n", 
            "\\newcommand{\\De}{\\Delta}\n", 
            "\\renewcommand{\\phi}{\\varphi}\n", 
            "\\renewcommand{\\th}{\\theta}\n", 
            "\\newcommand{\\om}{\\omega}\n", 
            "\\newcommand{\\Om}{\\Omega}\n", 
            "$"
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Convex Duality\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "Given some convex, proper, and lower semi-continuous function $f(x)$ defined for $x \\in \\RR^N$, its\n", 
            "Legendre-Fenchel dual function is defined as\n", 
            "$$ \\forall u \\in \\RR^N, \\quad f^*(u) = \\umax{x \\in \\RR^N} \\dotp{x}{u} - f(x). $$\n", 
            "\n", 
            "\n", 
            "One can show that $f^*$ is a convex function, and that it satisfies\n", 
            "$(f^*)^* = f$.\n", 
            "\n", 
            "\n", 
            "One can show if $f(x) = \\frac{1}{2}\\norm{A x - b}^2$ where $A \\in \\RR^{N \\times N}$\n", 
            "is an invertible matrix, then\n", 
            "$$f^*(u) = \\frac{1}{2}\\norm{\\tilde A u + b}^2\n", 
            "\\qwhereq \\tilde A = (A^*)^{-1}. $$\n", 
            "\n", 
            "\n", 
            "One can show that in the case of $\\ell^p$ norms\n", 
            "$$ f(x) = \\norm{x}_p = \\pa{ \\sum_{i=1}^N \\abs{x_i}^p }^{1/p} $$\n", 
            "with the usual extension to $p=+\\infty$\n", 
            "$$ \\norm{x}_\\infty = \\umax{1 \\leq i \\leq N} \\abs{x_i}$$\n", 
            "then one has\n", 
            "$$ f^*(u) = \\iota_{\\norm{\\cdot}_q \\leq 1}\n", 
            " \\qwhereq \\frac{1}{p}+\\frac{1}{q}=1, $$\n", 
            "where $\\iota_{\\Cc}$ is the indicator function of the convex set\n", 
            "$\\Cc$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "FB on the Fenchel-Rockafellar Dual Problem\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We are concerned with the minimization of composite problems of the form\n", 
            "$$ \\umin{x \\in \\RR^N} f(x) + g(A(x)) $$\n", 
            "where $ A \\in \\in \\RR^{P \\times N} $ is a linear map (a matrix),\n", 
            " $f : \\RR^N \\rightarrow \\RR $\n", 
            "and $g : \\RR^P \\rightarrow \\RR $\n", 
            "are convex functional.\n", 
            "\n", 
            "\n", 
            "We now assume that $f$ is a $L$-strongly convex function.\n", 
            "In this case, one can show that $f^*$ is a $C^1$ smooth function, and\n", 
            "that its gradient is $L$-Lipschitz.\n", 
            "\n", 
            "\n", 
            "In this case, the Fenchel-Rockafellar theorem shows that one can solve\n", 
            "the following dual problem\n", 
            "$$ \\umin{x \\in \\RR^N} f(x) + g(A(x)) =\n", 
            "  - \\umin{u \\in \\RR^P} f^*( -A^* u ) + g^*(u) $$\n", 
            "and recover the unique solution $x^\\star$\n", 
            "of the primal problem from a (non-necessarily unique) solution $u^\\star$\n", 
            "to the dual problem as\n", 
            "$$ x^\\star = \\nabla f^*( -A^* u^\\star ). $$\n", 
            "\n", 
            "\n", 
            "Denoting $F(u) = f^*( -A^* u )$ and $G(u) = g^*(u)$, one thus needs\n", 
            "to solve the problem\n", 
            "$$ \\umin{u \\in \\RR^P} F(u) + G(u). $$\n", 
            "\n", 
            "\n", 
            "We assume that the function $g$ is simple, in the sense that\n", 
            "one can compute in closed form\n", 
            "the so-called proximal mapping, which is defined as\n", 
            "$$ \\text{prox}_{\\ga g}(x) = \\uargmin{z \\in \\RR^N} \\frac{1}{2}\\norm{x-z}^2 + \\ga g(z). $$\n", 
            "for any $\\ga > 0$.\n", 
            "\n", 
            "\n", 
            "Note that $g$ being simple is equivalent to $g^*$ also being simple\n", 
            "because of\n", 
            "Moreau's identity:\n", 
            "$$ x = \\text{prox}_{\\tau g^*}(x) + \\tau \\text{prox}_{g/\\tau}(x/\\tau). $$\n", 
            "\n", 
            "\n", 
            "Since $F$ is smooth and $G$ is simple, one can apply the\n", 
            "Foward-Backward algorithm, which reads,\n", 
            "after initilizing $u^{(0)} \\in \\RR^P$,\n", 
            "$$ u^{(\\ell+1)} = \\text{prox}_{\\ga G}\\pa{ u^{(\\ell)} - \\ga \\nabla F( u^{(\\ell)} )  }. $$\n", 
            "with $\\ga < 2/L$.\n", 
            "\n", 
            "\n", 
            "The primal iterates are defined as\n", 
            "$$ x^{(\\ell)} = \\nabla F( -A^* u^{(\\ell)} ). $$\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Total Variation\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "The total variation of a smooth function $ \\phi : \\RR^2 \\rightarrow \\RR $ is defined as\n", 
            "$$ J(\\phi) = \\int \\norm{\\nabla \\phi(s)} d s$$\n", 
            "The total variation of an image is also equal to the total length of its level sets.\n", 
            "$$ J(\\phi) = \\int_{-\\infty}^{+\\infty} L( S_t(\\phi) ) dt. $$\n", 
            "Where $S_t(\\phi)$ is the level set at $t$ of the function $\\phi$\n", 
            "$$S_t(\\phi)= \\enscond{ s }{ \\phi(s)=t } . $$\n", 
            "This shows that the total variation can be extended to functions having\n", 
            "step discontinuities.\n", 
            "\n", 
            "\n", 
            "We consider images $x = (x_{i,j})_{i,j}  \\in \\RR^N$ of $N=n\\times n$ pixels.\n", 
            "\n", 
            "\n", 
            "We consider here a discretized gradient operator\n", 
            "$ A : \\RR^N \\rightarrow \\RR^P $ where $P=2N$ defined as\n", 
            "$$ A x  = u = (u^1,u^2)\n", 
            "\\qwhereq   u^1 = ( x_{i+1,j}-x_{i,j} )_{ i,j } \\in \\RR^N\n", 
            "\\qandq     u^2 = ( x_{i,j+1}-x_{i,j} )_{ i,j } \\in \\RR^N. $$\n", 
            "where we assume periodic boundary conditions for simplicity.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "from nt_toolbox import *", 
            "%matplotlib inline", 
            "%load_ext autoreload", 
            "%autoreload 2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The adjoint $A^*$ of the discrete gradient is minus the discrete divergence.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "As = lambda u: -div(u)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "In the following, while images $x \\in \\RR^{N}$ are stored as arrays of\n", 
            "size |(n,n)|, gradient vector fields $u \\in \\RR^P$ are stored as arrays\n", 
            "size |(n,n,2)|.\n", 
            "\n", 
            "\n", 
            "The discrete total variation is defined as the $\\ell^1-\\ell^2$ norm of\n", 
            "the discretized gradient\n", 
            "$$ J(x) = \\norm{A x}_{1,2}\n", 
            "    \\qwhereq\n", 
            "  \\norm{u}_{1,2} = \\sum_{i,j} \\norm{u_{i,j}} $$\n", 
            "where $u = (u_{i,j} \\in \\RR^2)_{i,j}$ is a vector field.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "norm12 = lambda u: sum(sum( sqrt(sum( u.^2,3 )) ))\n", 
            "J = lambda x: norm12(A(x))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Total Variation Regularization\n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider here denoising using total variation regularization. This was\n", 
            "first introduced in:\n", 
            "\n", 
            "\n", 
            "L.I. Rudin, S. Osher, E. Fatemi,\n", 
            "_Nonlinear total variation based noise removal algorithms,_\n", 
            "Physica D, vol. 60, pp. 259-268, 1992.\n", 
            "\n", 
            "\n", 
            "Given a noisy image $y \\in \\RR^N$, it computes\n", 
            "$$ x^\\star = \\uargmin{x \\in \\RR^N} \\frac{1}{2}\\norm{x-y}^2 + \\la J(x), $$\n", 
            "where the regularization parameter $\\la \\geq 0$ should be adapted to\n", 
            "the noise level.\n", 
            "\n", 
            "\n", 
            "Number of pixels.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "n = 256"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "First we load an image $x_0 \\in \\RR^N$ of $N=n \\times n$ pixels.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "name = 'hibiscus'\n", 
            "x0 = load_image(name,n)\n", 
            "x0 = rescale( sum(x0,3) )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display the original image $x_0$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(clamp(x0))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Add some noise to the original image, to obtain $y=x0+w$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "sigma = .1\n", 
            "y = x0 + randn(n,n)*sigma"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Display the noisy image $y$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "\n", 
            "imageplot(clamp(y))"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Set the regularization parameter $\\la$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "lambda = .2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "heading", 
          "level": 2, 
          "metadata": {}, 
          "source": [
            "Chambolle Dual Algorithm \n"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "We consider here the application of FB on the dual of the ROF problem, as\n", 
            "initially proposed in:\n", 
            "\n", 
            "\n", 
            "Antonin Chambolle,\n", 
            "_An Algorithm for Total Variation Minimization and Applications_,\n", 
            "Journal of Mathematical Imaging and Vision, 20(1-2), 2004.\n", 
            "\n", 
            "\n", 
            "An earlier version of this algorithm was proposed in:\n", 
            "\n", 
            "\n", 
            "B. Mercier,\n", 
            "_Inequations Variationnelles de la Mecanique_\n", 
            "Publications Mathematiques d'Orsay,\n", 
            "no. 80.01. Orsay, France, Universite de Paris-XI, 1980.\n", 
            "\n", 
            "\n", 
            "For a description of a more general framework, see:\n", 
            "\n", 
            "\n", 
            "P. L. Combettes, Dinh Dung, and B. C. Vu,\n", 
            "_Dualization of signal recovery problems_,\n", 
            "Set-Valued and Variational Analysis, vol. 18, pp. 373-404,\n", 
            "December 2010\n", 
            "\n", 
            "\n", 
            "The primal problem corresponds to minimizing $E(x) = f(x)+g(A(x))$ where\n", 
            "$$ f(x) = \\frac{1}{2}\\norm{x-y}^2\n", 
            "      \\qandq\n", 
            "  g(u) = \\la \\norm{u}_{1,2}. $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "mynorm = lambda x: norm(x(:))\n", 
            "f = lambda x: 1/2*mynorm(x-y)^2\n", 
            "g = lambda x: lambda*J(x)\n", 
            "E = lambda x: f(x)+g(x)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The dual problem corresponds to minimzing $F(u)+ G(u)$\n", 
            "where\n", 
            "$$ F(u) = \\frac{1}{2} \\norm{y - A^* u}^2 - \\frac{1}{2}\\norm{y}^2\n", 
            "      \\qandq\n", 
            "   G(u) = \\iota_{\\Cc}(u)\n", 
            "      \\qwhereq \\Cc = \\enscond{u}{\\norm{u}_{\\infty,2} \\leq \\la}. $$\n", 
            "where\n", 
            "$$ \\norm{u}_{\\infty,2} = \\umax{i,j} \\norm{u_{i,j}}  $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "F = lambda u: 1/2*mynorm(y-As(u))^2 - 1/2*mynorm(y)^2"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One can thus solves the ROF problem by computing\n", 
            "$$ x^\\star = y - A^* u^\\star $$\n", 
            "where\n", 
            "$$ u^\\star  \\in \\uargmin{ \\norm{u}_{1,2} \\leq \\la } \\norm{y - A^* u}  $$\n", 
            "\n", 
            "\n", 
            "One can compute explicitely the gradient of $F$:\n", 
            "$$ \\nabla F(u) =  A (A^* u - y). $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "nablaF = lambda u: A(As(u)-y)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "The proximal operator of $ G $ is the orthogonal projection on\n", 
            "$\\Cc$, which is obtained as\n", 
            "$$ \\text{prox}_{\\ga G}(u)_{i,j} = \\frac{u_{i,j}}{ \\max(1,\\norm{u_{i,j}}/\\lambda) }. $$\n", 
            "Note that it does not depends on $\\ga$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "d = lambda u: repmat( sqrt(sum(u.^2,3)), [1 1 2] )\n", 
            "proxG = lambda u,gamma: u ./ max( d(u)/lambda, 1 )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "The gradient step size of the FB should satisfy\n", 
            "$$ \\ga < \\frac{2}{\\norm{A^* A}} = \\frac{1}{4}. $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "gamma = 1/5"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Initialize the FB with $u=0 \\in \\RR^P$.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "u = zeros(n,n,2)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "One step of FB.\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "u = proxG( u - gamma * nablaF(u), gamma )"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "Update the solution using\n", 
            "$$ x^{(\\ell)} = y - A^* u. $$\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "x = y - As(u)"
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "heading", 
          "level": 3, 
          "metadata": {}, 
          "source": [
            "Exercise 1"
          ]
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "Perform Chambolle algorithm to solve the ROF problem.\n", 
            "Monitor the primal $E$ and dual $-F$ energies.\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            "\n", 
            ""
          ]
        }, 
        {
          "cell_type": "code", 
          "collapsed": false, 
          "input": [
            "## Insert your code here."
          ], 
          "language": "python", 
          "outputs": []
        }, 
        {
          "cell_type": "markdown", 
          "metadata": {}, 
          "source": [
            "\n", 
            "\n", 
            "\n", 
            "Display the denoised image $x^\\star$.\n", 
            ""
          ]
        }
      ]
    }
  ]
}